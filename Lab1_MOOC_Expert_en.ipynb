{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wPsCwp6aKTnD",
   "metadata": {
    "id": "wPsCwp6aKTnD"
   },
   "source": [
    "# Lab 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3LiciM1mW2j3",
   "metadata": {
    "id": "3LiciM1mW2j3"
   },
   "source": [
    "The goal of this lab session is to work with the various concepts discussed in Session 1.\n",
    "\n",
    "There are three main objectives in this lab:\n",
    "\n",
    "1. to study the impact of removing missing values on the results (Exercise 1),\n",
    "\n",
    "2. to illustrate the concept of ignorability of the missing data mechanism (Exercise 2),\n",
    "\n",
    "3. to learn how to generate missing values (mainly Exercises 3 and 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l-Cn1LcSX_Zt",
   "metadata": {
    "id": "l-Cn1LcSX_Zt"
   },
   "source": [
    "### Note on amputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NXgzKpFmYBVx",
   "metadata": {
    "id": "NXgzKpFmYBVx"
   },
   "source": [
    "A dataset is said to be *amputed* if it contains missing values that have been artificially generated. The amputation process refers to transforming a complete dataset into an incomplete one, or an already incomplete dataset into one with a higher proportion of missing values. In other words, amputation is the act of introducing missing values into the initial dataset. It is exactly the opposite of *imputation*.\n",
    "\n",
    "This is very important when dealing with missing data: it allows testing new algorithms or comparing different methods while having access to a reference score and the observed values, which are required for computing certain metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lMRqaWG2T_DZ",
   "metadata": {
    "id": "lMRqaWG2T_DZ"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c5f64",
   "metadata": {
    "id": "657c5f64"
   },
   "outputs": [],
   "source": [
    "### Classical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "\n",
    "### Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "### Real datasets\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "###  Specific libraries to handle missing values\n",
    "import pyampute\n",
    "import missingno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vipNZ1cxVC07",
   "metadata": {
    "id": "vipNZ1cxVC07"
   },
   "source": [
    "# Exercise 1: removing incomplete observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uts_vGkmRlAd",
   "metadata": {
    "id": "uts_vGkmRlAd"
   },
   "source": [
    "Consider a dataset composed of $n$ i.i.d. Gaussian samples $(X_{1.}, \\dots, X_{n.})$, where $X_{i.} \\sim N(\\mu, \\Sigma)$ with $\\mu \\in \\mathbb{R}^d$ and $\\Sigma \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "The goal is to empirically study the impact of removing incomplete observations. An observation (also called a row or sample) is considered incomplete if it contains at least one missing value.\n",
    "\n",
    "We revisit the example from Zhu et al. (2022), presented in the Module 1 video. The dataset has $d$ variables and a missing value rate of 1%. In low dimension ($d=1$), removing incomplete samples leaves about 95% of complete observations. In high dimension ($d=300$), only about 5% of observations remain complete.\n",
    "\n",
    "In this exercise, you will reproduce this example and study the impact of removing incomplete observations on the bias of the empirical mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d06e9",
   "metadata": {
    "id": "ad3d06e9"
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 5\n",
    "Mu = np.repeat(0, d)\n",
    "Sigma = 0.5 * (np.ones((d,d)) + np.eye(d))\n",
    "\n",
    "xfull = np.random.multivariate_normal(Mu, Sigma, size=n)  # complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55899d1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1756854209605,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "55899d1e",
    "outputId": "3542df4d-ca2b-4185-f038-b7447b8c05da"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xfull).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_gyIpxHoZL-c",
   "metadata": {
    "id": "_gyIpxHoZL-c"
   },
   "source": [
    "You will first generate missing values of the Missing Completely At Random (MCAR) type: the missingness does not depend on the values of the data themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zF8jJojSZYc-",
   "metadata": {
    "id": "zF8jJojSZYc-"
   },
   "source": [
    "## Question 1: generating MCAR missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_RkuR9IWmHvp",
   "metadata": {
    "id": "_RkuR9IWmHvp"
   },
   "source": [
    "To generate MCAR missing values, does the following approach seem satisfactory to you? If not, suggest a better strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048a7e9",
   "metadata": {
    "id": "0048a7e9"
   },
   "outputs": [],
   "source": [
    "p = 0.4\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "    miss_id = np.random.choice(n, np.floor(n*p).astype(int), replace=False)\n",
    "    xmiss[miss_id, j] = np.nan\n",
    "M = np.isnan(xmiss)  # mask: matrix indicating where the missing values are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UtPwaZPcuhoA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1756854209704,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "UtPwaZPcuhoA",
    "outputId": "bbd5081b-3b96-4523-c55b-f1ec33a399ac"
   },
   "outputs": [],
   "source": [
    "print(\"The percentage of NA is:\", np.sum(M) / (n*d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7vIwlnGmbYpF",
   "metadata": {
    "id": "7vIwlnGmbYpF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wVdB0GlHbkIO",
   "metadata": {
    "id": "wVdB0GlHbkIO"
   },
   "source": [
    "This method does not take into account the stochastic nature of the mask $M$, which indicates where the missing values are. It effectively treats $p$ as an exact percentage of missing values. This is why you get exactly 40% missing values (try rerunning the code to observe this!). It is better to generate the mask $M$ according to a binomial distribution with parameter $p$, which represents the probability of a value being missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RbW3ZGJAaEl4",
   "metadata": {
    "id": "RbW3ZGJAaEl4"
   },
   "outputs": [],
   "source": [
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p\n",
    "  xmiss[miss_id, j] = np.nan\n",
    "M = np.isnan(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pwfsHiK0Xiv5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1756854209735,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "pwfsHiK0Xiv5",
    "outputId": "a1b41715-cc03-4081-dbf4-3d1d8f5872a1"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xmiss).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Ei9FntPenNN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1756854209791,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "-Ei9FntPenNN",
    "outputId": "030a7b62-9e89-437f-faed-b05e2ec16a3e"
   },
   "outputs": [],
   "source": [
    "print(\"The total percentage of NAs is:\", np.sum(M) / (n*d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CUV5WKGAcm7y",
   "metadata": {
    "id": "CUV5WKGAcm7y"
   },
   "source": [
    "## Question 2: computing the bias of the empirical mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B6irL7fMZk_d",
   "metadata": {
    "id": "B6irL7fMZk_d"
   },
   "source": [
    "The dataset `xmiss` now contains missing values; it is the amputed dataset. You will compute the empirical mean of the variables after removing the incomplete observations, and compare it to the empirical mean that would have been obtained if all observations were fully observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UMuvqaXFZcwm",
   "metadata": {
    "id": "UMuvqaXFZcwm"
   },
   "source": [
    "Provide code to compute the biases of the empirical mean in both cases (per variable in a first step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe7ee2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756854209795,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "9ebe7ee2",
    "outputId": "7d3781a2-55be-4fda-a6de-40b8348029d6"
   },
   "outputs": [],
   "source": [
    "x_cc = pd.DataFrame(xmiss).dropna()\n",
    "x_cc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KgI7llTbe70W",
   "metadata": {
    "id": "KgI7llTbe70W",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee239b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1756854209825,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "60ee239b",
    "outputId": "58f2a0b3-c591-4846-fe0f-8e84b31e9bb2"
   },
   "outputs": [],
   "source": [
    "empirical_mean = np.mean(xfull, axis=0)\n",
    "empirical_mean_na = np.mean(x_cc, axis=0)\n",
    "\n",
    "bias = empirical_mean - Mu\n",
    "bias_na = empirical_mean_na - Mu\n",
    "\n",
    "print(\"Bias without NA:\", [f\"{x:.3f}\" for x in bias])\n",
    "print(\"Bias with NA:\", [f\"{x:.3f}\" for x in bias_na])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Eul3UMYIf7SQ",
   "metadata": {
    "id": "Eul3UMYIf7SQ"
   },
   "source": [
    "We can compute the L2 norm of the bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y84bFl5daDX9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1756854209841,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "y84bFl5daDX9",
    "outputId": "5b20a8b9-eccd-4072-c12f-5609006bbaf1"
   },
   "outputs": [],
   "source": [
    "norm2_bias = (bias ** 2).sum()\n",
    "norm2_bias_na = (bias_na ** 2).sum()\n",
    "\n",
    "print(\"L2 norm of the bias without NA:\", f\"{norm2_bias:.3f}\")\n",
    "print(\"L2 norm of the bias with NA:\", f\"{norm2_bias_na:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GePi91F7l5UQ",
   "metadata": {
    "id": "GePi91F7l5UQ"
   },
   "source": [
    "## Question 3: comparison across multiple simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZrdhEwOgm0ay",
   "metadata": {
    "id": "ZrdhEwOgm0ay"
   },
   "source": [
    "The goal now is to reproduce the experiment for several values of $d$ (the number of variables in the dataset) and $p$ (the probability of a value being missing). To get a sense of the order of magnitude of the bias, we want to repeat the experiment multiple times for each case.\n",
    "\n",
    "How can we complete the `compute_bias` function so that it returns the biases over multiple simulations? The function takes as input the number of simulations `n_sim`, the probability `p` that a value in the dataset is missing, the complete dataset `xfull`, and the theoretical mean `Mu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6y6DnUKAJW8p",
   "metadata": {
    "id": "6y6DnUKAJW8p"
   },
   "outputs": [],
   "source": [
    "def compute_bias(n_sim, p, xfull, Mu):\n",
    "    vec_norm2_bias = []\n",
    "    vec_norm2_bias_na = []\n",
    "\n",
    "    d = xfull.shape[1]\n",
    "\n",
    "    for it in range(n_sim):\n",
    "\n",
    "        ### Generate missing values ###\n",
    "\n",
    "        ### TO COMPLETE ###\n",
    "\n",
    "        vec_norm2_bias.append(norm2_bias)\n",
    "        vec_norm2_bias_na.append(norm2_bias_na)\n",
    "\n",
    "    return(vec_norm2_bias, vec_norm2_bias_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eZibZ2EwJhRl",
   "metadata": {
    "id": "eZibZ2EwJhRl"
   },
   "source": [
    "We can test the function with the following arguments: `n_sim`=10, `p`=10%. Then, we apply the function to obtain the values of the bias for various numbers of variables and different missing value probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QWGUNU6GsbbT",
   "metadata": {
    "id": "QWGUNU6GsbbT"
   },
   "source": [
    "### Note: stochasticity with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jDmddnRURoOW",
   "metadata": {
    "id": "jDmddnRURoOW"
   },
   "source": [
    "In the complete case, when performing multiple simulations on synthetic datasets, a common approach is to generate the dataset multiple times from the same distribution with known parameters, here $({\\Sigma},{\\mu})$. In our case, however, the stochasticity comes from the generation of missing values. We consider the complete dataset `xfull` as fixed, and generate missing values multiple times, which results in different amputed datasets `xmiss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cTpPbzHbJc-P",
   "metadata": {
    "id": "cTpPbzHbJc-P"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fdabb",
   "metadata": {
    "id": "ae5fdabb"
   },
   "outputs": [],
   "source": [
    "def compute_bias(n_sim, p, xfull, Mu):\n",
    "    vec_norm2_bias_na = []\n",
    "\n",
    "    n = xfull.shape[0]\n",
    "    d = xfull.shape[1]\n",
    "\n",
    "    empirical_mean = np.mean(xfull,axis=0)\n",
    "    bias = empirical_mean - Mu\n",
    "    norm2_bias = np.sqrt((bias ** 2).sum())\n",
    "\n",
    "    for it in range(n_sim):\n",
    "\n",
    "        ### Generate missing values ###\n",
    "        xmiss = np.copy(xfull)\n",
    "        for j in range(d):\n",
    "          miss_id = np.random.uniform(0, 1, size=np.floor(n).astype(int)) < p\n",
    "          xmiss[miss_id, j] = np.nan\n",
    "\n",
    "        x_cc = pd.DataFrame(xmiss).dropna()\n",
    "\n",
    "        if x_cc.shape[0] == 0:\n",
    "          vec_norm2_bias_na.append(np.nan)\n",
    "\n",
    "        empirical_mean_na = np.mean(x_cc, axis=0)\n",
    "\n",
    "        bias_na = empirical_mean_na - Mu\n",
    "\n",
    "        norm2_bias_na = (bias_na ** 2).sum()\n",
    "\n",
    "        vec_norm2_bias_na.append(norm2_bias_na)\n",
    "\n",
    "    return(norm2_bias, vec_norm2_bias_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01d5c1",
   "metadata": {
    "id": "2d01d5c1"
   },
   "outputs": [],
   "source": [
    "n_sim = 10\n",
    "p = 0.1\n",
    "norm2_bias, vec_norm2_bias_na = compute_bias(n_sim=n_sim, p=0.1, xfull=xfull, Mu=Mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1e780",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1756854210011,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "1ef1e780",
    "outputId": "374c6c7f-1e67-454a-eb6a-04297524434c"
   },
   "outputs": [],
   "source": [
    "print(\"Bias without NA:\", f\"{norm2_bias:.3f}\")\n",
    "print(\"Mean of the biases with NA over\", f\"{n_sim}\", \"simulations:\", f\"{np.mean(vec_norm2_bias_na):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g80z8NyGQjKZ",
   "metadata": {
    "id": "g80z8NyGQjKZ"
   },
   "outputs": [],
   "source": [
    "d_list = [5, 10, 100]\n",
    "p_list = [0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "vec_norm2_bias = np.zeros(len(d_list))\n",
    "mat_norm2_bias_na = np.zeros((len(d_list), len(p_list)))\n",
    "\n",
    "for pos_d, d in enumerate(d_list):\n",
    "\n",
    "    ### Complete dataset\n",
    "    Mu = np.repeat(0, d)\n",
    "    Sigma = 0.5 * (np.ones((d,d)) + np.eye(d))\n",
    "    xfull = np.random.multivariate_normal(Mu, Sigma, size=n)\n",
    "\n",
    "    for pos_perc, p in enumerate(p_list):\n",
    "        norm2_bias, vec_norm2_bias_na = compute_bias(n_sim=10, p=p, xfull=xfull, Mu=Mu)\n",
    "        mat_norm2_bias_na[pos_d, pos_perc] = round(np.mean(vec_norm2_bias_na), 3)\n",
    "\n",
    "    vec_norm2_bias[pos_d] = round(norm2_bias, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3816a39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1756854210926,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "e3816a39",
    "outputId": "957bbb61-c4e5-4509-a854-0561b4792324"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(vec_norm2_bias, index=[f\"d={d}\" for d in d_list], columns=['Without NA'])\n",
    "results_na = pd.DataFrame(mat_norm2_bias_na, index=[f\"d={d}\" for d in d_list], columns=[f\"p={p}\" for p in p_list])\n",
    "\n",
    "results.join(results_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PizBxaqRSDIt",
   "metadata": {
    "id": "PizBxaqRSDIt"
   },
   "source": [
    "## Question 4: interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H9x7xV5-SJdQ",
   "metadata": {
    "id": "H9x7xV5-SJdQ"
   },
   "source": [
    "Interpret the results obtained in question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9mJuFooFSdiG",
   "metadata": {
    "id": "9mJuFooFSdiG"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dZ9RVRz1kPCr",
   "metadata": {
    "id": "dZ9RVRz1kPCr"
   },
   "source": [
    "The bias is of the same order of magnitude for $d=5$ variables and a missingness probability $p$ ranging from 1% to 10%, or for $d=10$ variables with $p=1%$. Otherwise, in the other cases, the bias of the mean is significantly higher in the presence of missing values.\n",
    "\n",
    "There are `NA` in the results table whenever there is at least one simulation with no complete observations. In the next code cell, we define the function `compute_number_complete_individuals` to display the number of complete observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "omMQTmovi0eE",
   "metadata": {
    "id": "omMQTmovi0eE"
   },
   "outputs": [],
   "source": [
    "def compute_number_complete_individuals(n_sim,p,xfull):\n",
    "    vec_complete_individuals = []\n",
    "\n",
    "    n = xfull.shape[0]\n",
    "    d = xfull.shape[1]\n",
    "\n",
    "    for it in range(n_sim):\n",
    "        np.random.seed(it)\n",
    "\n",
    "        ### Generation of missing values\n",
    "        xmiss = np.copy(xfull)\n",
    "        for j in range(d):\n",
    "          miss_id = np.random.uniform(0, 1, size=np.floor(n).astype(int)) < p\n",
    "          xmiss[miss_id, j] = np.nan\n",
    "\n",
    "        x_cc = pd.DataFrame(xmiss).dropna()\n",
    "\n",
    "        number_complete_individuals = x_cc.shape[0]\n",
    "\n",
    "        vec_complete_individuals.append(number_complete_individuals)\n",
    "\n",
    "    return(vec_complete_individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6v1vptCLmNB8",
   "metadata": {
    "id": "6v1vptCLmNB8"
   },
   "outputs": [],
   "source": [
    "d_list = [5, 10, 100]\n",
    "p_list = [0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "mat_complete_individuals = np.zeros((len(d_list), len(p_list)))\n",
    "\n",
    "for pos_d, d in enumerate(d_list):\n",
    "\n",
    "    ### Complete dataset\n",
    "    Mu = np.repeat(0, d)\n",
    "    Sigma = 0.5 * (np.ones((d, d)) + np.eye(d))\n",
    "    xfull = np.random.multivariate_normal(Mu, Sigma, size=n)\n",
    "\n",
    "    for pos_perc, p in enumerate(p_list):\n",
    "        vec_complete_individuals = compute_number_complete_individuals(n_sim=10, p=p, xfull=xfull)\n",
    "        mat_complete_individuals[pos_d, pos_perc] = round(np.mean(vec_complete_individuals) / (n*d)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iKQU1YOXogvf",
   "metadata": {
    "id": "iKQU1YOXogvf"
   },
   "source": [
    "In this table, we display the percentage of complete observations in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hsjq6_dkmYcY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1756854211952,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "hsjq6_dkmYcY",
    "outputId": "1812c527-32e8-45b5-b7f1-515da0df47ee"
   },
   "outputs": [],
   "source": [
    "percentage_complete_individuals = pd.DataFrame(mat_complete_individuals, index=[f\"d={d}\" for d in d_list], columns=[f\"p={p}\" for p in p_list])\n",
    "\n",
    "percentage_complete_individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rkgbMPOuQGdb",
   "metadata": {
    "id": "rkgbMPOuQGdb"
   },
   "source": [
    "# Exercise 2: ignorability of the missing-data mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1WqzkoPVd5eL",
   "metadata": {
    "id": "1WqzkoPVd5eL"
   },
   "source": [
    "In this exercise, you will illustrate the concept of ignorability of the missing data mechanism.\n",
    "\n",
    "In the Module 1 video, you saw that the missing data mechanism is ignorable if it is MCAR or MAR, and non-ignorable in the MNAR case. Recall that the mechanism is Missing At Random (MAR) if the missingness depends on the observed data values, and Missing Not At Random (MNAR) if the missingness can depend on all data values, including the missing ones.\n",
    "\n",
    "Let us consider bivariate Gaussian data, the same dataset as in Exercise 1 with $d=2$ variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KSNIDYfWr2G5",
   "metadata": {
    "id": "KSNIDYfWr2G5"
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 2\n",
    "Mu = np.repeat(0, d)\n",
    "Sigma = 0.5 * (np.ones((d, d)) + np.eye(d))\n",
    "\n",
    "xfull = np.random.multivariate_normal(Mu, Sigma, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2LyX2rbNr_Os",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1756854211990,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "2LyX2rbNr_Os",
    "outputId": "f81f9290-16ff-4a6f-efb0-ac7282bc8e09"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xfull).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1gK8YrLzQrms",
   "metadata": {
    "id": "1gK8YrLzQrms"
   },
   "outputs": [],
   "source": [
    "# Complete data scatter plot\n",
    "sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khT5eultZNZj",
   "metadata": {
    "id": "khT5eultZNZj"
   },
   "source": [
    "To generate MCAR missing values, we use the code from Exercise 1 (Question 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NMAKFHFNZTVd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1756854212009,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "NMAKFHFNZTVd",
    "outputId": "c3be25f8-4b97-46a6-9a8a-722ceff3098c"
   },
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "xmiss_mcar = np.copy(xfull)\n",
    "miss_id_mcar = np.random.uniform(0, 1, size=n) < p\n",
    "xmiss_mcar[miss_id_mcar, 1] = np.nan\n",
    "M_mcar = np.isnan(xmiss_mcar)\n",
    "print(\"The total percentage of NAs is:\", np.sum(M_mcar[:, 1]) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0qLvTVsY20la",
   "metadata": {
    "id": "0qLvTVsY20la"
   },
   "source": [
    "## Question 1: generating MAR and MNAR missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_aBQm1rr22G-",
   "metadata": {
    "id": "_aBQm1rr22G-"
   },
   "source": [
    "Let us consider that only the second variable contains missing values. Propose code to generate MAR and MNAR missing values using the following link function `logit`:\n",
    "$$\\mathrm{logit}(x)=1/(1+e^{-(ax+b)}),$$\n",
    "where $a \\in \\mathbb{R}$ et $b \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wGyzE6Oy3UxW",
   "metadata": {
    "id": "wGyzE6Oy3UxW"
   },
   "outputs": [],
   "source": [
    "def logit(x,coeff,intercept):\n",
    "\n",
    "  res = 1 / (1 + np.exp(-(coeff * x + intercept)))\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ENHP_CKXfGzq",
   "metadata": {
    "id": "ENHP_CKXfGzq"
   },
   "source": [
    "We can set $a=-4$ and $b=0$. At this stage, we are not aiming to precisely control the percentage of missing values generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7hHQqzKdfXwC",
   "metadata": {
    "id": "7hHQqzKdfXwC"
   },
   "outputs": [],
   "source": [
    "a = -4\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xum4uvDoHfRF",
   "metadata": {
    "id": "xum4uvDoHfRF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mDbm_qmOHhZl",
   "metadata": {
    "id": "mDbm_qmOHhZl"
   },
   "source": [
    "Let $X = (X_{.0} \\quad X_{.1})$ denote the dataset, where $X_{.0} = (x_{10}, \\dots, x_{n0})^T \\in \\mathbb{R}^n$ is the first variable and $X_{.1} = (x_{11}, \\dots, x_{n1})^T \\in \\mathbb{R}^n$ is the second variable. Similarly, the mask is $M = (M_{.0} \\quad M_{.1})$. The mechanism is:\n",
    "\n",
    "\n",
    "* MAR if $$\\mathbb{P}(M_{.1}|X)=\\mathrm{logit}(X_{.0}).$$\n",
    "In this case, the missingness of the second variable depends on the first variable, which is observed.\n",
    "* MNAR if $$\\mathbb{P}(M_{.1}|X)=\\mathrm{logit}(X_{.1}).$$\n",
    "In this case, the missingness of the second variable depends on its own values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sCAHOi2bcguE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1756854212075,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "sCAHOi2bcguE",
    "outputId": "6e8a880b-554e-4510-86c0-0ba9a0580083"
   },
   "outputs": [],
   "source": [
    "###Generation of MAR values\n",
    "\n",
    "xmiss_mar = np.copy(xfull)\n",
    "proba_mar = logit(xfull[:, 0], a, b)\n",
    "miss_id_mar = np.random.uniform(0, 1, size=n) < proba_mar\n",
    "xmiss_mar[miss_id_mar, 1] = np.nan\n",
    "M_mar = np.isnan(xmiss_mar)\n",
    "print(\"The percentage of NA in the second variable is:\", np.sum(M_mar[:, 1])/(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H7EAg0vweyky",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756854212079,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "H7EAg0vweyky",
    "outputId": "c12cd402-27cb-4a61-a346-6e832c8e5d6b"
   },
   "outputs": [],
   "source": [
    "###Generation of MNAR values\n",
    "\n",
    "xmiss_mnar = np.copy(xfull)\n",
    "proba_mnar = logit(xfull[:, 1], a, b)\n",
    "miss_id_mnar = np.random.uniform(0, 1, size=n) < proba_mnar\n",
    "xmiss_mnar[miss_id_mnar, 1] = np.nan\n",
    "M_mnar = np.isnan(xmiss_mnar)\n",
    "print(\"The percentage of NA in the second variable is:\", np.sum(M_mnar[:, 1])/(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-INbGi3kfnFK",
   "metadata": {
    "id": "-INbGi3kfnFK"
   },
   "source": [
    "We can also represent missing values on a scatter plot. We can clearly observe that:\n",
    "* for MCAR : the missingness does not depend on the data values; the missing values are present throughout the scatter plot.\n",
    "* for MAR : the missingness depends on the abscissa, that is, on the first variable $X_{.0}$ which is not missing.\n",
    "* in the MNAR case : the missingness depends on the ordinate, that is, on the second variable $X_{.1}$ which is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fe8-e9LvZi64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1756854212595,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "Fe8-e9LvZi64",
    "outputId": "3c457f7e-b339-4ded-b642-1c65e72d00fa"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], hue=M_mcar[:, 1], palette=['#d1e5f0', '#2166ac'])\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.set_title('MCAR')\n",
    "ax.set_xlabel(r'$X_{.0}$')\n",
    "ax.set_ylabel(r'$X_{.1}$')\n",
    "ax.legend(handles, ['Observed', 'Missing'], loc='lower right', fontsize='13')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3vxGnpBvC_8o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1756854213273,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "3vxGnpBvC_8o",
    "outputId": "e77e1cb7-bc77-42ce-8f81-a76b7597763c"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], hue=M_mar[:, 1], palette=['#d1e5f0', '#2166ac'])\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.set_title('MAR')\n",
    "ax.set_xlabel(r'$X_{.0}$')\n",
    "ax.set_ylabel(r'$X_{.1}$')\n",
    "ax.legend(handles, ['Observed', 'Missing'], loc='lower right', fontsize='13')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dlEXBUOc8iu7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1756854213679,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "dlEXBUOc8iu7",
    "outputId": "290f382d-5db7-4582-82d1-ef374e52bcfa"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], hue=M_mnar[:, 1], palette=['#d1e5f0', '#2166ac'])\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.set_title('MNAR')\n",
    "ax.set_xlabel(r'$X_{.0}$')\n",
    "ax.set_ylabel(r'$X_{.1}$')\n",
    "ax.legend(handles, ['Observed', 'Missing'], loc='lower right', fontsize='13')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdVdPlJX3BhG",
   "metadata": {
    "id": "pdVdPlJX3BhG"
   },
   "source": [
    "## Question 2: calculation of the bias of the empirical mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JDmxd1SR3QU3",
   "metadata": {
    "id": "JDmxd1SR3QU3"
   },
   "source": [
    "The empirical means of the second variable are calculated by removing the missing values. Interpret the following results. Is there a bias in the MCAR case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PGlq1qn9ZB4D",
   "metadata": {
    "id": "PGlq1qn9ZB4D"
   },
   "outputs": [],
   "source": [
    "empirical_mean = np.mean(xfull[:, 1], axis=0)\n",
    "empirical_mean_mcar = np.nanmean(xmiss_mcar[:, 1], axis=0)\n",
    "empirical_mean_mar = np.nanmean(xmiss_mar[:, 1], axis=0)\n",
    "empirical_mean_mnar = np.nanmean(xmiss_mnar[:, 1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rt-dIdKz3WKt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1756854213699,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "Rt-dIdKz3WKt",
    "outputId": "71a47028-9ecd-44e9-e153-35d9ab8e772c"
   },
   "outputs": [],
   "source": [
    "print(\"Empirical mean:\", f\"{empirical_mean:.3f}\")\n",
    "print(\"Empirical mean, MCAR:\", f\"{empirical_mean_mcar:.3f}\")\n",
    "print(\"Empirical mean, MAR:\", f\"{empirical_mean_mar:.3f}\")\n",
    "print(\"Empirical mean, MNAR:\", f\"{empirical_mean_mnar:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hHvNuJRVOQuq",
   "metadata": {
    "id": "hHvNuJRVOQuq",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jag-HyTjHhn-",
   "metadata": {
    "id": "Jag-HyTjHhn-"
   },
   "source": [
    "In the MCAR case, there is no bias. We have:\n",
    "$$\\mathbb{E}\\left[\\frac{1}{n_{\\textrm{obs}}}\\sum_{i=1}^n (1-M_{i1}) X_{i1}\\right]=\\mathbb{E}[X_{i1}],$$\n",
    "where $n_{\\textrm{obs}}$ is the number of observed values in $X_{i1}$. Indeed,\n",
    "$\\mathbb{E}\\left[\\frac{1}{n_{\\textrm{obs}}}\\sum_{i=1}^n (1-M_{i1}) X_{i1}\\right]=\\frac{n}{n_{\\textrm{obs}}}\\mathbb{E}[(1-M_{.1})]\\mathbb{E}[X_{.1}],$ car $M_{.1}$ and $X_{.1}$ are independent in the MCAR case. Finally, we have $\\mathbb{E}[(1-M_{.1})]=n_{\\textrm{obs}}/n$, since $M_{.1}$ is drawn from a Bernoulli distribution with parameter $p=(n-n_{\\textrm{obs}})/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_1fmvWniOSie",
   "metadata": {
    "id": "_1fmvWniOSie"
   },
   "source": [
    "In the MAR case, and even more so in the MNAR case, the empirical mean is biased.\n",
    "\n",
    "If we look at the scatter plots showing where the missing values occur, this observation was expected (see the solution to Question 1).\n",
    "In the MNAR case, most of the negative values of $X_{.1}$ are missing. As a result, the empirical mean is positive.\n",
    "In the MAR case, even though the missingness does not depend on the value of the variable $X_{.1}$ itself but rather on $X_{.0}$, the linear relationship between the two variables also implies that many negative values of $X_{.1}$ are missing. Therefore, the empirical mean is again positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etPdwml03Xio",
   "metadata": {
    "id": "etPdwml03Xio"
   },
   "source": [
    "## Question 3: calculation of the bias of the maximum likelihood estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XRJwAg0wTzvE",
   "metadata": {
    "id": "XRJwAg0wTzvE"
   },
   "source": [
    "The previous question involved calculating the empirical mean based on the observed values. We will now compute the maximum likelihood estimator. We will revisit in detail how to derive its expression in the lab session for Module 3 (Exercise 1, Question 1).\n",
    "\n",
    "This estimator depends on the empirical mean of $X_{.1}$; instead of using only the observed values of $X_{.2}$ (as in the empirical mean computed in Question 2), it uses all available values in the dataset and thus takes advantage of the relationship between the variables. This helps to better preserve the empirical distribution of the data.\n",
    "\n",
    "We will observe that this likelihood-based estimator yields unbiased results in the MCAR and MAR cases, but biased results in the MNAR case.\n",
    "\n",
    "Interpret the following results. Then, propose code to obtain the results over multiple simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covkts5XRYwh",
   "metadata": {
    "id": "covkts5XRYwh"
   },
   "outputs": [],
   "source": [
    "def maximum_likelihood_estimate(miss_id,xmiss):\n",
    "\n",
    "  mu0 = np.mean(xmiss[:, 0])\n",
    "\n",
    "  bar_x0 = np.mean(xmiss[~miss_id, 0])\n",
    "  bar_x1 = np.mean(xmiss[~miss_id, 1])\n",
    "  sig_0 = np.mean((xmiss[~miss_id, 0] - bar_x0) ** 2)\n",
    "  sig_01 = np.mean((xmiss[~miss_id, 0] - bar_x0) * (xmiss[~miss_id, 1] - bar_x1))\n",
    "  mu1 = np.mean(xmiss[~miss_id, 1]) + sig_01 / sig_0 * (mu0 - np.mean(xmiss[~miss_id, 0]))\n",
    "\n",
    "  return(mu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z66Pe-4tTNG2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1756854213736,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "Z66Pe-4tTNG2",
    "outputId": "69189221-f8bc-4002-b02a-7e11390e140c"
   },
   "outputs": [],
   "source": [
    "mle_mcar = maximum_likelihood_estimate(miss_id_mcar,xmiss_mcar)\n",
    "mle_mar = maximum_likelihood_estimate(miss_id_mar,xmiss_mar)\n",
    "mle_mnar = maximum_likelihood_estimate(miss_id_mnar,xmiss_mnar)\n",
    "\n",
    "print(\"Empirical mean without NA:\", f\"{empirical_mean:.3f}\")\n",
    "print(\"Maximum likelihood estimator, MCAR:\", f\"{mle_mcar:.3f}\")\n",
    "print(\"Maximum likelihood estimator, MAR:\", f\"{mle_mar:.3f}\")\n",
    "print(\"Maximum likelihood estimator, MNAR:\", f\"{mle_mnar:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DB3B2NVFT62N",
   "metadata": {
    "id": "DB3B2NVFT62N",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IW5GTNgwMq4e",
   "metadata": {
    "id": "IW5GTNgwMq4e"
   },
   "source": [
    "In the computation of the maximum likelihood estimator, the missing data mechanism was not taken into account. This is why the results are biased in the MNAR case.\n",
    "\n",
    "We can repeat the experiment over multiple simulations and display the boxplots of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xWY_pV3BM_Ps",
   "metadata": {
    "id": "xWY_pV3BM_Ps"
   },
   "outputs": [],
   "source": [
    "def compute_bias_mle(n_sim, p, a, b, xfull, Mu):\n",
    "\n",
    "    vec_norm2_bias_mcar = []\n",
    "    vec_norm2_bias_mar = []\n",
    "    vec_norm2_bias_mnar = []\n",
    "\n",
    "    n = xfull.shape[0]\n",
    "    d = xfull.shape[1]\n",
    "\n",
    "    empirical_mean = np.mean(xfull[:, 1])\n",
    "    bias = empirical_mean - Mu[1]\n",
    "    norm2_bias = (bias ** 2)\n",
    "\n",
    "    for it in range(n_sim):\n",
    "\n",
    "        ### Generation of missing values\n",
    "        xmiss_mcar = np.copy(xfull)\n",
    "        miss_id_mcar = np.random.uniform(0, 1, size=n) < p\n",
    "        xmiss_mcar[miss_id_mcar, 1] = np.nan\n",
    "\n",
    "        xmiss_mar = np.copy(xfull)\n",
    "        proba_mar = logit(xfull[:, 0], a, b)\n",
    "        miss_id_mar = np.random.uniform(0, 1, size=n) < proba_mar\n",
    "        xmiss_mar[miss_id_mar, 1] = np.nan\n",
    "\n",
    "        xmiss_mnar = np.copy(xfull)\n",
    "        proba_mnar = logit(xfull[:, 1], a, b)\n",
    "        miss_id_mnar = np.random.uniform(0, 1, size=n) < proba_mnar\n",
    "        xmiss_mnar[miss_id_mnar, 1] = np.nan\n",
    "\n",
    "        mle_mcar = maximum_likelihood_estimate(miss_id_mcar, xmiss_mcar)\n",
    "        mle_mar = maximum_likelihood_estimate(miss_id_mar, xmiss_mar)\n",
    "        mle_mnar = maximum_likelihood_estimate(miss_id_mnar, xmiss_mnar)\n",
    "\n",
    "        bias_mcar = mle_mcar - Mu\n",
    "        bias_mar = mle_mar - Mu\n",
    "        bias_mnar = mle_mnar - Mu\n",
    "\n",
    "        norm2_bias_mcar = (bias_mcar ** 2).sum()\n",
    "        norm2_bias_mar = (bias_mar ** 2).sum()\n",
    "        norm2_bias_mnar = (bias_mnar ** 2).sum()\n",
    "\n",
    "        vec_norm2_bias_mcar.append(norm2_bias_mcar)\n",
    "        vec_norm2_bias_mar.append(norm2_bias_mar)\n",
    "        vec_norm2_bias_mnar.append(norm2_bias_mnar)\n",
    "\n",
    "    return(norm2_bias, vec_norm2_bias_mcar, vec_norm2_bias_mar, vec_norm2_bias_mnar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I9dUHnAsPAvk",
   "metadata": {
    "id": "I9dUHnAsPAvk"
   },
   "outputs": [],
   "source": [
    "norm2_bias, vec_norm2_bias_mcar, vec_norm2_bias_mar, vec_norm2_bias_mnar = compute_bias_mle(n_sim=10, p=0.5, a=-4, b=0, xfull=xfull, Mu=Mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86p3sMb6O4en",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1756854214265,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "86p3sMb6O4en",
    "outputId": "79442358-efe4-4887-b1cd-426833616a63"
   },
   "outputs": [],
   "source": [
    "res_na = pd.DataFrame({\"MCAR\":vec_norm2_bias_mcar, \"MAR\":vec_norm2_bias_mar, \"MNAR\":vec_norm2_bias_mnar})\n",
    "ax = sns.boxplot(res_na)\n",
    "ax.set_title(\"Bias in the mean estimation\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-b75lNxf38Je",
   "metadata": {
    "id": "-b75lNxf38Je"
   },
   "source": [
    "# Exercise 3: challenges related to generating missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tLsQ_EBJ7lih",
   "metadata": {
    "id": "tLsQ_EBJ7lih"
   },
   "source": [
    "In this exercise, you will explore the challenges that can arise when attempting to generate missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mGZnm7WDDenM",
   "metadata": {
    "id": "mGZnm7WDDenM"
   },
   "source": [
    "Let us consider the Gaussian dataset from the previous exercises with $d = 3$ variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Rgoc2d7DYUQ",
   "metadata": {
    "id": "8Rgoc2d7DYUQ"
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 3\n",
    "Mu = np.repeat(0, d)\n",
    "Sigma = 0.5 * (np.ones((d, d)) + np.eye(d))\n",
    "\n",
    "xfull = np.random.multivariate_normal(Mu, Sigma, size=n) #complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SkanIrXREJ6X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756854214276,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "SkanIrXREJ6X",
    "outputId": "e6f7eabf-95c4-47fc-a043-79fdc1c2ba6a"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xfull).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Der6Czvj7cV4",
   "metadata": {
    "id": "Der6Czvj7cV4"
   },
   "source": [
    "## Question 1: percentage of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t-jUA9Dc7gVc",
   "metadata": {
    "id": "t-jUA9Dc7gVc"
   },
   "source": [
    "To generate MCAR-type missing values, it is straightforward to obtain a specific overall percentage of missing data. We saw how to proceed in the previous exercises, by drawing the missingness mask from a Bernoulli distribution with parameter $p$ (the probability of being missing). If we want 40% missing values in total, we can choose $p = 0.4$.\n",
    "\n",
    "When the goal is to generate MAR or MNAR-type missing values, things become more complicated.\n",
    "\n",
    "More specifically, suppose the objective is to generate MAR-type missing values in the second variable using the logistic function, such that the mechanism is defined as:\n",
    "\n",
    "$$\\mathbb{P}(M_{.1}|X=(X_{.0},X_{.1},X_{.2}))=1/(1+e^{-(a_0X_{.0}+a_2X_{.2}+b)}),$$\n",
    "with $a_0 \\in \\mathbb{R},\\ a_2 \\in \\mathbb{R},\\ b \\in \\mathbb{R}$.\n",
    "\n",
    "To control the proportion of missing values in the variable $X_{.1}$, one approach is to randomly choose the coefficients $a_0$ and $a_2$, and then adjust the value of $b$ accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bblw3VLBkHVE",
   "metadata": {
    "id": "bblw3VLBkHVE"
   },
   "source": [
    "How is the choice of the intercept $b$ adjusted if the following function `choose_intercept` is used? Generate MNAR missing values using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EV3UNwkDDRog",
   "metadata": {
    "id": "EV3UNwkDDRog"
   },
   "outputs": [],
   "source": [
    "def logit(x, coeff, intercept):\n",
    "\n",
    "  res = 1 / (1+np.exp(-(x.dot(coeff) + intercept)))\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nZ0DmFDdARTL",
   "metadata": {
    "id": "nZ0DmFDdARTL"
   },
   "outputs": [],
   "source": [
    "def choose_intercept(xfull, coeff, idx_var, p):\n",
    "\n",
    "    def f(x):\n",
    "        return logit(xfull[:, idx_var], coeff, x).mean().item() - p\n",
    "\n",
    "    intercepts = optimize.bisect(f, -50, 50)\n",
    "\n",
    "    return intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kF21_evXG-qf",
   "metadata": {
    "id": "kF21_evXG-qf"
   },
   "outputs": [],
   "source": [
    "idx_var = [0, 2]\n",
    "coeff = np.random.normal(size=len(idx_var))\n",
    "intercept = choose_intercept(xfull, coeff, idx_var, p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H0D5TIK6HeuD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1756854214339,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "H0D5TIK6HeuD",
    "outputId": "7f7adc0a-9273-43d1-95b6-9708ffe7c9ac"
   },
   "outputs": [],
   "source": [
    "print(\"The chosen coefficients are:\", coeff)\n",
    "print(\"The chosen intercept is:\", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A9JKGrQBbZuM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1756854214394,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "A9JKGrQBbZuM",
    "outputId": "ddfd165b-dc3a-4e22-b726-8e2adcfe6e26"
   },
   "outputs": [],
   "source": [
    "###Generation of MAR values\n",
    "\n",
    "xmiss_mar = np.copy(xfull)\n",
    "proba_mar = logit(xfull[:, idx_var], coeff, intercept)\n",
    "miss_id_mar = np.random.uniform(0, 1, size=n) < proba_mar\n",
    "xmiss_mar[miss_id_mar, 1] = np.nan\n",
    "M_mar = np.isnan(xmiss_mar)\n",
    "print(\"The percentage of NA in the second variable is:\", np.sum(M_mar[:, 1]) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S936ujrjncTg",
   "metadata": {
    "id": "S936ujrjncTg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rXBgE9dgnd6c",
   "metadata": {
    "id": "rXBgE9dgnd6c"
   },
   "source": [
    "We choose $b$ such that, on average, the probability of being missing for a value of the variable $X_{.1}$ equals $p$. This is therefore an optimization problem, where we seek the root of the following function:\n",
    "\n",
    "$f(x)=\\frac{1}{n}\\sum_{i=1}^n 1/(1+e^{-(a_0X_{i0}+a_2X_{i2}+b)})-p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fnVUSC_07rK",
   "metadata": {
    "id": "0fnVUSC_07rK"
   },
   "source": [
    "To generate MNAR missing values, we can consider the following mechanism:\n",
    "$$\\mathbb{P}(M_{.1}|X)=\\mathrm{logit}(X_{.1}).$$\n",
    "Here is the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IzSnyV4G0h1A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756854214395,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "IzSnyV4G0h1A",
    "outputId": "155191d3-bec6-444b-c26a-229d96e2991b"
   },
   "outputs": [],
   "source": [
    "### MNAR case\n",
    "idx_var = [1]\n",
    "coeff = np.random.normal(size=len(idx_var))\n",
    "intercept = choose_intercept(xfull, coeff, idx_var, p=0.4)\n",
    "\n",
    "xmiss_mnar = np.copy(xfull)\n",
    "proba_mnar = logit(xfull[:, idx_var], coeff, intercept)\n",
    "miss_id_mnar = np.random.uniform(0, 1, size=n) < proba_mnar\n",
    "xmiss_mnar[miss_id_mnar, 1] = np.nan\n",
    "M_mnar = np.isnan(xmiss_mnar)\n",
    "print(\"The percentage of NA in the second variable is:\", np.sum(M_mnar[:, 1]) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7udG2-Na1QUk",
   "metadata": {
    "id": "7udG2-Na1QUk"
   },
   "source": [
    "We can quickly verify the generation of missing values using plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YW74yLnT1Wur",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1756854215062,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "YW74yLnT1Wur",
    "outputId": "e588084a-d701-462e-8a0c-89d056aad339"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], hue=M_mar[:, 1], palette=['#d1e5f0', '#2166ac'])\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.set_title('MAR')\n",
    "ax.set_xlabel(r'$X_{.0}$')\n",
    "ax.set_ylabel(r'$X_{.1}$')\n",
    "ax.legend(handles, ['Observed', 'Missing'], loc='lower right', fontsize='13')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "USLUC-2Q1vdT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1756854215643,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "USLUC-2Q1vdT",
    "outputId": "8e309154-7219-4dfb-987f-60923543e483"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], hue=M_mnar[:, 1], palette=['#d1e5f0', '#2166ac'])\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.set_title('MNAR')\n",
    "ax.set_xlabel(r'$X_{.0}$')\n",
    "ax.set_ylabel(r'$X_{.1}$')\n",
    "ax.legend(handles, ['Observed', 'Missing'], loc='lower right', fontsize='13')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFnzDzlWoa7p",
   "metadata": {
    "id": "SFnzDzlWoa7p"
   },
   "source": [
    "## Question 2: specificity of the MAR case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U8ECh4GKpyAd",
   "metadata": {
    "id": "U8ECh4GKpyAd"
   },
   "source": [
    "The specificity of the MAR case is that the missingness depends on observed values of the data.\n",
    "\n",
    "In the MNAR case, we can generate missing values in each variable, for example by using the logistic function as follows:\n",
    "$$ \\forall j \\in \\{1,\\dots,d\\}, \\mathbb{P}(M_{.j}|X)=1/(1+e^{-(aX_{.j}+b)}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Oar3EmxMpk8Z",
   "metadata": {
    "id": "Oar3EmxMpk8Z"
   },
   "source": [
    "In the MAR case, should certain variables be considered fully observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QWIDMwrgqvmt",
   "metadata": {
    "id": "QWIDMwrgqvmt",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etizCnM0qxll",
   "metadata": {
    "id": "etizCnM0qxll"
   },
   "source": [
    "The vast majority of codes for generating missing values consider one or more variables as fully observed. However, this is not necessary to simulate MAR values.\n",
    "\n",
    "The original definition of the MAR mechanism considers vectorized quantities in $\\mathbb{P}(M \\mid X_{\\mathrm{obs}(M)})$, meaning that $M$ is a vector of size $n \\times d$ and $X_{\\mathrm{obs}(M)}$ is a vector of size equal to the number of observed values in $X$.\n",
    "\n",
    "In fact, this vectorized representation corresponds to simulating missing values by row (pattern or missingness patterns). With three variables, it is perfectly possible to have MAR missing values in all variables, for example with the following patterns and mechanisms.\n",
    "\n",
    "Patterns:\n",
    "- $i \\in \\mathrm{Pattern}_1$ if $M_{i.}=(1,0,0)$, that is only the first variable is missing.\n",
    "- $i \\in \\mathrm{Pattern}_2$ if $M_{i.}=(0,1,0)$, that is only the second variable is missing\n",
    "- $i \\in \\mathrm{Pattern}_3$ if $M_{i.}=(0,0,1)$, that is only the third mechanism is missing\n",
    "\n",
    "Mechanisms:\n",
    "- $i \\in \\mathrm{Pattern}_1, \\mathbb{P}(M_{i1}|X)=\\mathrm{logit}(X_{i2},X_{i3})$,\n",
    "- $i \\in \\mathrm{Pattern}_2, \\mathbb{P}(M_{i2}|X)=\\mathrm{logit}(X_{i1},X_{i2})$,\n",
    "- $i \\in \\mathrm{Pattern}_3, \\mathbb{P}(M_{i3}|X)=\\mathrm{logit}(X_{i1},X_{i2})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DkTaZ1dN50z3",
   "metadata": {
    "id": "DkTaZ1dN50z3"
   },
   "source": [
    "## Question 3: use of the `pyampute` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IOpK1ZMN5-k_",
   "metadata": {
    "id": "IOpK1ZMN5-k_"
   },
   "source": [
    "To generate missing values by pattern, you can use the `pyampute` library. Documentation is available [here](https://rianneschouten.github.io/pyampute/build/html/index.html). The `MultivariateAmputation` function allows you to generate missing values in a (initially complete) dataset. There are two main arguments:\n",
    "* `prop`: the proportion of missing values per variable,\n",
    "* `patterns`: a list of dictionaries that notably include the following entries:\n",
    "  * `incomplete_vars`: ndices of the variables with missing values\n",
    "  * `weights`: weights on the variables that will influence the missingness\n",
    "  * `mechanism`: missing data mechanism\n",
    "  * `freq`: frequency of the pattern in the amputed dataset\n",
    "\n",
    "  Each dictionary corresponds to the description of a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jKOlnYD77BqZ",
   "metadata": {
    "id": "jKOlnYD77BqZ"
   },
   "source": [
    "Use the `MultiviriateAmputation` function to generate MAR missing values as specified in the solution to the previous question, with pattern frequencies of 10%, 50%, and 40%, respectively. Are the results consistent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HYK-rpgkdYhL",
   "metadata": {
    "id": "HYK-rpgkdYhL"
   },
   "source": [
    "Warning, the `pyampute` library has not been maintained since 2022. The pattern visualization function (Python module `pyampute.exploration`) returns an error; the following code can be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LFaXEqoRaaUv",
   "metadata": {
    "id": "LFaXEqoRaaUv"
   },
   "outputs": [],
   "source": [
    "def plot_patterns(res):\n",
    "\n",
    "  #### res is a DataFrame containing all possible missing-data patterns of an incomplete dataset\n",
    "  #### Example: res = np.unique(M,axis=0), with M the mask\n",
    "\n",
    "  myred = \"#B61A51B3\"\n",
    "  myblue = \"#006CC2B3\"\n",
    "  cmap = colors.ListedColormap(['#d1e5f0', '#2166ac'])\n",
    "\n",
    "  fig, ax = plt.subplots(1)\n",
    "  ax.imshow(res.astype(bool), aspect=\"auto\", cmap=cmap)\n",
    "\n",
    "\n",
    "  ax.set_yticks(np.arange(0, len(res.index), 1))\n",
    "  ax.set_yticks(np.arange(-0.5, len(res.index), 1), minor=True)\n",
    "  ax.set_xticks(np.arange(0, len(res.columns), 1))\n",
    "  ax.set_xticks(np.arange(-0.5, len(res.columns), 1), minor=True)\n",
    "\n",
    "\n",
    "  ax.set_xticklabels([k for k in res.columns])\n",
    "  ax.set_yticklabels([k for k in res.index])\n",
    "  ax.grid(which=\"minor\", color=\"w\", linewidth=1)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgDAEG1r7ZZJ",
   "metadata": {
    "id": "pgDAEG1r7ZZJ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ixrwMPgW7btr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1756854215661,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "ixrwMPgW7btr",
    "outputId": "6ad78d04-2ff2-4937-8054-31a3ff3293ca"
   },
   "outputs": [],
   "source": [
    "pattern1 = {\"incomplete_vars\": [0], \"mechanism\": \"MAR\", \"freq\":0.1}\n",
    "pattern2 = {\"incomplete_vars\": [1], \"mechanism\": \"MAR\", \"freq\":0.5}\n",
    "pattern3 = {\"incomplete_vars\": [2], \"mechanism\": \"MAR\", \"freq\":0.4}\n",
    "patterns = [pattern1, pattern2, pattern3]\n",
    "\n",
    "\n",
    "ma = pyampute.ampute.MultivariateAmputation(prop=0.9, patterns=patterns)\n",
    "xmiss = ma.fit_transform(xfull)\n",
    "M = np.isnan(xmiss)\n",
    "print(\"The total percentage of NAs is:\", np.sum(M)/(n*d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oDwosdWMVQ7F",
   "metadata": {
    "id": "oDwosdWMVQ7F"
   },
   "source": [
    "The argument `prop` is indeed the proportion of rows containing at least one missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wB1iFZLgVYIL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756854215668,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "wB1iFZLgVYIL",
    "outputId": "58196990-5180-4c3c-8f75-f572d123e91d"
   },
   "outputs": [],
   "source": [
    "x_cc = pd.DataFrame(xmiss).dropna()\n",
    "print(\"The percentage of incomplete rows is:\", x_cc.shape[0] / n * 100, \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Po6zd7E7u1A",
   "metadata": {
    "id": "1Po6zd7E7u1A"
   },
   "source": [
    "We can verify that the frequency of the missingness patterns has been properly respected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HDPru2qvW9ZJ",
   "metadata": {
    "id": "HDPru2qvW9ZJ"
   },
   "outputs": [],
   "source": [
    "### Visualisation of the missing-data patterns\n",
    "\n",
    "which_patterns, counts_patterns = np.unique(M, axis=0, return_counts=True)\n",
    "\n",
    "res = pd.DataFrame(which_patterns * 1, columns=[\"X0\", \"X1\", \"X2\"], index=[\"Complete row\", \"Pattern1\", \"Pattern2\", \"Pattern3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJWrxt0ea_l-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1756854215865,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "yJWrxt0ea_l-",
    "outputId": "1b54955b-011c-4cc5-aff2-ddf94d75c9fb"
   },
   "outputs": [],
   "source": [
    "plot_patterns(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l0Tdbj7LdI66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1756854215898,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "l0Tdbj7LdI66",
    "outputId": "c20faf39-8f61-4480-bf49-cec1e2ece820"
   },
   "outputs": [],
   "source": [
    "### Frequency of the missing-data patterns\n",
    "\n",
    "res[\"Percentage\"] = counts_patterns / n * 100\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WuxT1Ze26his",
   "metadata": {
    "id": "WuxT1Ze26his"
   },
   "source": [
    "# Exercise 4: pseudo-realistic mechanisms in a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fwlZZGc4Ll7",
   "metadata": {
    "id": "0fwlZZGc4Ll7"
   },
   "source": [
    "In this exercise, you will consider a real dataset, Breast Cancer Wisconsin, available from the UCI repository [here](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n",
    ", which initially contains no missing values. The variables are calculated from images of breast tumors. More specifically, they describe each cell nucleus with ten measurements (radius, texture, perimeter, area, etc.). Finally, the 30 variables available in the dataset correspond to the mean of the measurements over the nuclei, the standard error, and the worst measurement (in the sense of the largest value, which is most likely to indicate a malignant tumor diagnosis).\n",
    "\n",
    "Generally, this dataset is used for prediction purposes to classify patients according to tumor type: malignant or benign.\n",
    "\n",
    "The goal of this exercise is to generate missing values with a *pseudo-realistic* pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TT7vizG_Xsfh",
   "metadata": {
    "id": "TT7vizG_Xsfh"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "xfull = data['data']  # covariates, without missing values\n",
    "diagnosis = data['target']  # target variable to predict, when the learning task is prediction\n",
    "features_names = data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sl7qp3fuAv4E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1756854216015,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "sl7qp3fuAv4E",
    "outputId": "8db7feb1-b3f9-4cae-f37b-cea2b265d83e"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xfull, columns=features_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kk6mSmOMouQm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1756854216016,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "Kk6mSmOMouQm",
    "outputId": "7a8f46e5-a3a9-41ec-c02c-83910f81acb9"
   },
   "outputs": [],
   "source": [
    "features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4GeV3VMnA6Fk",
   "metadata": {
    "id": "4GeV3VMnA6Fk"
   },
   "outputs": [],
   "source": [
    "n, d = xfull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RJl61H8PBesA",
   "metadata": {
    "id": "RJl61H8PBesA"
   },
   "source": [
    "## Question 1: generation of a basic MCAR mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YLKLoDvBYXB6",
   "metadata": {
    "id": "YLKLoDvBYXB6"
   },
   "source": [
    "Generate MCAR missing values across all variables, with a missingness probability of $p = 0.3$ for the first 10 variables (corresponding to the means of the measurements), $p = 0.6$ for the next 10 (standard errors), and $p = 0.8$ for the last 10 (worst measurements). Explain why this mechanism is truly MCAR.\n",
    "\n",
    "This is not the most realistic missingness scenario. One can imagine that the values are manually recorded by three different people, and depending on their diligence and available time (which are independent of the data values), there are more or fewer missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g3lSEye5Lf_2",
   "metadata": {
    "id": "g3lSEye5Lf_2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uXU0UcN2Pjan",
   "metadata": {
    "id": "uXU0UcN2Pjan"
   },
   "source": [
    "The mechanism is indeed MCAR here because the probability of a value being missing does not depend on the data values. Having different probabilities of missingness across variables is not incompatible with the MCAR mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GrKHT2VUpRoS",
   "metadata": {
    "id": "GrKHT2VUpRoS"
   },
   "outputs": [],
   "source": [
    "p = [0.3, 0.6, 0.8]\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(10):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p[0]\n",
    "  xmiss[miss_id, j] = np.nan\n",
    "for j in range(10, 20):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p[1]\n",
    "  xmiss[miss_id, j] = np.nan\n",
    "for j in range(20, 30):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p[2]\n",
    "  xmiss[miss_id, j] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l160vpNQFWt0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1756854216066,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "l160vpNQFWt0",
    "outputId": "b3bd1cc2-273c-4ce3-e50e-f861cd5c41de"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xmiss,columns=features_names).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GehBhqzEL2o2",
   "metadata": {
    "id": "GehBhqzEL2o2"
   },
   "source": [
    "## Question 2: using the `missingno` visualization library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iUicz4yjMGRl",
   "metadata": {
    "id": "iUicz4yjMGRl"
   },
   "source": [
    "The `missingno` library is a Python visualization library for handling missing data. Documentation is available [here](https://github.com/ResidentMario/missingno).\n",
    ".\n",
    "\n",
    "Use the `matrix` and `bar` functions from the `missingno` library to visualize the amputed dataset `xmiss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kv68qROpMxbN",
   "metadata": {
    "id": "kv68qROpMxbN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xHMGeZTHNBSq",
   "metadata": {
    "id": "xHMGeZTHNBSq"
   },
   "source": [
    "The `matrix` function provides a visualization of the missingness patterns in `xmiss`. We observe that the first 10 variables have more observed values (black squares) than the next 10, which in turn have more observed values than the last 10. This was expected, given the missing value generation in Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X0DDWjonFfmA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1756854217193,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "X0DDWjonFfmA",
    "outputId": "2758ce35-1ba8-4b86-ec83-8f88856dfc94"
   },
   "outputs": [],
   "source": [
    "missingno.matrix(pd.DataFrame(xmiss)) #global visualisation of missing-data patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F8R5Zl9CN865",
   "metadata": {
    "id": "F8R5Zl9CN865"
   },
   "source": [
    "The `bar` function allows visualization of the number of observed values per variable (at the top) and the percentage of observed values per variable (on the left y-axis).\n",
    "\n",
    "The results remain consistent with the missing value generation from Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CpGJespqNhJZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 1620,
     "status": "ok",
     "timestamp": 1756854218817,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "CpGJespqNhJZ",
    "outputId": "b63bdc79-5150-4ed2-98c2-a59484836a72"
   },
   "outputs": [],
   "source": [
    "missingno.bar(pd.DataFrame(xmiss))\n",
    "#percentage of observed values, and number of observed values per variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tqqAJgL3RmF_",
   "metadata": {
    "id": "tqqAJgL3RmF_"
   },
   "source": [
    "## Question 3: generation of a mask with dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SUnJmOK6Rr8R",
   "metadata": {
    "id": "SUnJmOK6Rr8R"
   },
   "source": [
    "Now consider that the first 10 variables are missing with probability $p = 0.3$. Furthermore, suppose that if the first variable is missing, then the 11th variable is also missing; if the second variable is missing, then the 12th is missing as well, and so on. In fact, referring back to the example in Question 1 where the values were manually recorded, we can assume there were only two people. The first person either recorded the mean values (variables 1 to 10) and the standard error values (variables 11 to 20), or neither of these sets. The second person recorded all the values (variables 21 to 30).\n",
    "\n",
    "Generate the mask corresponding to this scenario. Use the `heatmap` function from `missingno` to visualize the influence of the presence of the first ten variables on the presence of the next ten. Does the mechanism remain MCAR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rktQOagWTZSO",
   "metadata": {
    "id": "rktQOagWTZSO",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHcyk5weUeVJ",
   "metadata": {
    "id": "ZHcyk5weUeVJ"
   },
   "source": [
    "Using the `heatmap` function, we observe that the presence of the first 10 variables is directly linked (with a correlation of 1) to the presence of the next 10 variables, as expected from the mask generation.\n",
    "\n",
    "The missing data mechanism remains MCAR. The probability of being missing for each value does not depend on the data values. Here, we introduced a dependency between the masks $M_{.,j}$ and $M_{.,j+10})$ for $j=0,\\dots,9$, but the mask remains independent of the data values, that is, $\\mathbb{P}(M|X)=\\mathbb{P}(M)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ERnI99DvTa4_",
   "metadata": {
    "id": "ERnI99DvTa4_"
   },
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(10):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p\n",
    "  xmiss[miss_id, j] = np.nan\n",
    "  xmiss[miss_id, j+10] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KH5FxaFNUQz-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1756854220632,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "KH5FxaFNUQz-",
    "outputId": "2bc31591-79ec-4d53-c48e-0a7932db90a8"
   },
   "outputs": [],
   "source": [
    "missingno.heatmap(pd.DataFrame(xmiss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Rl3otJoDHVD",
   "metadata": {
    "id": "8Rl3otJoDHVD"
   },
   "source": [
    "## Question 4: Case of a mask dependent on the diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blxjcbN-DPmf",
   "metadata": {
    "id": "blxjcbN-DPmf"
   },
   "source": [
    "A common practical scenario occurs when an individual has more or fewer missing values depending on the group they belong to. Here, we can imagine that patients with a benign tumor have more missing values because the images are of lower quality or come from patients with another pathology, and doctors do not necessarily retake images for these patients.\n",
    "\n",
    "Generate missing values under this missingness scenario. What type of missing data mechanism is this, depending on whether the variable `diagnosis` (indicating if the tumor is malignant with a `0` or benign with a `1`) is observed or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ce9_BjbqDTLk",
   "metadata": {
    "id": "Ce9_BjbqDTLk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ezjvnO_YlcL3",
   "metadata": {
    "id": "ezjvnO_YlcL3"
   },
   "source": [
    "The mechanism is MAR if the variable `diagnosis` is fully observed, and MNAR if it is completely missing (and thus latent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O_XhnGaKXkL7",
   "metadata": {
    "id": "O_XhnGaKXkL7"
   },
   "outputs": [],
   "source": [
    "p_benign = 0.7  # probability of being missing for the values of the population with a benign tumor\n",
    "p_malign = 0.1  # probability of being missing for the values of the population with a malignant tumor\n",
    "\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "  benign_idx = np.where(diagnosis == 1)[0]\n",
    "  miss_id = np.random.uniform(0, 1, size=benign_idx.shape[0]) < p_benign\n",
    "  xmiss[benign_idx[miss_id], j] = np.nan\n",
    "\n",
    "for j in range(d):\n",
    "  malign_idx = np.where(diagnosis == 0)[0]\n",
    "  miss_id = np.random.uniform(0, 1, size=malign_idx.shape[0]) < p_malign\n",
    "  xmiss[malign_idx[miss_id], j] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yfa4Rh6KZH-y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1756854220650,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "Yfa4Rh6KZH-y",
    "outputId": "78e278d0-2816-42db-ef4e-32826d5ffb9e"
   },
   "outputs": [],
   "source": [
    "M = np.isnan(xmiss)\n",
    "print(\"The total percentage of NAs is in the healthy population:\", np.sum(M[diagnosis == 1, :]) / (sum(diagnosis == 1) *d))\n",
    "print(\"The total percentage of NAs is in the sick population:\", np.sum(M[diagnosis == 0, :]) / (sum(diagnosis == 0) *d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wr8EVddOxpzE",
   "metadata": {
    "id": "wr8EVddOxpzE"
   },
   "source": [
    "### Note: Amputation on an incomplete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_p94lqlTPNmJ",
   "metadata": {
    "id": "_p94lqlTPNmJ"
   },
   "source": [
    "This practical session does not address the case of amputation on an incomplete dataset that already contains *native* missing values. In this case, the problem is that there is neither a reference score nor a way to directly compare imputation methods by calculating the imputation error. One solution is to introduce new missing values. It is then relevant to generate them according to the distribution of the native missing values. This is challenging because it requires estimating the distribution $p(MX)$, and thus knowing whether the mechanism is MCAR, MAR, or MNAR. A first step is to respect the same patterns as those of the native missing values. The new missing values can be introduced on complete rows of the dataset if possible, or by completing already existing patterns.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1HDLmb6OqHO19Mj8gw_yuXC2c4jT3dySk",
     "timestamp": 1756817969255
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "semipy-venv",
   "language": "python",
   "name": "semipy-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
