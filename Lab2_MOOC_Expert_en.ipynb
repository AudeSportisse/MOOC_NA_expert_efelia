{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3zlLT4JzZo7"
   },
   "source": [
    "# Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy38Xm1bzam7"
   },
   "source": [
    "This second practical session will allow you to work with the imputation methods presented in Session 2, namely:\n",
    "* mean imputation,\n",
    "* k-nearest neighbors imputation,\n",
    "* iterative imputation.\n",
    "\n",
    "The first exercise is more theoretical and helps develop an intuition for how the methods work. Exercises 2 and 3 are more practical.\n",
    "\n",
    "**Note** : Imputation can have different objectives. Here, you will study methods aimed at minimizing imputation error by always choosing the most probable values. This is a perfectly valid goal, but these methods have the drawback of distorting the data distribution, notably by reducing the variance. Therefore, they are not suitable when the goal is to estimate the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llHpZvLmC2MC"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qnNR2u2C8RH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyLGu6VsxIfP",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Libraries imported in the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXg6YCx7xMKV"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYkHK6mH1sWM"
   },
   "source": [
    "# Exercise 1: Basic applications of imputation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3Y2LiBiaeHP"
   },
   "source": [
    "In this exercise, you will use synthetic two-dimensional data to facilitate the visualization of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUrrqNhSuFK2"
   },
   "source": [
    "## Question 1 : Bivariate Gaussian sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7raSkPRuGqB"
   },
   "source": [
    "Generate a bivariate sample (*i.e.* with $d=2$ variables), Gaussian,\n",
    "\n",
    "$$\n",
    "\\left( X_{i.} \\right)_{1\\leq i\\leq n}\n",
    "=\\left( X_{i0}, X_{i1} \\right)_{1\\leq i\\leq n}\n",
    "$$\n",
    "\n",
    "of size $n=500$, with mean $\\mu$ and covariance matrix $\\Sigma$:\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\n",
    "  \\mu_0 \\\\[6pt] \\mu_1\n",
    "\\end{bmatrix}, \\quad\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "  \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\[6pt]\n",
    "  \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "with the following values :\n",
    "$$\n",
    "\\mu_0 = 0,~\\mu_1 = 0,~\\sigma_0 = 1,~\\sigma_1 = 0.7,~\\rho = 0.8\n",
    "$$\n",
    "\n",
    "Save the sample in a variable `xfull`.\n",
    "\n",
    "Plot the sample using a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuCIfN2byT0D",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1750879439610,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "Csd4WDtxDWe8",
    "outputId": "9fb4944b-c511-4e55-8edf-26bc3eb6bf04"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n = 500\n",
    "d = 2\n",
    "mu0 = 0.\n",
    "mu1 = 0.\n",
    "sig0 = 1.\n",
    "sig1 = 0.7\n",
    "rho = 0.8\n",
    "\n",
    "mean = np.array([mu0, mu1])\n",
    "cov = np.array([\n",
    "    [sig0 ** 2, rho * sig0 * sig1],\n",
    "    [rho * sig0 * sig1, sig1 ** 2]\n",
    "    ])\n",
    "\n",
    "xfull = np.random.multivariate_normal(mean, cov, size=n)\n",
    "\n",
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], color=['#d1e5f0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-q8XGIIvTTx"
   },
   "source": [
    "## Question 2 : generating missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HavvXoT5B3a"
   },
   "source": [
    "### Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFnvB8eCeiIl"
   },
   "source": [
    "Run the following cell to generate missing values as in Notebook 1, and obtain the amputed dataset, `xmiss`. What is the missing data mechanism here? What does the variable `p` represent? How many possible patterns, i.e. possible NA combinations, are there for a row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1750879439651,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "QE6b3fzts-6n",
    "outputId": "3d11e21a-cb52-4d7d-edb1-6584fd27d45f"
   },
   "outputs": [],
   "source": [
    "p = 0.4\n",
    "\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "  miss_id = (np.random.uniform(0, 1, size=n) < p)\n",
    "  xmiss[miss_id, j] = np.nan\n",
    "\n",
    "display(pd.DataFrame(xmiss).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUwBouD1vfhk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_OivtvRvg_N"
   },
   "source": [
    "The mechanism is MCAR because the missingness `miss_id` is independent of `xfull`.\n",
    "\n",
    "`p` represents the probability for each variable to be missing (note that we could choose a different `p_j` for each variable and still have MCAR).\n",
    "\n",
    "There are 4 possible patterns :\n",
    "\n",
    "* Both $X_0,~X_1$ are observed,\n",
    "* Only $X_0$ is missing,\n",
    "* Only $X_1$ is missing,\n",
    "* Both $X_0,~X_1$ are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNDn2sQAv0ta"
   },
   "source": [
    "### Question 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gocs0lT9tBxp"
   },
   "source": [
    "Run the following cell to visualize `xmiss`. Interpret the plot: what does each group of points represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1750879439985,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "tpX6DZPkDtn-",
    "outputId": "87dafb86-7238-4437-dbd7-54273aed5574"
   },
   "outputs": [],
   "source": [
    "where_full = ~np.isnan(xmiss[:, 0]) & ~np.isnan(xmiss[:, 1])\n",
    "where_na0 = np.isnan(xmiss[:, 0]) & ~np.isnan(xmiss[:, 1])\n",
    "where_na1 = np.isnan(xmiss[:, 1]) & ~np.isnan(xmiss[:, 0])\n",
    "where_na01 = np.isnan(xmiss[:, 0]) & np.isnan(xmiss[:, 1])\n",
    "\n",
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'], label=\"?\")\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=xmin, y=xmiss[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False, label=\"?\")\n",
    "_ = sns.scatterplot(x=xmiss[where_na1, 0], y=ymin, color=['#2138ac'], ax=ax, clip_on=False, label=\"?\")\n",
    "_ = sns.scatterplot(x=[xmin], y=[ymin], color=['#ac6721'], ax=ax, clip_on=False, label=\"?\")\n",
    "\n",
    "_ = ax.set_xlim(xmin, xmax)\n",
    "_ = ax.set_ylim(ymin, ymax)\n",
    "\n",
    "_ = ax.set_xlabel(r'$X_0$')\n",
    "_ = ax.set_ylabel(r'$X_1$')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm3WPcnguSQm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z86NSbpnuTvP"
   },
   "source": [
    "In the first group, both variables $X_{.0}$ et $X_{.1}$are observed, so the points are displayed normally.\n",
    "\n",
    "In the second group, variable $X_{.0}$ is missing, and we only have access to $X_{.1}$. In this case, the points are displayed along the vertical axis, since we cannot place them horizontally.\n",
    "\n",
    "Conversely, in the third group, variable $X_{.1}$ is missing, so the points are displayed along the horizontal axis, as we cannot place them vertically.\n",
    "\n",
    "Finally, the points in the last group, for which both variables are missing, are displayed in the bottom-left corner.\n",
    "\n",
    "These four cases correspond to the four possible *patterns*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1SvkewBweKs"
   },
   "source": [
    "## Question 3 : mean imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI9CQNtgwiJt"
   },
   "source": [
    "The simplest possible imputation is mean imputation. Using the `sklearn.impute` module, replace the missing values in `xmiss` with the mean of each variable, and visualize the imputed dataset by replicating the type of plot used in Question 2b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pldCDBz4w54d",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6v55d_Jw8Fw"
   },
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "ximp_mean = mean_imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1750879440479,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "DqPOoqERzV4h",
    "outputId": "20ece7fa-0934-4fed-e9ee-3b075614dd76"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'], label=\"Complete\")\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_mean[where_na0, 0], y=ximp_mean[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False, label=r\"$X_0$ imputed\")\n",
    "_ = sns.scatterplot(x=ximp_mean[where_na1, 0], y=ximp_mean[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False, label=r\"$X_1$ imputed\")\n",
    "_ = sns.scatterplot(x=ximp_mean[where_na01, 0], y=ximp_mean[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False, label=r\"$X_0,X_1$ imputed\")\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);\n",
    "\n",
    "_ = ax.set_xlabel(r'$X_0$')\n",
    "_ = ax.set_ylabel(r'$X_1$')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oene59ds3rhT"
   },
   "source": [
    "## Question 4: Minimization of the theoretical imputation error (difficult question requiring knowledge of the conditional expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u49P3IWg3wBD"
   },
   "source": [
    "For a value $X_{i.} = \\left( X_{i0}, X_{i1} \\right)$ of the sample (the real data), we define its squared imputation error $E_i$ as follows:\n",
    "\n",
    "$$\n",
    "E_i\n",
    "= \\lVert X_{i.} - \\hat X_{i.} \\rVert^2\n",
    "= \\left(X_{i0} - \\hat X_{i0} \\right)^2\n",
    "  + \\left(X_{i1} - \\hat X_{i1} \\right)^2\n",
    "$$\n",
    "\n",
    "where $\\hat X_{i.}$ is the imputed value.\n",
    "\n",
    "The mean squared imputation error $E$ on the sample is given by:\n",
    "\n",
    "$$E = \\frac{1}{n} \\sum_{i=1}^{n} E_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaIq-7xoYAiy"
   },
   "source": [
    "### Question 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8siot9kfVl_5"
   },
   "source": [
    "Compute the mean squared error of the mean imputation from Question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngEBBEYYVynb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LjJqrY5XE0Z"
   },
   "outputs": [],
   "source": [
    "def mse(x_imp, x_true):\n",
    "  n = len(x_true)\n",
    "  return (1 / n) * np.sum((x_imp - x_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1750879440551,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "aBAJYmiwXbBI",
    "outputId": "3c40b7e7-73f9-4a9f-db8a-88e00a712899"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_mean, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKFvBaZr5WiK"
   },
   "source": [
    "### Question 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wNblVrZ5ays"
   },
   "source": [
    "In this bivariate case, how does the error $E_i$ simplify depending on the pattern at row $i$, that is, depending on which variables are missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrhsM2_76e6t",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfqTPVke6gaV"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "E_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "0 & \\textrm{if all is observed} \\\\\n",
    "\\left(X_{i0} - \\hat X_{i0} \\right)^2 & \\textrm{if }X_{i, 0}\\textrm{ only missing} \\\\\n",
    "\\left(X_{i1} - \\hat X_{i1} \\right)^2 & \\textrm{if }X_{i1}\\textrm{ only missing} \\\\\n",
    "\\left(X_{i0} - \\hat X_{i0} \\right)^2 + \\left(X_{i1} - \\hat X_{i1} \\right)^2 & \\textrm{if }X_{i0}, X_{i1}\\textrm{ missing}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycGeW3n86yH6"
   },
   "source": [
    "### Question 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlTvA4Zs5R0_"
   },
   "source": [
    "Suppose we want to minimize the mean squared imputation error.\n",
    "\n",
    "In this Gaussian model, to which well-known problem does each pattern reduce? What is the optimal imputation in each pattern? Express it in terms of $\\mu_0$, $\\mu_1$, $\\sigma_0$, $\\sigma_1$, $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Nq4dnn73tMG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fSiPY0M3vJC"
   },
   "source": [
    "This least squares minimization problem in each pattern has a known solution, which is the conditional expectation.\n",
    "\n",
    "When $X_{.1}$ is the only missing variable:\n",
    "$$\n",
    "\\mathbb{E}[X_{.1} \\mid X_{.0} = x_{.0}] = \\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_0} (x_{.0} - \\mu_0)\n",
    "$$\n",
    "When $X_{.0}$ is the only missing variable:\n",
    "$$\n",
    "\\mathbb{E}[X_{.0} \\mid X_{.1} = x_{.1}] = \\mu_0 + \\rho \\frac{\\sigma_0}{\\sigma_1} (x_{.1} - \\mu_1)\n",
    "$$\n",
    "When $X_{.0}$ and $X_{.1}$ are both missing:\n",
    "$$\n",
    "\\mathbb{E}[X_{.0}, X_{.1}] = \\left(\\mu_0,~\\mu_1\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4srdiAjR7XD"
   },
   "source": [
    "### Question 4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNx7nO92R-LM"
   },
   "source": [
    "Implement this imputation in a function. Compute its mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Apz5mPXkSQnM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuHj7ZI5SjoA"
   },
   "outputs": [],
   "source": [
    "def conditional_expectation_imputation(xmiss, mu0, mu1, sig0, sig1, rho):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get integer patterns from the 2D boolean mask\n",
    "    # (0,0) -> 0; (1,0) -> 1; (0,1) -> 2; (1,1) -> 3\n",
    "    patterns = mask[:, 0].astype(int) + 2 * mask[:, 1].astype(int)  # shape: (N,)\n",
    "\n",
    "    impx0 = np.c_[mu0 + rho * sig0 / sig1 * (xmiss[:, 1] - mu1), xmiss[:, 1]]\n",
    "    impx1 = np.c_[xmiss[:, 0], mu1 + rho * sig1 / sig0 * (xmiss[:, 0] - mu0)]\n",
    "    impx01 = np.c_[np.full_like(xmiss[:, 0], mu0), np.full_like(xmiss[:, 1], mu1)]\n",
    "\n",
    "    # stack impputation cases into shape (4, N, 2)\n",
    "    imputations = np.stack([xmiss, impx0, impx1, impx01], axis=0)  # shape: (4, N, 2)\n",
    "\n",
    "    # select appropriate rows from each imputation case using the patterns\n",
    "    rows = np.arange(len(patterns))  # shape: (500,)\n",
    "\n",
    "    imp = imputations[patterns, rows]  # shape: (500, 2)\n",
    "\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1750879440583,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "GjvG_eB3TMsV",
    "outputId": "8fa27771-d3b5-4999-ff2b-7645f54fa9ef"
   },
   "outputs": [],
   "source": [
    "ximp_ce = conditional_expectation_imputation(xmiss, mu0, mu1, sig0, sig1, rho)\n",
    "print(mse(ximp_ce, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgzAu2yKGcvn"
   },
   "source": [
    "### Question 4e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhT4GXDLGgoz"
   },
   "source": [
    "Still using the plot from question 2b as inspiration, represent the imputation by the conditional expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgytbnaPGtRX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1750879440864,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "cuaGtorX6Lld",
    "outputId": "f7f34be2-44dc-490b-a4de-6c7e138515a0"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'], label=\"Complete\")\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(\n",
    "    x=ximp_ce[where_na0, 0], y=ximp_ce[where_na0, 1],\n",
    "    color=['#2194ac'], ax=ax, clip_on=False, label=r\"$X_0$ imputed\")\n",
    "_ = sns.scatterplot(\n",
    "    x=ximp_ce[where_na1, 0], y=ximp_ce[where_na1, 1],\n",
    "    color=['#2138ac'], ax=ax, clip_on=False, label=r\"$X_1$ imputed\")\n",
    "_ = sns.scatterplot(\n",
    "    x=ximp_ce[where_na01, 0], y=ximp_ce[where_na01, 1],\n",
    "    color=['#ac6721'], ax=ax, clip_on=False, label=r\"$X_0,X_1$ imputed\")\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);\n",
    "\n",
    "_ = ax.set_xlabel(r'$X_0$')\n",
    "_ = ax.set_ylabel(r'$X_1$')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEob-UCyK0Zi"
   },
   "source": [
    "**Note**: The purpose of this theoretical question was, in addition to working with the concept of patterns, to show that even in a Gaussian case, the ideal imputation is not simply to reduce everything to a single regression line. There is an expression for each pattern!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gX3tzrE1Cdy"
   },
   "source": [
    "## Question 5: Iterative imputation with linear regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5bQQ1KC1JQi"
   },
   "source": [
    "In practice, of course, we do not know $\\mu$ and $\\Sigma$, and we don’t even know if the data are Gaussian. Therefore, it is necessary to perform linear regressions to estimate the distribution parameters. This can be done iteratively using the `IterativeImputer` class from the `sklearn.impute` module, using a linear regression as the base estimator. Implement this imputation, represent it in the same way as before, and also calculate its Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWxxecoa1Hbv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhkOIlVO1Mn3"
   },
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(estimator=LinearRegression())\n",
    "ximp_lr = imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750879440927,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "UU_yrpk-YjCQ",
    "outputId": "2f29f498-2132-4906-dc1d-fa7b1d7c9da6"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_lr, xfull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1750879441196,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "eM3p8v9N1M6T",
    "outputId": "86e7cd93-4799-4310-bce9-42d0b59988dc"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'])\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_lr[where_na0, 0], y=ximp_lr[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_lr[where_na1, 0], y=ximp_lr[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_lr[where_na01, 0], y=ximp_lr[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False)\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EjjCENg0Fhd"
   },
   "source": [
    "## Question 6: nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdA0YGlh0IMl"
   },
   "source": [
    "The Gaussian linear model does not always fit the data studied in practice. In the general case, non-parametric methods are more flexible. An essential example of non-parametric imputation is k-nearest neighbors imputation.\n",
    "\n",
    "Implement this imputation using `sklearn.impute`, calculate the MSE, and visualize the imputation on a scatter plot.\n",
    "\n",
    "The most important hyperparameter is the number of neighbors to use, to find a bias-variance trade-off: compare several values both in terms of MSE and graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg0QjJTr1EFm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNg_xAku0MK5"
   },
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=10)\n",
    "ximp_knn = imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1750879441260,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "JoBbU-Bpq5uM",
    "outputId": "a4d7062a-a7ab-41cb-ce95-7c349c0af38b"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_knn, xfull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1750879441415,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "t1TKLy6_0fhK",
    "outputId": "d9883cdf-a934-48ba-c7b0-0a93881b9c47"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'])\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_knn[where_na0, 0], y=ximp_knn[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_knn[where_na1, 0], y=ximp_knn[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_knn[where_na01, 0], y=ximp_knn[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False)\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-ImuvEMb7J-"
   },
   "source": [
    "## Question 7: random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHbJfNcxb8ze"
   },
   "source": [
    "Another important example of non-parametric imputation is iterative imputation with random forests — or other decision tree–based models. Use `IterativeImputer` again to impute the dataset, this time with a Random Forest as the base estimator. Calculate its MSE and visualize the results.\n",
    "\n",
    "The most important hyperparameters to tune are first those of the base estimator: for the random forest, these are the number of estimators and the maximum depth of the trees. For iterative imputation, it can be useful to limit the tree depth for greater stability. In `IterativeImputer`also tune the maximum number of iterations `max_iter` and the stopping criterion `tol`.\n",
    "\n",
    "Be careful: with large datasets and many variables, this method can be computationally expensive. You may want to adjust the hyperparameters `n_nearest_features` and `skip_complete` to reduce computation time.\n",
    "\n",
    "Note that `IterativeImputer` is still experimental and its API may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OAcyhI5caiB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1750879441709,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "PKkapKsYcZLy",
    "outputId": "2ca1ac79-a865-4a13-a63a-1f04af30e6ac"
   },
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, max_depth=3), max_iter=10, tol=0.001)\n",
    "ximp_rf = imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750879441727,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "jMKfnCEQcs3q",
    "outputId": "152855fc-7c8d-48ba-f4b3-a4da8f9cef2d"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_rf, xfull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1750879441947,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "D8ec2iPacvC5",
    "outputId": "605f8518-917e-4ae5-d28b-e69d0e901405"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'])\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_rf[where_na0, 0], y=ximp_rf[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_rf[where_na1, 0], y=ximp_rf[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_rf[where_na01, 0], y=ximp_rf[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False)\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chLqkAqQ4L1C"
   },
   "source": [
    "# Exercise 2: Iterative imputation with random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqlQ84_2n0lE"
   },
   "source": [
    "In this exercise, you will re-implement by hand the iterative imputation algorithm, starting from the base estimator `RandomForestRegressor`. You will define a function, beginning from the following cell, which takes as input the incomplete dataset and returns the imputed dataset.\n",
    "\n",
    "We are essentially implementing the MissForest algorithm, as described in [Steckhoven et al. (2012)](https://academic.oup.com/bioinformatics/article/28/1/112/219101) (Algorithm 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H489_CJ3_yaI"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    x_imputed = ...\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETkPRHJQohEO"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NmAtyNwoimL"
   },
   "source": [
    "In the function `impute_manualrandomforest`, create a boolean variable `mask` indicating where the missing values are in `xmiss`.\n",
    "\n",
    "Then, determine the order of the columns of `xmiss` by increasing missing value rate and save the result in a variable `order` (here there are only 2 columns, but you will implement the general algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JbZo8AsJNco",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GwAnFlVJOpH"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    x_imputed = ...\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBxKw9Y6oS8G"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hXYnOOVoUuW"
   },
   "source": [
    "The initialization of the algorithm is a simple imputation by the mean. In the function, use `SimpleImputer` to impute `xmiss` with its means, saving the result in the variable `x_imputed`. Test the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNymshsFnNKT",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJC4ejJKnOQl"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    # impute the array by its means\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    x_imputed = mean_imputer.fit_transform(xmiss)\n",
    "\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1750879442062,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "qpUb3pG9qmSF",
    "outputId": "a577ffe9-3322-4658-9ac8-f0980de5ad31"
   },
   "outputs": [],
   "source": [
    "ximp_manrf = impute_manualrandomforest(xmiss)\n",
    "print(mse(ximp_manrf, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyyGTId1LuYm"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZTQxKb8LwO9"
   },
   "source": [
    "Using the order established in question 1, select the column with the fewest missing values, `col`.\n",
    "\n",
    "Using only the indices `i` where `xmiss[i, col]` is observed (i.e., where `mask[i, col] == 0`), train a random forest `rf` on the already imputed dataset `x_imputed` to predict `x_imputed[:, col]` from all the other variables.\n",
    "\n",
    "Then use `rf` to predict the missing values of `xmiss[i, col]`. These new predictions replace the initial naive imputation in `x_imputed`.\n",
    "\n",
    "Hint: here there are only 2 columns. To train the forest with a single feature, the following function may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_yl7ufPerTg"
   },
   "outputs": [],
   "source": [
    "def ensure_2d_column(x):\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        return x.reshape(-1, 1)  # (N,) → (N, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS1PIWzyLw_I",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U56MXo4uLyd8"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    # impute the array by its means\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    x_imputed = mean_imputer.fit_transform(xmiss)\n",
    "\n",
    "    # select column to impute\n",
    "    col = order[0]\n",
    "    other_cols = [c for c in order if c != col]\n",
    "\n",
    "    # select indices where col is observed\n",
    "    obs_indices = (mask[:, col] == 0)\n",
    "    mis_indices = (mask[:, col] == 1)\n",
    "\n",
    "    # fit random forest\n",
    "    rf = RandomForestRegressor(n_estimators=10, max_depth=3)\n",
    "    rf.fit(X=ensure_2d_column(x_imputed[obs_indices, other_cols]), y=x_imputed[obs_indices, col])\n",
    "    # get new prediction\n",
    "    pred = rf.predict(X=ensure_2d_column(x_imputed[mis_indices, other_cols]))\n",
    "    # replace in array\n",
    "    x_imputed[mis_indices, col] = pred\n",
    "\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1750879442085,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "uaGB04ptQktb",
    "outputId": "03d29272-7d62-45b4-d676-bec54046fe32"
   },
   "outputs": [],
   "source": [
    "ximp_manrf = impute_manualrandomforest(xmiss)\n",
    "print(mse(ximp_manrf, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7tFthxlfD74"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8nodeY_fHlS"
   },
   "source": [
    "The algorithm consists of repeating the previous step by looping over all columns, as many times as necessary until the stopping criterion is met or the maximum number of iterations is reached.\n",
    "\n",
    "The stopping criterion is met when the difference between two successive imputations is less than a threshold. The difference function is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HosN1kWGgUht"
   },
   "outputs": [],
   "source": [
    "def difference(x_new, x_old):\n",
    "  return np.sum((x_new - x_old) ** 2) / np.sum(x_new ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXu-zsbcfHwB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_lgsXTPfGh2"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss, max_iter=10, tol=0.001):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    # impute the array by its means\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    x_imputed = mean_imputer.fit_transform(xmiss)\n",
    "\n",
    "    # no more than max_iter loops\n",
    "    for iteration in range(max_iter):\n",
    "        # loop over columns to impute\n",
    "        for col in order:\n",
    "            # save a copy to measure the difference between 2 successive imputations\n",
    "            x_imputed_old = np.copy(x_imputed)\n",
    "\n",
    "            other_cols = [c for c in order if c != col]\n",
    "\n",
    "            # select indices where col is observed\n",
    "            obs_indices = (mask[:, col] == 0)\n",
    "            mis_indices = (mask[:, col] == 1)\n",
    "\n",
    "            # fit random forest\n",
    "            rf = RandomForestRegressor(n_estimators=10, max_depth=3)\n",
    "            rf.fit(X=ensure_2d_column(x_imputed[obs_indices, other_cols]), y=x_imputed[obs_indices, col])\n",
    "            # get new prediction\n",
    "            pred = rf.predict(X=ensure_2d_column(x_imputed[mis_indices, other_cols]))\n",
    "            # replace in array\n",
    "            x_imputed[mis_indices, col] = pred\n",
    "\n",
    "            diff = difference(x_imputed, x_imputed_old)\n",
    "            if diff < tol:\n",
    "                return x_imputed\n",
    "\n",
    "    warnings.warn(\"max_iter was reached.\")\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1750879442267,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "R7tTKG73izZI",
    "outputId": "73882a88-6cd6-4a15-af43-6a61595e732b"
   },
   "outputs": [],
   "source": [
    "ximp_manrf = impute_manualrandomforest(xmiss)\n",
    "print(mse(ximp_manrf, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bbs5s7v1u-9"
   },
   "source": [
    "# Exercise 3: Comparison of imputation methods on a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfWzwi3337iI"
   },
   "source": [
    "In this exercise, you consider the same complete real dataset *Breast Cancer Wisconsin* as in Notebook 1 (Exercise 4). We generate 30% missing values of the MCAR type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iuclBDi4b1-"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "xfull = data['data']  # covariates, without missing values\n",
    "diagnosis = data['target']  # target variable to predict, when the learning task is prediction\n",
    "features_names = data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1750879442340,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "RcZjnLOt4cpH",
    "outputId": "b8c3ef70-9df7-4582-a082-258b3dbe65ba"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xfull, columns=features_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prTC9bzu4fC1"
   },
   "outputs": [],
   "source": [
    "n, d = xfull.shape  # data dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4g3OYDuY-km"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "p = 0.3\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p\n",
    "  xmiss[miss_id, j] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxVaL8wqn6WV"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQHFFym_n7wQ"
   },
   "source": [
    "Apply the imputation methods from Exercise 1 to this dataset, optimizing the hyperparameters:\n",
    "* mean imputation\n",
    "* k-nearest neighbors imputation\n",
    "* iterative imputation based on linear regression\n",
    "* iterative imputation based on random forest\n",
    "\n",
    "Compare their MSEs: which method performs best on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfauxruroNoC",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kht9iQUtoO0l"
   },
   "outputs": [],
   "source": [
    "def evaluate_imputer(imputer, xfull, xmiss):\n",
    "    ximp = imputer.fit_transform(xmiss)\n",
    "    score = mse(ximp, xfull)\n",
    "    print(f\"{imputer.__str__():<60}: MSE = {score:.6f}\")\n",
    "    return score\n",
    "\n",
    "def compare_imputers(xfull, xmiss):\n",
    "    results = {}\n",
    "\n",
    "    # 1. SimpleImputer (mean)\n",
    "    simple = SimpleImputer(strategy='mean')\n",
    "    score = evaluate_imputer(simple, xfull, xmiss)\n",
    "    results['SimpleImputer (mean)'] = score\n",
    "\n",
    "    # 2. KNNImputer (optimize k)\n",
    "    for k in [3, 5, 10, 15]:\n",
    "        knn = KNNImputer(n_neighbors=k)\n",
    "        score = evaluate_imputer(knn, xfull, xmiss)\n",
    "        results[f'KNNImputer (best_k={k})'] = score\n",
    "\n",
    "    # 3. IterativeImputer + LinearRegression (optimize tol)\n",
    "    for tol in [0.1, 0.01, 0.001, 0.0001]:\n",
    "        iter_lr = IterativeImputer(estimator=LinearRegression(), tol=tol)\n",
    "        score = evaluate_imputer(iter_lr, xfull, xmiss)\n",
    "        results[f'IterativeImputer + LR (tol={tol})'] = score\n",
    "\n",
    "    # 4. IterativeImputer + RandomForestRegressor (optimize depth, tol)\n",
    "    param_grid = ParameterGrid({\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'tol': [0.1, 0.01]\n",
    "    })\n",
    "    for params in param_grid:\n",
    "        rf = RandomForestRegressor(n_estimators=10, max_depth=params['max_depth'], random_state=0)\n",
    "        iter_rf = IterativeImputer(estimator=rf, max_iter=10, tol=params['tol'])\n",
    "        score = evaluate_imputer(iter_rf, xfull, xmiss)\n",
    "        key = f\"IterativeImputer + RF (depth={params['max_depth']}, tol={params['tol']})\"\n",
    "        results[key] = score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74042,
     "status": "ok",
     "timestamp": 1750882530834,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "vrd9M2AEqgcX",
    "outputId": "79cd5743-ca70-43b2-ba0d-e969852b38e4"
   },
   "outputs": [],
   "source": [
    "results = compare_imputers(xfull, xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1750882530885,
     "user": {
      "displayName": "Famille Sportisse Prost",
      "userId": "08952667866865499063"
     },
     "user_tz": -120
    },
    "id": "6nx5i26ZuwcM",
    "outputId": "5f7fba12-d1f8-41f3-bfab-4e64662b3fc9"
   },
   "outputs": [],
   "source": [
    "for name, score in sorted(results.items(), key=lambda x: x[1]):\n",
    "    print(f\"{name:<45}: MSE = {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSkUfTZxvrV2"
   },
   "source": [
    "Among the models tested, the best imputation is achieved by iterative imputation. The choice of the base estimator, as well as the selection of hyperparameters, does not appear to be crucial — or should instead be subjected to a more thorough model selection process.\n",
    "\n",
    "The poor performance of $k$-nearest neighbors imputation is most likely due to the high dimensionality."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO5MekR+/2AiMK5+9IaaKva",
   "provenance": [
    {
     "file_id": "1aKEu3SXllmWRgB7mOFne49ZP-P8S4trV",
     "timestamp": 1756843121958
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
