{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3zlLT4JzZo7"
   },
   "source": [
    "# Travaux Pratiques 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy38Xm1bzam7"
   },
   "source": [
    "Ce deuxième TP vous permettra de manipuler les méthodes d'imputation présentées dans la vidéo du module 2, à savoir :\n",
    "* l'imputation par la moyenne,\n",
    "* l'imputation par plus proches voisins,\n",
    "* l'imputation itérative.\n",
    "\n",
    "Le premier exercice est le plus théorique et permet de développer une intuition du fonctionnement des méthodes. Les exercices 2 et 3 sont plus pratiques.\n",
    "\n",
    "**Note :** une imputation peut avoir plusieurs objectifs. Ici, vous étudierez des méthodes ayant pour but de minimiser une erreur d'imputation, en choisissant toujours les valeurs les plus probables. C'est un but tout à fait légitime, mais ces méthodes ont l'inconvénient de déformer la distribution des données, notamment en réduisant la variance. Elles ne sont donc pas indiquées lorsqu'on cherche à estimer la distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llHpZvLmC2MC"
   },
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qnNR2u2C8RH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyLGu6VsxIfP"
   },
   "source": [
    "## Librairies importées dans la solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXg6YCx7xMKV"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYkHK6mH1sWM"
   },
   "source": [
    "# Exercice 1 : applications basiques des méthodes d'imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3Y2LiBiaeHP"
   },
   "source": [
    "Dans cet exercice, vous utiliserez des données synthétiques en deux dimensions, afin de faciliter la visualisation des résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUrrqNhSuFK2"
   },
   "source": [
    "## Question 1 : échantillon bivarié gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7raSkPRuGqB"
   },
   "source": [
    "Générez un échantillon bivarié (*i.e.* à $d=2$ variables), gaussien,\n",
    "\n",
    "$$\n",
    "\\left( X_{i.} \\right)_{1\\leq i\\leq n}\n",
    "=\\left( X_{i0}, X_{i1} \\right)_{1\\leq i\\leq n}\n",
    "$$\n",
    "\n",
    "de taille $n=500$, de moyenne $\\mu$ et de matrice de covariance $\\Sigma$, en notant :\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\n",
    "  \\mu_0 \\\\[6pt] \\mu_1\n",
    "\\end{bmatrix}, \\quad\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "  \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\[6pt]\n",
    "  \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "avec les valeurs suivantes :\n",
    "$$\n",
    "\\mu_0 = 0,~\\mu_1 = 0,~\\sigma_0 = 1,~\\sigma_1 = 0.7,~\\rho = 0.8\n",
    "$$\n",
    "\n",
    "Vous enregistrerez l'échantillon dans une variable `xfull`.\n",
    "\n",
    "Représentez l'échantillon à l'aide d'un nuage de points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuCIfN2byT0D",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "Csd4WDtxDWe8",
    "outputId": "9fb4944b-c511-4e55-8edf-26bc3eb6bf04"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n = 500\n",
    "d = 2\n",
    "mu0 = 0.\n",
    "mu1 = 0.\n",
    "sig0 = 1.\n",
    "sig1 = 0.7\n",
    "rho = 0.8\n",
    "\n",
    "mean = np.array([mu0, mu1])\n",
    "cov = np.array([\n",
    "    [sig0 ** 2, rho * sig0 * sig1],\n",
    "    [rho * sig0 * sig1, sig1 ** 2]\n",
    "    ])\n",
    "\n",
    "xfull = np.random.multivariate_normal(mean, cov, size=n)\n",
    "\n",
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], color=['#d1e5f0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-q8XGIIvTTx"
   },
   "source": [
    "## Question 2 : génération de valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HavvXoT5B3a"
   },
   "source": [
    "### Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFnvB8eCeiIl"
   },
   "source": [
    "Exécutez la cellule suivante pour générer les valeurs manquantes comme dans le TP1, et obtenir le jeu de données amputé, `xmiss`. Quel est le mécanisme de données manquantes ici ? Que représente la variable `p` ? Combien y a-t-il de *patterns* possibles, c'est-à-dire de cas de figure de NA possibles pour une ligne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "QE6b3fzts-6n",
    "outputId": "3d11e21a-cb52-4d7d-edb1-6584fd27d45f"
   },
   "outputs": [],
   "source": [
    "p = 0.4\n",
    "\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "  miss_id = (np.random.uniform(0, 1, size=n) < p)\n",
    "  xmiss[miss_id, j] = np.nan\n",
    "\n",
    "display(pd.DataFrame(xmiss).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUwBouD1vfhk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_OivtvRvg_N"
   },
   "source": [
    "Le mécanisme est MCAR car le manque de données, `miss_id`, est indépendant de `xfull`.\n",
    "\n",
    "`p` représente la probabilité que chaque variable soit manquante (notons qu'on pourrait choisir un `p_j` différent pour chaque variable et rester MCAR).\n",
    "\n",
    "Il y a ici 4 patterns :\n",
    "* $X_0,~X_1$ observés,\n",
    "* $X_0$ seul manquant,\n",
    "* $X_1$ seul manquant,\n",
    "* $X_0,~X_1$ manquants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNDn2sQAv0ta"
   },
   "source": [
    "### Question 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gocs0lT9tBxp"
   },
   "source": [
    "Exécutez la cellule suivante pour représenter `xmiss`. Interprétez le graphique : que signifie chaque groupe de points ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "tpX6DZPkDtn-",
    "outputId": "87dafb86-7238-4437-dbd7-54273aed5574"
   },
   "outputs": [],
   "source": [
    "where_full = ~np.isnan(xmiss[:, 0]) & ~np.isnan(xmiss[:, 1])\n",
    "where_na0 = np.isnan(xmiss[:, 0]) & ~np.isnan(xmiss[:, 1])\n",
    "where_na1 = np.isnan(xmiss[:, 1]) & ~np.isnan(xmiss[:, 0])\n",
    "where_na01 = np.isnan(xmiss[:, 0]) & np.isnan(xmiss[:, 1])\n",
    "\n",
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'], label=\"?\")\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=xmin, y=xmiss[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False, label=\"?\")\n",
    "_ = sns.scatterplot(x=xmiss[where_na1, 0], y=ymin, color=['#2138ac'], ax=ax, clip_on=False, label=\"?\")\n",
    "_ = sns.scatterplot(x=[xmin], y=[ymin], color=['#ac6721'], ax=ax, clip_on=False, label=\"?\")\n",
    "\n",
    "_ = ax.set_xlim(xmin, xmax)\n",
    "_ = ax.set_ylim(ymin, ymax)\n",
    "\n",
    "_ = ax.set_xlabel(r'$X_0$')\n",
    "_ = ax.set_ylabel(r'$X_1$')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm3WPcnguSQm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z86NSbpnuTvP"
   },
   "source": [
    "Dans le premier groupe, les variables $X_{.0}$ et $X_{.1}$ sont observées, les points sont donc représentées normalement.\n",
    "\n",
    "Dans le deuxième groupe, la variable $X_{.0}$ est manquante, on n'a accès qu'à la variable $X_{.1}$. Dans ce cas, les points sont donc représentés sur l'axe vertical, puisqu'on ne sait pas les placer horizontalement.\n",
    "\n",
    "Inversement, dans le troisième groupe, la variable $X_{.1}$ est manquante, donc les points sont représentés sur l'axe horizontal car on ne sait pas les placer verticalement.\n",
    "\n",
    "Finalement, les points du dernier groupe, pour lesquels les deux variables sont manquantes sont représentés dans le coin inférieur gauche.\n",
    "\n",
    "Ces quatre cas de figure correspondent aux quatre *patterns* possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1SvkewBweKs"
   },
   "source": [
    "## Question 3 : imputation par les moyennes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI9CQNtgwiJt"
   },
   "source": [
    "L'imputation la plus simple possible est l'imputation par les moyennes. En utilisant le module `sklearn.impute`, remplacez les valeurs manquantes de `xmiss` par la moyenne dans chaque variable, et représentez le jeu de données imputé en vous inspirant du graphique de la question 2b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pldCDBz4w54d",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6v55d_Jw8Fw"
   },
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "ximp_mean = mean_imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "DqPOoqERzV4h",
    "outputId": "20ece7fa-0934-4fed-e9ee-3b075614dd76"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'], label=\"Complet\")\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_mean[where_na0, 0], y=ximp_mean[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False, label=r\"$X_0$ imputé\")\n",
    "_ = sns.scatterplot(x=ximp_mean[where_na1, 0], y=ximp_mean[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False, label=r\"$X_1$ imputé\")\n",
    "_ = sns.scatterplot(x=ximp_mean[where_na01, 0], y=ximp_mean[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False, label=r\"$X_0,X_1$ imputés\")\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);\n",
    "\n",
    "_ = ax.set_xlabel(r'$X_0$')\n",
    "_ = ax.set_ylabel(r'$X_1$')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oene59ds3rhT"
   },
   "source": [
    "## Question 4 : minimisation de l'erreur d'imputation théorique * (question difficile nécessitant de connaître la notion d'espérance conditionnelle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u49P3IWg3wBD"
   },
   "source": [
    "Pour un point $X_{i.} = \\left( X_{i0}, X_{i1} \\right)$ de l'échantillon (la donnée réelle sans valeur manquante), on définit son erreur quadratique moyenne d'imputation $E_i$ par:\n",
    "\n",
    "$$\n",
    "E_i\n",
    "= \\lVert X_{i.} - \\hat X_{i.} \\rVert^2\n",
    "= \\left(X_{i0} - \\hat X_{i0} \\right)^2\n",
    "  + \\left(X_{i1} - \\hat X_{i1} \\right)^2\n",
    "$$\n",
    "\n",
    "où $\\hat X_{i.}$ est la donnée imputée.\n",
    "\n",
    "L'erreur quadratique moyenne d'imputation $E$ sur l'échantillon est donnée par:\n",
    "\n",
    "$$E = \\frac{1}{n} \\sum_{i=1}^{n} E_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaIq-7xoYAiy"
   },
   "source": [
    "### Question 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8siot9kfVl_5"
   },
   "source": [
    "Calculez l'erreur quadratique moyenne (MSE) de l'imputation par la moyenne de la question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngEBBEYYVynb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LjJqrY5XE0Z"
   },
   "outputs": [],
   "source": [
    "def mse(x_imp, x_true):\n",
    "  n = len(x_true)\n",
    "  return (1 / n) * np.sum((x_imp - x_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBAJYmiwXbBI",
    "outputId": "3c40b7e7-73f9-4a9f-db8a-88e00a712899"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_mean, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKFvBaZr5WiK"
   },
   "source": [
    "### Question 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wNblVrZ5ays"
   },
   "source": [
    "Dans ce cas bivarié, comment se simplifie l'erreur $E_i$ en fonction du pattern à la ligne $i$, c'est-à-dire en fonction de quelles variables sont manquantes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrhsM2_76e6t",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfqTPVke6gaV"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "E_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "0 & \\textrm{si tout observé} \\\\\n",
    "\\left(X_{i0} - \\hat X_{i0} \\right)^2 & \\textrm{si }X_{i0}\\textrm{ seul manquante} \\\\\n",
    "\\left(X_{i1} - \\hat X_{i1} \\right)^2 & \\textrm{si }X_{i1}\\textrm{ seul manquante} \\\\\n",
    "\\left(X_{i0} - \\hat X_{i0} \\right)^2 + \\left(X_{i1} - \\hat X_{i1} \\right)^2 & \\textrm{si }X_{i0}, X_{i1}\\textrm{ manquantes}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycGeW3n86yH6"
   },
   "source": [
    "### Question 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlTvA4Zs5R0_"
   },
   "source": [
    "Supposons qu'on cherche à minimiser l'erreur quadratique moyenne d'imputation.\n",
    "\n",
    "Dans ce modèle gaussien, à quel problème bien connu se ramène-t-on dans chaque pattern ? Quelle est l'imputation optimale dans chaque pattern ? Exprimez-la en fonction de $\\mu_0$, $\\mu_1$, $\\sigma_0$, $\\sigma_1$, $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Nq4dnn73tMG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fSiPY0M3vJC"
   },
   "source": [
    "Ce problème de minimisation des moindres carrés dans chaque pattern a une solution connue qui est l'espérance conditionnelle.\n",
    "\n",
    "Lorsque $X_{.1}$ est la seule variable manquante:\n",
    "$$\n",
    "\\mathbb{E}[X_{.1} \\mid X_{.0} = x_{.0}] = \\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_0} (x_{.0} - \\mu_0)\n",
    "$$\n",
    "Lorsque $X_{.0}$ est la seule variable manquante:\n",
    "$$\n",
    "\\mathbb{E}[X_{.0} \\mid X_{.1} = x_{.1}] = \\mu_0 + \\rho \\frac{\\sigma_0}{\\sigma_1} (x_{.1} - \\mu_1)\n",
    "$$\n",
    "Lorsque $X_{.0}$ et $X_{.1}$ sont manquantes:\n",
    "$$\n",
    "\\mathbb{E}[X_{.0}, X_{.1}] = \\left(\\mu_0,~\\mu_1\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4srdiAjR7XD"
   },
   "source": [
    "### Question 4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNx7nO92R-LM"
   },
   "source": [
    "Implémentez cette imputation dans une fonction. Calculez son erreur quadratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Apz5mPXkSQnM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuHj7ZI5SjoA"
   },
   "outputs": [],
   "source": [
    "def conditional_expectation_imputation(xmiss, mu0, mu1, sig0, sig1, rho):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get integer patterns from the 2D boolean mask\n",
    "    # (0,0) -> 0; (1,0) -> 1; (0,1) -> 2; (1,1) -> 3\n",
    "    patterns = mask[:, 0].astype(int) + 2 * mask[:, 1].astype(int)  # shape: (N,)\n",
    "\n",
    "    impx0 = np.c_[mu0 + rho * sig0 / sig1 * (xmiss[:, 1] - mu1), xmiss[:, 1]]\n",
    "    impx1 = np.c_[xmiss[:, 0], mu1 + rho * sig1 / sig0 * (xmiss[:, 0] - mu0)]\n",
    "    impx01 = np.c_[np.full_like(xmiss[:, 0], mu0), np.full_like(xmiss[:, 1], mu1)]\n",
    "\n",
    "    # stack impputation cases into shape (4, N, 2)\n",
    "    imputations = np.stack([xmiss, impx0, impx1, impx01], axis=0)  # shape: (4, N, 2)\n",
    "\n",
    "    # select appropriate rows from each imputation case using the patterns\n",
    "    rows = np.arange(len(patterns))  # shape: (500,)\n",
    "\n",
    "    imp = imputations[patterns, rows]  # shape: (500, 2)\n",
    "\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjvG_eB3TMsV",
    "outputId": "8fa27771-d3b5-4999-ff2b-7645f54fa9ef"
   },
   "outputs": [],
   "source": [
    "ximp_ce = conditional_expectation_imputation(xmiss, mu0, mu1, sig0, sig1, rho)\n",
    "print(mse(ximp_ce, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgzAu2yKGcvn"
   },
   "source": [
    "### Question 4e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhT4GXDLGgoz"
   },
   "source": [
    "Toujours en vous inspirant du graphique de la question 2b, représentez l'imputation par l'espérance conditionnelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgytbnaPGtRX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "cuaGtorX6Lld",
    "outputId": "f7f34be2-44dc-490b-a4de-6c7e138515a0"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'], label=\"Complet\")\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(\n",
    "    x=ximp_ce[where_na0, 0], y=ximp_ce[where_na0, 1],\n",
    "    color=['#2194ac'], ax=ax, clip_on=False, label=r\"$X_0$ imputé\")\n",
    "_ = sns.scatterplot(\n",
    "    x=ximp_ce[where_na1, 0], y=ximp_ce[where_na1, 1],\n",
    "    color=['#2138ac'], ax=ax, clip_on=False, label=r\"$X_1$ imputé\")\n",
    "_ = sns.scatterplot(\n",
    "    x=ximp_ce[where_na01, 0], y=ximp_ce[where_na01, 1],\n",
    "    color=['#ac6721'], ax=ax, clip_on=False, label=r\"$X_0,X_1$ imputés\")\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);\n",
    "\n",
    "_ = ax.set_xlabel(r'$X_0$')\n",
    "_ = ax.set_ylabel(r'$X_1$')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEob-UCyK0Zi"
   },
   "source": [
    "**Remarque:** l'objet de cette question théorique était, en plus de manipuler la notion de pattern, de montrer que même dans un cas gaussien, l'imputation idéale ne consiste pas à tout ramener à une seule et unique droite de régression. Il y a une expression par pattern !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gX3tzrE1Cdy"
   },
   "source": [
    "## Question 5 : imputation itérative avec régressions linéaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5bQQ1KC1JQi"
   },
   "source": [
    "En pratique, on ne connaît bien sûr pas $\\mu$ et $\\Sigma$, on ne sait même pas si les données sont gaussiennes. Il faut donc réaliser des régressions linéaires pour estimer les paramètres de la distribution. On peut le faire de manière itérative avec la classe `IterativeImputer` du module `sklearn.impute`, en utilisant une régression linéaire comme estimateur de base. Implémentez cette imputation, et représentez-la de la même manière que précédemment. Calculez également sa MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWxxecoa1Hbv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhkOIlVO1Mn3"
   },
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(estimator=LinearRegression())\n",
    "ximp_lr = imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UU_yrpk-YjCQ",
    "outputId": "2f29f498-2132-4906-dc1d-fa7b1d7c9da6"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_lr, xfull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "eM3p8v9N1M6T",
    "outputId": "86e7cd93-4799-4310-bce9-42d0b59988dc"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'])\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_lr[where_na0, 0], y=ximp_lr[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_lr[where_na1, 0], y=ximp_lr[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_lr[where_na01, 0], y=ximp_lr[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False)\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EjjCENg0Fhd"
   },
   "source": [
    "## Question 6 : plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdA0YGlh0IMl"
   },
   "source": [
    "Le modèle linéaire gaussien ne convient pas toujours aux données qu'on étudie en pratique. Dans le cas général, les méthodes non paramétriques sont plus flexibles. Un exemple essentiel d'imputation non paramétrique est l'imputation par plus proches voisins.\n",
    "\n",
    "Implémentez cette imputation à l'aide de `sklearn.impute`, calculez la MSE et représentez l'imputation sur un nuage de points.\n",
    "\n",
    "L'hyperparamètre le plus important est le nombre de voisins à utiliser, pour trouver un équilibre biais-variance : comparez plusieurs valeurs, en MSE et graphiquement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg0QjJTr1EFm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNg_xAku0MK5"
   },
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=10)\n",
    "ximp_knn = imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoBbU-Bpq5uM",
    "outputId": "a4d7062a-a7ab-41cb-ce95-7c349c0af38b"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_knn, xfull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "t1TKLy6_0fhK",
    "outputId": "d9883cdf-a934-48ba-c7b0-0a93881b9c47"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'])\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_knn[where_na0, 0], y=ximp_knn[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_knn[where_na1, 0], y=ximp_knn[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_knn[where_na01, 0], y=ximp_knn[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False)\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-ImuvEMb7J-"
   },
   "source": [
    "## Question 7 : forêts aléatoires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHbJfNcxb8ze"
   },
   "source": [
    "Un autre exemple important d'imputation non paramétrique est l'imputation itérative avec forêts aléatoires ou un autre modèle à base d'arbres de décision. Utilisez à nouveau `IterativeImputer` pour imputer le jeu de données, cette fois-ci avec une forêt aléatoire comme estimateur de base. Calculez sa MSE et représentez-la graphiquement.\n",
    "\n",
    "Les hyperparamètres les plus importants à régler sont d'abord ceux de l'estimateur de base : pour la forêt aléatoire, c'est le nombre d'estimateurs et la profondeur maximale des arbres. Pour l'imputation itérative, il peut être intéressant de limiter la profondeur des arbres pour plus de stabilité. Dans `IterativeImputer`, réglez également le nombre d'itérations maximal `max_iter` et le critère d'arrêt `tol`.\n",
    "\n",
    "Attention, avec de jeux de données de grande dimension (beaucoup d'observations, beaucoup de variables), cette méthode est coûteuse en temps de calcul. Vous pourrez ajuster les hyperparamètres `n_nearest_features` et `skip_complete` pour réduire ce coût.\n",
    "\n",
    "Notons que `IterativeImputer` est encore en phase exérimentale et son API pourra être amenée à changer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OAcyhI5caiB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKkapKsYcZLy",
    "outputId": "2ca1ac79-a865-4a13-a63a-1f04af30e6ac"
   },
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, max_depth=3), max_iter=10, tol=0.001)\n",
    "ximp_rf = imputer.fit_transform(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMKfnCEQcs3q",
    "outputId": "152855fc-7c8d-48ba-f4b3-a4da8f9cef2d"
   },
   "outputs": [],
   "source": [
    "print(mse(ximp_rf, xfull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "D8ec2iPacvC5",
    "outputId": "605f8518-917e-4ae5-d28b-e69d0e901405"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xmiss[where_full, 0], y=xmiss[where_full, 1], color=['#d1e5f0'])\n",
    "\n",
    "(xmin, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "_ = sns.scatterplot(x=ximp_rf[where_na0, 0], y=ximp_rf[where_na0, 1], color=['#2194ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_rf[where_na1, 0], y=ximp_rf[where_na1, 1], color=['#2138ac'], ax=ax, clip_on=False)\n",
    "_ = sns.scatterplot(x=ximp_rf[where_na01, 0], y=ximp_rf[where_na01, 1], color=['#ac6721'], ax=ax, clip_on=False)\n",
    "\n",
    "ax.set_xlim(xmin, xmax);\n",
    "ax.set_ylim(ymin, ymax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chLqkAqQ4L1C"
   },
   "source": [
    "# Exercice 2: imputation itérative avec forêts aléatoires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqlQ84_2n0lE"
   },
   "source": [
    "Dans cet exercice, vous recoderez à la main l'algorithme d'imputation itérative, à partir de l'estimateur de base `RandomForestRegressor`. Vous définirez une fonction, en partant de la cellule suivante, qui prend en entrée le jeu de données incomplet, et renvoie le jeu imputé.\n",
    "\n",
    "On implémente en fait l'algorithme `missforest`, accessible dans [Steckhoven et al. (2012), Algorithm 1](https://academic.oup.com/bioinformatics/article/28/1/112/219101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H489_CJ3_yaI"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    x_imputed = ...\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETkPRHJQohEO"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NmAtyNwoimL"
   },
   "source": [
    "Dans la fonction `impute_manualrandomforest`, créez une variable booléenne `mask` indiquant où sont les valeurs manquantes dans `xmiss`.\n",
    "\n",
    "Ensuite, déterminez l'ordre des colonnes de`xmiss` par taux de `NA` croissant et enregistrez le résultat dans une variable `order` (ici il n'y a que 2 colonnes, mais vous implémenterez l'algorithme général)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JbZo8AsJNco",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GwAnFlVJOpH"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    x_imputed = ...\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBxKw9Y6oS8G"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hXYnOOVoUuW"
   },
   "source": [
    "L'initialisation de l'algorithme est une imputation simple, par la moyenne. Dans la fonction, utilisez `SimpleImputer` pour imputer `xmiss` par ses moyennes, en enregistrant le résultat dans la variable `x_imputed`. Testez la fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNymshsFnNKT",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJC4ejJKnOQl"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    # impute the array by its means\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    x_imputed = mean_imputer.fit_transform(xmiss)\n",
    "\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpUb3pG9qmSF",
    "outputId": "a577ffe9-3322-4658-9ac8-f0980de5ad31"
   },
   "outputs": [],
   "source": [
    "ximp_manrf = impute_manualrandomforest(xmiss)\n",
    "print(mse(ximp_manrf, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyyGTId1LuYm"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZTQxKb8LwO9"
   },
   "source": [
    "Grâce à l'ordre établi à la question 1, sélectionnez la colonne avec le moins de valeurs manquantes, `col`.\n",
    "\n",
    "En n'utilisant que les indices `i` où `xmiss[i, col]` est observée (donc où `mask[i, col]==0`), utilisez le jeu déjà imputé `x_imputed` pour entraîner une forêt aléatoire `rf` à prédire `x_imputed[:, col]` à partir de toutes les autres variables.\n",
    "\n",
    "Utilisez ensuite `rf` pour prédire les valeurs manquantes de `xmiss[i, col]`. Ces nouvelles prédictions viennent remplacer, dans `x_imputed`, la première imputation naïve.\n",
    "\n",
    "Indication : ici il n'y a que 2 colonnes. Pour entraîner la forêt avec une seule colonne, la fonction suivante pourra vous être utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_yl7ufPerTg"
   },
   "outputs": [],
   "source": [
    "def ensure_2d_column(x):\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        return x.reshape(-1, 1)  # (N,) → (N, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS1PIWzyLw_I",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U56MXo4uLyd8"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    # impute the array by its means\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    x_imputed = mean_imputer.fit_transform(xmiss)\n",
    "\n",
    "    # select column to impute\n",
    "    col = order[0]\n",
    "    other_cols = [c for c in order if c != col]\n",
    "\n",
    "    # select indices where col is observed\n",
    "    obs_indices = (mask[:, col] == 0)\n",
    "    mis_indices = (mask[:, col] == 1)\n",
    "\n",
    "    # fit random forest\n",
    "    rf = RandomForestRegressor(n_estimators=10, max_depth=3)\n",
    "    rf.fit(X=ensure_2d_column(x_imputed[obs_indices, other_cols]), y=x_imputed[obs_indices, col])\n",
    "    # get new prediction\n",
    "    pred = rf.predict(X=ensure_2d_column(x_imputed[mis_indices, other_cols]))\n",
    "    # replace in array\n",
    "    x_imputed[mis_indices, col] = pred\n",
    "\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaGB04ptQktb",
    "outputId": "03d29272-7d62-45b4-d676-bec54046fe32"
   },
   "outputs": [],
   "source": [
    "ximp_manrf = impute_manualrandomforest(xmiss)\n",
    "print(mse(ximp_manrf, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7tFthxlfD74"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8nodeY_fHlS"
   },
   "source": [
    "L'algorithme consiste à répéter l'étape précédente en bouclant sur toutes les colonnes, autant de fois qu'il est nécessaire jusqu'à ce que le critère d'arrêt soit vérifié ou que le nombre maximal de boucles soit atteint.\n",
    "\n",
    "Le critère d'arrêt est vérifié lorsque la différence entre deux imputations successives est inférieure à un seuil. La fonction de différence est fournie ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HosN1kWGgUht"
   },
   "outputs": [],
   "source": [
    "def difference(x_new, x_old):\n",
    "  return np.sum((x_new - x_old) ** 2) / np.sum(x_new ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXu-zsbcfHwB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_lgsXTPfGh2"
   },
   "outputs": [],
   "source": [
    "def impute_manualrandomforest(xmiss, max_iter=10, tol=0.001):\n",
    "    mask = np.isnan(xmiss)\n",
    "    # get order of columns by increasing number of nans\n",
    "    order = np.argsort(np.isnan(xmiss).sum(axis=0))\n",
    "\n",
    "    # impute the array by its means\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    x_imputed = mean_imputer.fit_transform(xmiss)\n",
    "\n",
    "    # no more than max_iter loops\n",
    "    for iteration in range(max_iter):\n",
    "        # loop over columns to impute\n",
    "        for col in order:\n",
    "            # save a copy to measure the difference between 2 successive imputations\n",
    "            x_imputed_old = np.copy(x_imputed)\n",
    "\n",
    "            other_cols = [c for c in order if c != col]\n",
    "\n",
    "            # select indices where col is observed\n",
    "            obs_indices = (mask[:, col] == 0)\n",
    "            mis_indices = (mask[:, col] == 1)\n",
    "\n",
    "            # fit random forest\n",
    "            rf = RandomForestRegressor(n_estimators=10, max_depth=3)\n",
    "            rf.fit(X=ensure_2d_column(x_imputed[obs_indices, other_cols]), y=x_imputed[obs_indices, col])\n",
    "            # get new prediction\n",
    "            pred = rf.predict(X=ensure_2d_column(x_imputed[mis_indices, other_cols]))\n",
    "            # replace in array\n",
    "            x_imputed[mis_indices, col] = pred\n",
    "\n",
    "            diff = difference(x_imputed, x_imputed_old)\n",
    "            if diff < tol:\n",
    "                return x_imputed\n",
    "\n",
    "    warnings.warn(\"max_iter was reached.\")\n",
    "    return x_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7tTKG73izZI",
    "outputId": "73882a88-6cd6-4a15-af43-6a61595e732b"
   },
   "outputs": [],
   "source": [
    "ximp_manrf = impute_manualrandomforest(xmiss)\n",
    "print(mse(ximp_manrf, xfull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bbs5s7v1u-9"
   },
   "source": [
    "# Exercice 3 : comparaison des méthodes d'imputation dans un jeu de données réel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfWzwi3337iI"
   },
   "source": [
    "Dans cet exercice, vous considérez le même jeu de données réel complet *Breast Cancer Wisconsin* que dans le TP1 (exercice 4). On génère 30% de valeurs manquantes de type MCAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iuclBDi4b1-"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "xfull = data['data']  # covariates, without missing values\n",
    "diagnosis = data['target']  # target variable to predict, when the learning task is prediction\n",
    "features_names = data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "RcZjnLOt4cpH",
    "outputId": "b8c3ef70-9df7-4582-a082-258b3dbe65ba"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xfull, columns=features_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prTC9bzu4fC1"
   },
   "outputs": [],
   "source": [
    "n, d = xfull.shape  # data dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4g3OYDuY-km"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "p = 0.3\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p\n",
    "  xmiss[miss_id, j] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxVaL8wqn6WV"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQHFFym_n7wQ"
   },
   "source": [
    "Appliquez les méthodes d'imputation vues dans l'exercice 1 à ce jeu de données, en optimisant les hyperparamètres :\n",
    "* imputation par la moyenne\n",
    "* imputation par plus proches voisins\n",
    "* imputation itérative à base de régression linéaire\n",
    "* imputation itérative à base de forêt aléatoire\n",
    "\n",
    "Comparez leurs MSE : quelle est la meilleure méthode pour ce jeu de données ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfauxruroNoC",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kht9iQUtoO0l"
   },
   "outputs": [],
   "source": [
    "def evaluate_imputer(imputer, xfull, xmiss):\n",
    "    ximp = imputer.fit_transform(xmiss)\n",
    "    score = mse(ximp, xfull)\n",
    "    print(f\"{imputer.__str__():<60}: MSE = {score:.6f}\")\n",
    "    return score\n",
    "\n",
    "def compare_imputers(xfull, xmiss):\n",
    "    results = {}\n",
    "\n",
    "    # 1. SimpleImputer (mean)\n",
    "    simple = SimpleImputer(strategy='mean')\n",
    "    score = evaluate_imputer(simple, xfull, xmiss)\n",
    "    results['SimpleImputer (mean)'] = score\n",
    "\n",
    "    # 2. KNNImputer (optimize k)\n",
    "    for k in [3, 5, 10, 15]:\n",
    "        knn = KNNImputer(n_neighbors=k)\n",
    "        score = evaluate_imputer(knn, xfull, xmiss)\n",
    "        results[f'KNNImputer (best_k={k})'] = score\n",
    "\n",
    "    # 3. IterativeImputer + LinearRegression (optimize tol)\n",
    "    for tol in [0.1, 0.01, 0.001, 0.0001]:\n",
    "        iter_lr = IterativeImputer(estimator=LinearRegression(), tol=tol)\n",
    "        score = evaluate_imputer(iter_lr, xfull, xmiss)\n",
    "        results[f'IterativeImputer + LR (tol={tol})'] = score\n",
    "\n",
    "    # 4. IterativeImputer + RandomForestRegressor (optimize depth, tol)\n",
    "    param_grid = ParameterGrid({\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'tol': [0.1, 0.01]\n",
    "    })\n",
    "    for params in param_grid:\n",
    "        rf = RandomForestRegressor(n_estimators=10, max_depth=params['max_depth'], random_state=0)\n",
    "        iter_rf = IterativeImputer(estimator=rf, max_iter=10, tol=params['tol'])\n",
    "        score = evaluate_imputer(iter_rf, xfull, xmiss)\n",
    "        key = f\"IterativeImputer + RF (depth={params['max_depth']}, tol={params['tol']})\"\n",
    "        results[key] = score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrd9M2AEqgcX",
    "outputId": "79cd5743-ca70-43b2-ba0d-e969852b38e4"
   },
   "outputs": [],
   "source": [
    "results = compare_imputers(xfull, xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nx5i26ZuwcM",
    "outputId": "5f7fba12-d1f8-41f3-bfab-4e64662b3fc9"
   },
   "outputs": [],
   "source": [
    "for name, score in sorted(results.items(), key=lambda x: x[1]):\n",
    "    print(f\"{name:<45}: MSE = {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSkUfTZxvrV2"
   },
   "source": [
    "Parmi les modèles testés, la meilleure imputation est réalisée par l'imputation itérative. Le choix de l'estimateur de base, ainsi que le choix des hyperparamètres ne semble pas prépondérant, ou alors devrait être soumis à une procédure de sélection de modèles plus poussée.\n",
    "\n",
    "La faible performance de l'imputation par plus proches voisins s'explique très probablement par la grande dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEpQSSO5dS1d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "semipy-venv",
   "language": "python",
   "name": "semipy-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
