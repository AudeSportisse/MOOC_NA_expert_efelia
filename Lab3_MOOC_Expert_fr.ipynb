{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wPsCwp6aKTnD",
   "metadata": {
    "id": "wPsCwp6aKTnD"
   },
   "source": [
    "# Travaux Pratiques 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KqupnOnON5Iz",
   "metadata": {
    "id": "KqupnOnON5Iz"
   },
   "source": [
    "Dans ce TP, vous allez manipuler des méthodes d'estimation basées sur la vraisemblance.\n",
    "\n",
    "En présence de valeurs manquantes, on maximise la vraisemblance observée :\n",
    "$$\\theta^{ML} \\in \\textrm{argmax}_\\theta \\: L_{{\\mathrm{{obs}}}}(\\theta;X_{\\mathrm{obs}(M)}),$$\n",
    "\n",
    "$$\\mathrm{avec} \\: L_{{\\mathrm{{obs}}}}(\\theta;X_{\\mathrm{obs}(M)})={\\int} p(X;\\theta) {dX_{\\mathrm{mis}(M)}}.$$\n",
    "\n",
    "Cela permet d'estimer le paramètre $\\theta$ de la distribution des données $p(X; \\theta)$ et possiblement de prédire les valeurs manquantes.\n",
    "\n",
    "\n",
    "L'exercice 1 vous permettra de découvrir l'algorithme Espérance-Maximisation dans un cas bivarié Gaussien pour estimer les paramètres en présence de valeurs manquantes. Dans l'exercice 2, vous verrez pourquoi et comment ces méthodes peuvent aussi être utilisées pour faire de l'imputation. Dans l'exercice 3, vous manipulerez les méthodes de rang faible, et dans l'exercice 4, une méthode utilisant l'apprentissage profond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lMRqaWG2T_DZ",
   "metadata": {
    "id": "lMRqaWG2T_DZ"
   },
   "source": [
    "# Importation de librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TIz9NU5chuBK",
   "metadata": {
    "id": "TIz9NU5chuBK"
   },
   "outputs": [],
   "source": [
    "### Classical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "### Specific for missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "### Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "### Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "### Pytorch for exercice 4 (deep learning)\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adK13Y2BQdDq",
   "metadata": {
    "id": "adK13Y2BQdDq"
   },
   "source": [
    "# Exercice 1: algorithme Espérance-Maximisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y_oI1M3oUeHX",
   "metadata": {
    "id": "y_oI1M3oUeHX"
   },
   "source": [
    "Considérons un échantillon bivarié (*i.e.* à $d=2$ variables) gaussien,\n",
    "\n",
    "$$\n",
    "\\left( X_{i.} \\right)_{1\\leq i\\leq n}\n",
    "=\\left( X_{i0}, X_{i1} \\right)_{1\\leq i\\leq n}\n",
    "$$\n",
    "\n",
    "de taille $n$, de moyenne $\\mu$ et de matrice de covariance $\\Sigma$, en notant :\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\n",
    "  \\mu_0 \\\\[6pt] \\mu_1\n",
    "\\end{bmatrix}, \\quad\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "  \\sigma_{0} & \\sigma_{01} \\\\[6pt]\n",
    "  \\sigma_{01} & \\sigma_{1}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "Supposons également qu'il y a des valeurs manquantes de type Missing Completely At Random (MCAR) sur $X_{.1}$: le manque des données ne dépend pas des valeurs des données elles-mêmes et le mécanisme de données manquantes peut donc être ignoré dans l'analyse statistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zOpveSthgSIS",
   "metadata": {
    "id": "zOpveSthgSIS"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n = 100\n",
    "d = 2\n",
    "mu0 = 5.\n",
    "mu1 = 1.\n",
    "sig0 = 1.\n",
    "sig1 = 1.\n",
    "sig01 = 0.5\n",
    "\n",
    "mean = np.array([mu0, mu1])\n",
    "cov = np.array([\n",
    "    [sig0, sig01],\n",
    "    [sig01, sig1]\n",
    "    ])\n",
    "\n",
    "xfull = np.random.multivariate_normal(mean, cov, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5nh7dMK8glAT",
   "metadata": {
    "id": "5nh7dMK8glAT"
   },
   "outputs": [],
   "source": [
    "p = 0.4\n",
    "\n",
    "xmiss = np.copy(xfull)\n",
    "miss_id = (np.random.uniform(0, 1, size=n) < p)\n",
    "xmiss[miss_id, 1] = np.nan\n",
    "\n",
    "M = np.isnan(xmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C-cdOVebUEJo",
   "metadata": {
    "id": "C-cdOVebUEJo"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], hue=M[:, 1], palette=['#d1e5f0', '#2166ac'])\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.set_title('MCAR')\n",
    "ax.set_xlabel(r'$X_{.0}$')\n",
    "ax.set_ylabel(r'$X_{.1}$')\n",
    "ax.legend(handles, ['Observed', 'Missing'], loc='lower right', fontsize='13')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0UwnUw9Ctv-_",
   "metadata": {
    "id": "0UwnUw9Ctv-_"
   },
   "source": [
    "## Question 1 : estimateur du maximum de vraisemblance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3Skv7TIinsb",
   "metadata": {
    "id": "o3Skv7TIinsb"
   },
   "source": [
    "La log-vraisemblance observée à maximiser est\n",
    "$$\\ell_{\\textrm{obs}}(\\theta;X_{.0},X_{\\mathrm{obs}(M_{.1})1})=\\sum_{i=1}^n \\int \\log p(X_{i.};\\theta) dX_{\\mathrm{mis}(M_{i1})1},$$\n",
    "où $\\theta=(\\mu,\\Sigma)$. Par convention, on a $X_{i1}=X_{\\mathrm{mis}(M_{i1})1}$ si $M_{i1}=1$, c'est-à-dire lorsque $X_{i1}$ est manquant. De même, on a $X_{i1}=X_{\\mathrm{obs}(M_{i1})1}$ si $M_{i1}=0$, c'est-à-dire lorsque $X_{i1}$ est observé.\n",
    "Dans la suite, vous pouvez considérer, sans perdre de généralité, que les $r$ premières observations sont observées et les $n-r$ restantes sont manquantes, c'est-à-dire $M_{i1}=0$ pour $i=1,\\dots,r$ et $M_{i1}=1$ pour $i=r+1,\\dots,n$.\n",
    "\n",
    "Vous allez montrer dans la question 1a que son expression est, à des constantes près:\n",
    "$$\\ell(\\mu,\\Sigma;X_{.0},X_{\\mathrm{obs}(M_{.1})1})=-\\frac{n}{2}\\log(\\sigma_{0}^2)-\\frac{1}{2}\\sum_{i=1}^{n}\\frac{(X_{i0}-\\mu_0)^2}{\\sigma_{0}^2}\n",
    "-\\frac{r}{2}\\log\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2 \\\\\n",
    "-\\frac{1}{2}\\sum_{i=1}^{r}\\frac{(X_{i1}-\\mu_1-\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0))^2}{\\left(\\sigma_{11}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2}$$\n",
    "et dans la question 1b, vous allez donner l'expression du maximum de vraisemblance. La question 1c est de l'implémentation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VUc5EUDot_jl",
   "metadata": {
    "id": "VUc5EUDot_jl"
   },
   "source": [
    "### Question 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zK9CTo5i8Qy",
   "metadata": {
    "id": "1zK9CTo5i8Qy"
   },
   "source": [
    "Voici les premières étapes pour obtenir la vraisemblance observée.\n",
    "\n",
    "\\begin{align}\n",
    "\\ell_{\\textrm{obs}}(\\theta;X_{.0},X_{\\mathrm{obs}(M_{.1})1})&=\\sum_{i=1}^n \\int \\log p(X_{i.};\\theta) dX_{\\mathrm{mis}(M_{.1})1} \\\\\n",
    "&=\\sum_{i=1}^n \\int \\log p(X_{i0})p(X_{i0}|X_{i1};\\theta) dX_{\\mathrm{mis}(M_{.1})1} \\\\\n",
    "&= \\sum_{i=1}^n \\log p(X_{i0}) + \\sum_{i=1}^n \\int \\log p(X_{i0}|X_{i1};\\theta) dX_{\\mathrm{mis}(M_{.1})1}\n",
    "\\end{align}\n",
    "\n",
    "Détaillez les deux termes, en utilisant pour le second les formules classiques d'un vecteur gaussien rappelées ci-dessous:\n",
    "\n",
    "$X_{i1}|X_{i0} \\sim N(\\mathbb{E}[X_{i1}|X_{i0}],\\mathrm{Var}(X_{i1}|X_{i0}))$ avec\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[X_{i1}|X_{i0}]&=\\mu_1+\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0) \\\\\n",
    "\\mathrm{Var}(X_{i1}|X_{i0})&=\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43G84aijV_nb",
   "metadata": {
    "id": "43G84aijV_nb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vhCu8O9CV-5w",
   "metadata": {
    "id": "vhCu8O9CV-5w"
   },
   "source": [
    "Pour le premier terme, comme $X_{.0}\\sim N(\\mu_0,\\sigma_{0})$, on a simplement\n",
    "$\\sum_{i=1}^n \\log p(X_{i0};\\theta)= -\\sum_{i=1}^n \\frac{(X_{i0}-\\mu_0)^2}{2\\sigma_{0}^2}-\\frac{n}{2}\\log\\sigma_{0}^2$.\n",
    "\n",
    "Pour le second terme, il y a deux cas. Si $X_{i1}$ est manquant, l'intégrale vaut 1 (densité intégrée).\n",
    "Si $X_{i1}$ est observé (càd pour $X_{i1}$ avec $i=1,\\dots,r$), le terme $p(X_{i1}|X_{i1};\\theta)$ sort de l'intégrale et on utilise les formules classiques pour obtenir $-\\frac{r}{2}\\log\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2\n",
    "-\\frac{1}{2}\\sum_{i=1}^{r}\\frac{(X_{i1}-\\mu_1-\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0))^2}{\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tl25WZuLi9eK",
   "metadata": {
    "id": "Tl25WZuLi9eK"
   },
   "source": [
    "### Question 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CmvOgca7dBij",
   "metadata": {
    "id": "CmvOgca7dBij"
   },
   "source": [
    "L'estimateur du maximum de vraisemblance est\n",
    "$\\theta^{ML} \\in \\textrm{argmin}_\\theta \\: \\ell_{\\textrm{obs}}(\\theta;X_{.0},X_{\\mathrm{obs}(M_{.1})1})$. Donnez son expression pour estimer la moyenne, en considérant $\\Sigma$ connu pour simplifier les calculs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7GL1UCXJdeeE",
   "metadata": {
    "id": "7GL1UCXJdeeE",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53asbJZ0dlSc",
   "metadata": {
    "id": "53asbJZ0dlSc"
   },
   "source": [
    "À $\\Sigma$ fixé (connu), on dérive la vraisemblance observée par rapport à $\\mu$, les expressions suivantes sont obtenues:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mu_0} \\ell(\\mu,\\Sigma;X_{.0},X_{.1})&=\\sum_{i=1}^{n}\\frac{X_{i0}-\\mu_0}{\\sigma_{0}^2}\n",
    " \\\\\n",
    " \\nabla_{\\mu_1}\\ell(\\mu,\\Sigma;X_{.0},X_{.1})&=\\sum_{i=1}^{r}\\frac{X_{i1}-\\mu_1-\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0)}{\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mu_0} \\ell(\\mu,\\Sigma;X_{.0},X_{.1})=0 &\\Longleftrightarrow {\\mu}^{ML}_0 = \\frac{1}{n}\\sum_{i=1}^n X_{i0} \\\\\n",
    "\\nabla_{\\mu_1} \\ell(\\mu,\\Sigma;X_{.0},X_{.1})=0 &\\Longleftrightarrow {\\mu}^{ML}_1 = \\frac{1}{r}\\sum_{i=1}^r X_{i1} + \\frac{\\sigma_{01}}{\\sigma_{0}}\\left({\\mu}_0^{ML}-\\frac{1}{r} \\sum_{i=1}^r X_{i0}\\right),\n",
    "\\end{align*}\n",
    "où on remplace $\\mu_0$ par son estimateur ${\\mu}^{ML}_0$ dans l'expression de ${\\mu}^{ML}_1$ (méthode de plug-in). On ne fait pas la preuve de concavité pour la vraisemblance $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XvRn8oKpjYkm",
   "metadata": {
    "id": "XvRn8oKpjYkm"
   },
   "source": [
    "### Question 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5lzAV5rjZx8",
   "metadata": {
    "id": "e5lzAV5rjZx8"
   },
   "source": [
    "Implémentez l'estimateur du maximum de vraisemblance pour la moyenne à partir de l'expression trouvée en question 1b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682pwD8xhSll",
   "metadata": {
    "id": "682pwD8xhSll",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-EwGdAqzf7Yo",
   "metadata": {
    "id": "-EwGdAqzf7Yo"
   },
   "outputs": [],
   "source": [
    "mu0_ML = np.mean(xmiss[:, 0])\n",
    "mu1_ML = np.mean(xmiss[~miss_id, 1]) + sig01 / sig0 * (mu0_ML - np.mean(xmiss[~miss_id, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DJuCn9b1hD1Z",
   "metadata": {
    "id": "DJuCn9b1hD1Z"
   },
   "outputs": [],
   "source": [
    "print(\"Estimateur de mu0 :\", mu0_ML)\n",
    "print(\"Estimateur de mu1 :\", mu1_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LJcZT6pitwrg",
   "metadata": {
    "id": "LJcZT6pitwrg"
   },
   "source": [
    "## Question 2 : Etape E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ntjDpmf_Vjfu",
   "metadata": {
    "id": "ntjDpmf_Vjfu"
   },
   "source": [
    "Comme vous avez vu dans la vidéo du module 3, le paramètre $\\theta$ peut aussi être estimé de manière itérative avec l'algorithme EM.\n",
    "\n",
    "Il y a une étape d'initialisation où l'on obtient $\\theta^{(0)}$, puis deux étapes répétées jusqu'à convergence :\n",
    "* l'étape E, où l'espérance conditionnelle suivante est calculée (itération $t$):\n",
    "$$Q(\\theta;{\\theta}^{(t)})=\\mathbb{E}[\\quad \\ell(\\theta;X)\\quad\\big|X_{.0},X_{\\mathrm{obs}(M_{.1})1},{\\theta}^{(t)}],$$\n",
    "avec $\\ell(\\theta;X)=\\sum_{i=1}^n\\log p(X;\\theta)$ la log-vraisemblance complète.\n",
    "\n",
    "* l'étape M, où l'espérance conditionnelle est maximisée pour mettre à jour les paramètres\n",
    "$${\\theta}^{(t+1)}\\in\\textrm{argmax}_\\theta \\: Q(\\theta;{\\theta}^{(t)})$$\n",
    "\n",
    "Vous allez détailler l'étape E.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mCu0XPjVbZvq",
   "metadata": {
    "id": "mCu0XPjVbZvq"
   },
   "source": [
    "La log-vraisemblance complète est donnée ci-dessous:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\theta;X)&=-\\frac{n}{2}\\log(\\mathrm{det}(\\Sigma))-\\frac{1}{2}\\sum_{i=1}^n\\begin{pmatrix} X_{i0}-\\mu_0 & X_{i1}-\\mu_1\n",
    "\\end{pmatrix}\\Sigma^{-1}\\begin{pmatrix} X_{i0}-\\mu_0 & X_{i1}-\\mu_1\n",
    "\\end{pmatrix}^T \\\\\n",
    "&= \\frac{n}{2}\\log(\\mathrm{det}(\\Sigma))-\\frac{1}{2}\\sum_{i=1}^n (X_{i0}-\\mu_0)^2\\tilde{\\sigma}_{0} + 2(X_{i0}-\\mu_0)(X_{i1}-\\mu_1)(\\tilde{\\sigma}_{01})^2 + (X_{i1}-\\mu_1)^2\\tilde{\\sigma}_{1},\n",
    "\\end{align*}\n",
    "avec $\\Sigma^{-1}=\\begin{pmatrix} \\tilde{\\sigma}_{0} & \\tilde{\\sigma}_{01} \\\\\n",
    "\\tilde{\\sigma}_{01} & \\tilde{\\sigma}_{1}  \n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UrxiQdLNcQN-",
   "metadata": {
    "id": "UrxiQdLNcQN-"
   },
   "source": [
    "$\\ell(\\theta;X)$ est linéaire en les quantités suivantes: $\\sum_{i=1}^n X_{i0}$, $\\sum_{i=1}^n X_{i0}^2$, $\\sum_{i=1}^n X_{i1}$, $\\sum_{i=1}^n X_{i1}^2$ et $\\sum_{i=1}^n X_{i0}X_{i1}$.\n",
    "Pour obtenir l'expression de l'espérance conditionnelle, il suffit donc de calculer les quantités suivantes:\n",
    "\\begin{align*}\n",
    "s_j&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{ij}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right], j=0,1 \\\\\n",
    "s_{jj}&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{ij}^2|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right], j=0,1 \\\\\n",
    "s_{jk}&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{ij}X_{ik}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right], j=0, k=1\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z-_B9FnccNJ2",
   "metadata": {
    "id": "z-_B9FnccNJ2"
   },
   "source": [
    "### Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bZLA2SJKehHc",
   "metadata": {
    "id": "bZLA2SJKehHc"
   },
   "source": [
    "Calculez $s_0$ et $s_{00}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T93oZU0demjM",
   "metadata": {
    "id": "T93oZU0demjM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2F-piYyXenjX",
   "metadata": {
    "id": "2F-piYyXenjX"
   },
   "source": [
    "Les expressions pour $s_0$ et $s_{00}$ sont les plus faciles à obtenir. En effet, on conditionne par rapport à $X_{.0}$, on obtient donc :\n",
    "$$s_0 = \\sum_{i=1}^n X_{i0}, \\quad  s_{00} = \\sum_{i=1}^n X_{i0}^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fxt2t2ldfUtB",
   "metadata": {
    "id": "Fxt2t2ldfUtB"
   },
   "source": [
    "### Question 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "br58u7L4fV-4",
   "metadata": {
    "id": "br58u7L4fV-4"
   },
   "source": [
    "Calculez $s_1$. La technique est la même que pour le calcul du second terme de la question 1a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J7iK937Nfex9",
   "metadata": {
    "id": "J7iK937Nfex9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vCVSSsWMhwgm",
   "metadata": {
    "id": "vCVSSsWMhwgm"
   },
   "source": [
    "\\begin{align*}\n",
    "        \\mathbb{E}\\left[ X_{i1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]&=  \\int X_{i1} p(X_{\\textrm{mis}(M_{i1})1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)})dX_{\\textrm{mis}(M_{i1})1} \\\\\n",
    "        &=\\begin{cases}\n",
    "\t\tX_{\\textrm{obs}(M_{i1})1}  &\\textrm{ si } X_{i1} \\textrm{ est observé} \\\\\n",
    "\t\t\\int X_{\\textrm{mis}(M_{i1})1} p(X_{\\textrm{mis}(M_{i1})1}|X_{i0},{\\theta}^{(t)})dX_{\\textrm{mis}(M_{i1})1} &\\textrm{ sinon.}\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "Dans le premier cas, si $X_{i1}$ est observé, $X_{i1}=X_{\\textrm{obs}(M_{i1})1}$ et peut être sorti de l'intégrale, qui vaut $1$. Dans le second cas, c'est simplement l'espérance de la distribution conditionnele de $X_{i1}$ sachant $X_{i0}$. Pour cela, on peut utiliser les formules classiques des vecteurs gaussiens (rappelées en question 1a) et obtenir :\n",
    "\\begin{align*}\n",
    "        \\mathbb{E}\\left[ X_{i1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]&= \\begin{cases}\n",
    "\t\tX_{i1} &\\textrm{ si } X_{i1} \\textrm{ est observé} \\\\\n",
    "\t\t\\mu_1^{(t)} + \\frac{\\sigma_{01}^{(t)}}{\\sigma_{0}^{(t)}}( X_{i0}-\\mu_0^{(t)}) &\\textrm{ sinon.}\n",
    "\t\\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3gOGHjxNgIDt",
   "metadata": {
    "id": "3gOGHjxNgIDt"
   },
   "source": [
    "Pour les termes $s_{11}$ et $s_{01}$, on emploie exactement la même stratégie que dans la question 2b pour obtenir:\n",
    "\\begin{align}\n",
    "s_{11}&=\\mathbb{E}\\left[ \\sum_{i=1}^nX_{i1}^2|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]=\\sum_{i=1}^{r} X_{i1}^2 + \\sum_{i=r+1}^n \\left(\\left(\\mu_1^{(t)} + \\frac{\\sigma^{(t)}_{01}}{\\sigma^{(t)}_{0}}( X_{i0}-\\mu^{(t)}_0)\\right)^2+\\sigma^{(t)}_{1}-\\frac{(\\sigma^{(t)}_{01})^2}{\\sigma^{(t)}_{0}}\\right) \\\\\n",
    "s_{01}&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{i0}X_{i1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]=\\sum_{i=1}^{r} X_{i0}X_{i1} + \\sum_{i=r+1}^n X_{i0}\\left(\\mu^{(t)}_1 +\\frac{\\sigma^{(t)}_{01}}{\\sigma^{(t)}_{0}}\\left( X_{i0}-\\mu^{(t)}_0\\right)\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4FjGov4Zt1pg",
   "metadata": {
    "id": "4FjGov4Zt1pg"
   },
   "source": [
    "## Question 3: Etape M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAzDO4v08LLs",
   "metadata": {
    "id": "wAzDO4v08LLs"
   },
   "source": [
    "Sans détailler les calculs, comment pouvez-vous maximiser la vraisemblance à l'étape M ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j7EkJAXQ8a2s",
   "metadata": {
    "id": "j7EkJAXQ8a2s",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lVP67ln78b9G",
   "metadata": {
    "id": "lVP67ln78b9G"
   },
   "source": [
    "On retrouve l'estimateur classique du maximum de vraisemblance dans le cas Gaussien (voir par exemple section 2.3.4 du\n",
    "[livre de Bishop, 2006](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)), en remplaçant les quantités impliquant des valeurs manquantes par les espérances conditionnelles calculées dans la question 2.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mu_0^{(t+1)}&=\\frac{s_0}{n} \\\\\n",
    "    \\mu_1^{(t+1)}&=\\frac{s_1}{n} \\\\\n",
    "    \\sigma_{0}^{(t+1)}&=\\frac{s_{00}}{n}-(\\mu_0^{(t+1)})^2 \\\\\n",
    "    \\sigma_{1}^{(t+1)}&=\\frac{s_{11}}{n}-(\\mu_1^{(t+1)})^2 \\\\\n",
    "    \\sigma_{01}^{(t+1)}&=\\frac{s_{01}}{n}-(\\mu_0^{(t+1)}\\mu_1^{(t+1)})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AmnHPKJha7ko",
   "metadata": {
    "id": "AmnHPKJha7ko"
   },
   "source": [
    "## Question 4: Mise en pratique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DZzGWofb-v17",
   "metadata": {
    "id": "DZzGWofb-v17"
   },
   "source": [
    "### Question 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3pKRaqjp_JDn",
   "metadata": {
    "id": "3pKRaqjp_JDn"
   },
   "source": [
    "Il faut d'abord trouver une initialisation pour $\\theta^{(0)}$. Vous allez initialiser l'algorithme avec les quantités empiriques (moyenne et matrice de variance-covariance) calculées sur les lignes complètes. Ce n'est pas toujours une solution viable (par exemple, lorsqu'il y a trop de valeurs manquantes), une autre solution peut être d'imputer les valeurs manquantes par une méthode pas trop coûteuse et de calculer les quantités empiriques pour $\\theta^{(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_KW5e5zw-xHu",
   "metadata": {
    "id": "_KW5e5zw-xHu"
   },
   "source": [
    "Ecrivez le code qui permet l'initialisation. Que remarquez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cBr1HgRlBd8B",
   "metadata": {
    "id": "cBr1HgRlBd8B",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CLPDIZBiBe0_",
   "metadata": {
    "id": "CLPDIZBiBe0_"
   },
   "source": [
    "Les résultats pour la matrice de variance-covariance $\\Sigma$ sont biaisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NgHU5w4y_z7c",
   "metadata": {
    "id": "NgHU5w4y_z7c"
   },
   "outputs": [],
   "source": [
    "mu_init = np.nanmean(xmiss,axis=0)\n",
    "mu_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ev5PxGdOAdnk",
   "metadata": {
    "id": "Ev5PxGdOAdnk"
   },
   "outputs": [],
   "source": [
    "Sigma_init = np.array(pd.DataFrame(xmiss).cov())\n",
    "Sigma_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8tWo0U_GCo",
   "metadata": {
    "id": "ef8tWo0U_GCo"
   },
   "source": [
    "### Question 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gX8aEs-QBma5",
   "metadata": {
    "id": "gX8aEs-QBma5"
   },
   "source": [
    "Ecrivez deux fonctions pour l'étape E et M de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uWzUgQ23CAwE",
   "metadata": {
    "id": "uWzUgQ23CAwE"
   },
   "outputs": [],
   "source": [
    "def Estep(xmiss, mu, Sigma, miss_id):\n",
    "\n",
    "    ### TO COMPLETE ###\n",
    "\n",
    "    return {\n",
    "        's0': np.sum(s0),\n",
    "        's1': np.sum(s1),\n",
    "        's00': np.sum(s00),\n",
    "        's11': np.sum(s11),\n",
    "        's01': np.sum(s01)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WhU24guzDPxs",
   "metadata": {
    "id": "WhU24guzDPxs"
   },
   "outputs": [],
   "source": [
    "def Mstep(xmiss, s0, s1, s00, s11, s01):\n",
    "\n",
    "    ### TO COMPLETE ###\n",
    "\n",
    "    return {'mu': mu, 'Sigma': Sigma}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U7UcirzeBzPX",
   "metadata": {
    "id": "U7UcirzeBzPX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xv7IgbytCBlX",
   "metadata": {
    "id": "Xv7IgbytCBlX"
   },
   "outputs": [],
   "source": [
    "def Estep(xmiss, mu, Sigma, miss_id):\n",
    "\n",
    "    n = xmiss.shape[0]\n",
    "\n",
    "    # For the variable X0\n",
    "    s0 = xmiss[:, 0]\n",
    "    s00 = xmiss[:, 0] ** 2\n",
    "\n",
    "    s1 = np.zeros(n)\n",
    "    s11 = np.zeros(n)\n",
    "\n",
    "    # For observed values of X1\n",
    "    s1[~miss_id] = xmiss[~miss_id, 1]\n",
    "    s11[~miss_id] = xmiss[~miss_id, 1] ** 2\n",
    "\n",
    "    # For missing values of X1\n",
    "    s1[miss_id] = mu[1] + (Sigma[0,1] / Sigma[0,0]) * (xmiss[miss_id, 0] - mu[0])\n",
    "    s11[miss_id] = (\n",
    "        s1[miss_id] ** 2 +\n",
    "        Sigma[1,1] - (Sigma[0,1] ** 2) / Sigma[0,0]\n",
    "    )\n",
    "\n",
    "    s01 = s0 * s1\n",
    "\n",
    "    return {\n",
    "        's0': np.sum(s0),\n",
    "        's1': np.sum(s1),\n",
    "        's00': np.sum(s00),\n",
    "        's11': np.sum(s11),\n",
    "        's01': np.sum(s01)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bmZVfRYmDW08",
   "metadata": {
    "id": "bmZVfRYmDW08"
   },
   "outputs": [],
   "source": [
    "def Mstep(xmiss, s0, s1, s00, s11, s01):\n",
    "\n",
    "    n = xmiss.shape[0]\n",
    "\n",
    "    mu0 = s0 / n\n",
    "    mu1 = s1 / n\n",
    "\n",
    "    sig0 = s00 / n - mu0 ** 2\n",
    "    sig1 = s11 / n - mu1 ** 2\n",
    "    sig01 = s01 / n - mu0 * mu1\n",
    "\n",
    "    mu = np.array([mu0, mu1])\n",
    "    Sigma = np.array([\n",
    "        [sig0, sig01],\n",
    "        [sig01, sig1]\n",
    "    ])\n",
    "\n",
    "    return {'mu': mu, 'Sigma': Sigma}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03TBNn4gF2sw",
   "metadata": {
    "id": "03TBNn4gF2sw"
   },
   "source": [
    "### Question 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WYD9skLlF4Dy",
   "metadata": {
    "id": "WYD9skLlF4Dy"
   },
   "source": [
    "Appliquez l'algorithme sur 50 itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hb5Kzjc8JUIj",
   "metadata": {
    "id": "hb5Kzjc8JUIj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MTEK-wtpF_X-",
   "metadata": {
    "id": "MTEK-wtpF_X-"
   },
   "outputs": [],
   "source": [
    "mu_EM = mu_init\n",
    "Sigma_EM = Sigma_init\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "  # E-step\n",
    "  res_Estep = Estep(xmiss, mu_EM, Sigma_EM, miss_id)\n",
    "  s0 = res_Estep[\"s0\"]\n",
    "  s1 = res_Estep[\"s1\"]\n",
    "  s00 = res_Estep[\"s00\"]\n",
    "  s11 = res_Estep[\"s11\"]\n",
    "  s01 = res_Estep[\"s01\"]\n",
    "\n",
    "  # M-step\n",
    "  res_Mstep = Mstep(xmiss, s0, s1, s00, s11, s01)\n",
    "  mu_EM = res_Mstep[\"mu\"]\n",
    "  Sigma_EM = res_Mstep[\"Sigma\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "px5dKIQjGtzT",
   "metadata": {
    "id": "px5dKIQjGtzT"
   },
   "outputs": [],
   "source": [
    "mu_EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6qpbaMDcGgGm",
   "metadata": {
    "id": "6qpbaMDcGgGm"
   },
   "outputs": [],
   "source": [
    "Sigma_EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5wQWOPI2Ql-v",
   "metadata": {
    "id": "5wQWOPI2Ql-v"
   },
   "source": [
    "# Exercice 2: estimer pour imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qxJU8XQdQxtx",
   "metadata": {
    "id": "qxJU8XQdQxtx"
   },
   "source": [
    "Dans ce court exercice, vous allez imputer les valeurs manquantes à partir de l'estimation du paramètre $\\theta$ de la distribution des données $p(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Auu9PnIpmQT_",
   "metadata": {
    "id": "Auu9PnIpmQT_"
   },
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3GegQYVSmSQs",
   "metadata": {
    "id": "3GegQYVSmSQs"
   },
   "source": [
    "Dans le cas de l'exercice 1 (bivarié gaussien), proposez une stratégie pour imputer, et donc tirer un échantillon suivant la loi $p(X_{i1}|X_{i0})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692SnGY4qBB",
   "metadata": {
    "id": "0692SnGY4qBB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TlZ19TMP4rCb",
   "metadata": {
    "id": "TlZ19TMP4rCb"
   },
   "source": [
    "Dans le cas Gaussien bivarié, cette loi est explicite. On peut de nouveau utiliser les formules classiques d'un vecteur Gaussien rappelées en question 1a.\n",
    "\n",
    "On a:\n",
    "$X_{i1}|X_{i0} \\sim N(\\mathbb{E}[X_{i1}|X_{i0}],\\mathrm{Var}(X_{i1}|X_{i0}))$ avec\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[X_{i1}|X_{i0}]&=\\mu_1+\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0) \\\\\n",
    "\\mathrm{Var}(X_{i1}|X_{i0})&=\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UvgaqyQI4k2e",
   "metadata": {
    "id": "UvgaqyQI4k2e"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TG1cTAKG4mRC",
   "metadata": {
    "id": "TG1cTAKG4mRC"
   },
   "source": [
    "Implémentez la stratégie choisie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4t51EgEM4oA3",
   "metadata": {
    "id": "4t51EgEM4oA3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6sBjQqF4pUJ",
   "metadata": {
    "id": "c6sBjQqF4pUJ"
   },
   "outputs": [],
   "source": [
    "def imputation_EM(mu, Sigma, xmiss, miss_id):\n",
    "\n",
    "  ximp = np.copy(xmiss)\n",
    "\n",
    "  for i in range(xmiss.shape[0]):\n",
    "    if miss_id[i]:\n",
    "      mean = mu[1] + (Sigma[0, 1] / Sigma[0, 0]) * (xmiss[i, 0] - mu[0])\n",
    "      cov = Sigma[1, 1] - (Sigma[0, 1] ** 2) / Sigma[0,0]\n",
    "      ximp[i, 1] = np.random.normal(mean, cov, size=1)\n",
    "\n",
    "  return ximp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R5uJ33muQKPc",
   "metadata": {
    "id": "R5uJ33muQKPc"
   },
   "outputs": [],
   "source": [
    "ximp_EM = imputation_EM(mu_EM, Sigma_EM, xmiss, miss_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BdWwpIbjScB1",
   "metadata": {
    "id": "BdWwpIbjScB1"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=ximp_EM[:, 0], y=ximp_EM[:, 1], hue=M[:, 1], palette=['#d1e5f0', '#2166ac'])\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.set_title('MCAR values imputed with the EM algorithm')\n",
    "ax.set_xlabel(r'$X_{.0}$')\n",
    "ax.set_ylabel(r'$X_{.1}$')\n",
    "ax.legend(handles, ['Observed', 'Imputed'], loc='lower right', fontsize='13')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Es24dccORfo3",
   "metadata": {
    "id": "Es24dccORfo3"
   },
   "source": [
    "# Exercice 3: méthodes de rang faible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XBTaPtoEVy3W",
   "metadata": {
    "id": "XBTaPtoEVy3W"
   },
   "source": [
    "Dans cet exercice, vous allez implémenter l'algorithme présenté dans la vidéo du module qui propose de prédire les valeurs manquantes avec une Analyse en Composantes Principales (ACP) itérative. En question 2, vous verrez plus de détails sur une de ses variantes, qui gère mieux le bruit dans les données, appelée `missMDA`: cette méthode est implémentée dans un package `R` très utilisé en pratique. En question 3, vous implémenterez vous-mêmes une autre de ses variantes, l'algorithme `softImpute`.\n",
    "\n",
    "Dans les méthodes à rang faible, on fait l'hypothèse que la matrice de données peut être décomposée en une matrice à rang faible et un bruit gaussien.\n",
    "$$X = \\Theta + \\epsilon,\n",
    "$$\n",
    "avec $\\epsilon\\sim N(0_d,\\sigma^2I_{d\\times d})$.\n",
    "\n",
    "Pour prédire les valeurs manquantes dans $X$, on va estimer $\\Theta$ en utilisant l'algorithme d'ACP itérative (présenté dans la vidéo). L'algorithme prend en entrée $r$, le nombre de dimensions à garder pour l'ACP, ainsi que le jeu de données incomplet $X^\\textrm{NA}$.\n",
    "\n",
    "- Il y a une étape d'**initialisation** d'imputation naïve (par exemple, par la moyenne ou même l'imputation par 0). On obtient alors un jeu de donnée complet $X^{(0)}$.\n",
    "\n",
    "Deux étapes sont ensuites effectuées de manière itérative.\n",
    "\n",
    "- l'**étape d'estimation**: on estime $\\theta$ avec la décomposition en valeurs singulières pour laquelle on ne garde que les $r$ premières dimensions.\n",
    "$$\\Theta^{(t)}=\\text{SVD}_{\\textbf{r}}({X}^{(t)})=U_{{\\textbf{r}}}D_{{\\textbf{r}}}V_{{\\textbf{r}}}^t,$$\n",
    "\n",
    "- l'**étape d'imputation**: les valeurs manquantes sont prédites par la valeur de $\\Theta^{(t)}$.\n",
    "\n",
    "$$X^{(t+1)}=X \\odot (\\mathbf{1}_{n\\times d}-{M}) + \\: \\Theta^{(t)} \\odot M.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SaFTqIRXhTuD",
   "metadata": {
    "id": "SaFTqIRXhTuD"
   },
   "source": [
    "Dans cet exercice, on va considérer le même jeu de données réel complet *Breast Cancer Wisconsin* que dans les modules précédents (exercice 4 du module 1 et exercice 3 du module 2). On introduit 30% de valeurs manquantes de type MCAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "igHdsiBZhxOI",
   "metadata": {
    "id": "igHdsiBZhxOI"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "xfull = data['data'] # covariates, without missing values\n",
    "diagnosis = data['target']   # target variable to predict, when the learning task is prediction\n",
    "features_names = data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hnk70tSwh27O",
   "metadata": {
    "id": "Hnk70tSwh27O"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xfull, columns=features_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SgXNTcB1h4V8",
   "metadata": {
    "id": "SgXNTcB1h4V8"
   },
   "outputs": [],
   "source": [
    "n, d = xfull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hiBKVibIh58C",
   "metadata": {
    "id": "hiBKVibIh58C"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "p = 0.3\n",
    "xmiss = np.copy(xfull)\n",
    "for j in range(d):\n",
    "  miss_id = np.random.uniform(0, 1, size=n) < p\n",
    "  xmiss[miss_id, j] = np.nan\n",
    "mask = np.isnan(xmiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5Q_k5Zam9vv",
   "metadata": {
    "id": "M5Q_k5Zam9vv"
   },
   "source": [
    "Vous allez vous concentrer sur la tâche d'imputation et vous calculerez donc les erreurs quadratiques moyennes (MSE) pour évaluer les méthodes (score introduit dans le module 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iOOqZ0AmnPOY",
   "metadata": {
    "id": "iOOqZ0AmnPOY"
   },
   "outputs": [],
   "source": [
    "def mse(x_imp, x_true):\n",
    "  n = len(x_true)\n",
    "  return (1 / n) * np.sum((x_imp - x_true) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W4YeAyljcmtN",
   "metadata": {
    "id": "W4YeAyljcmtN"
   },
   "source": [
    "## Question 1: ACP itérative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "if7K23CZKnp1",
   "metadata": {
    "id": "if7K23CZKnp1"
   },
   "source": [
    "### Question 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fVNPjq8f_OD",
   "metadata": {
    "id": "3fVNPjq8f_OD"
   },
   "source": [
    "Proposez une imputation naïve par la moyenne et une autre par 0 dans des fonctions que vous appelez `init_mean_imputation` et `init_zero_imputation`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qR8av2eAgDCS",
   "metadata": {
    "id": "qR8av2eAgDCS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OvkKLP3ngRsv",
   "metadata": {
    "id": "OvkKLP3ngRsv"
   },
   "outputs": [],
   "source": [
    "def init_mean_imputation(xmiss, mask):\n",
    "    mean_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "    ximp = mean_imputer.fit_transform(xmiss)\n",
    "    return ximp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2SyvZKiRKqlc",
   "metadata": {
    "id": "2SyvZKiRKqlc"
   },
   "outputs": [],
   "source": [
    "def init_zero_imputation(xmiss, mask):\n",
    "    ximp = xmiss.copy()\n",
    "    ximp[mask] = 0\n",
    "    return ximp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fs-JFJYfKUpe",
   "metadata": {
    "id": "Fs-JFJYfKUpe"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(init_mean_imputation(xmiss, mask)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MDjUNAWFKrlB",
   "metadata": {
    "id": "MDjUNAWFKrlB"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(init_zero_imputation(xmiss, mask)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cf4_C2bKgh_A",
   "metadata": {
    "id": "Cf4_C2bKgh_A"
   },
   "source": [
    "### Question 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tTISLuG0gjMw",
   "metadata": {
    "id": "tTISLuG0gjMw"
   },
   "source": [
    "Implémentez l'algorithme de l'ACP itérative dans la fonction suivante `iterative_ACP` qui prend comme arguments:\n",
    "* `xmiss`: le jeu de données manquant,\n",
    "* `r`: le nombre de dimensions à garder pour l'ACP,\n",
    "* `maxit`: le nombre maximum d'itérations de l'algorithme. Un critère de convergence sera défini en question 3.\n",
    "\n",
    "Vous utiliserez l'imputation par la moyenne comme initialisation.\n",
    "\n",
    "Pour la décomposition en valeurs singulières, vous pouvez utilisez la fonction `linalg.svd` de `numpy` (documentation disponible [ici](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QSSmV_cfmXLp",
   "metadata": {
    "id": "QSSmV_cfmXLp"
   },
   "outputs": [],
   "source": [
    "def iterative_ACP(xmiss, r, maxit):\n",
    "\n",
    "    ximp = ...\n",
    "\n",
    "    return ximp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gncdymG7l0D_",
   "metadata": {
    "id": "gncdymG7l0D_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PpNzqQauXX9y",
   "metadata": {
    "id": "PpNzqQauXX9y"
   },
   "outputs": [],
   "source": [
    "def iterative_ACP(xmiss, r, maxit):\n",
    "\n",
    "    mask = np.isnan(xmiss)\n",
    "\n",
    "    ximp = init_mean_imputation(xmiss, mask)\n",
    "\n",
    "    for i in range(maxit):\n",
    "        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n",
    "        d_r = d[:r]\n",
    "        U_r = U[:, :r]\n",
    "        V_r = V[:r, :]\n",
    "        D_r = np.diag(d_r)\n",
    "        svd_r = np.dot(U_r, np.dot(D_r, V_r))\n",
    "        ximp[mask] = svd_r[mask]\n",
    "\n",
    "    return ximp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GwYgnGn8l3Fi",
   "metadata": {
    "id": "GwYgnGn8l3Fi"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iterative_ACP(xmiss, 5, 50)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iM2-YseKn1h0",
   "metadata": {
    "id": "iM2-YseKn1h0"
   },
   "source": [
    "### Question 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Us2xw8PGn3A9",
   "metadata": {
    "id": "Us2xw8PGn3A9"
   },
   "source": [
    "Calculez la MSE, et comparez-la avec celle obtenue en utilisation une imputation par la moyenne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwsjE2hzn9cL",
   "metadata": {
    "id": "DwsjE2hzn9cL",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5vXvC-8Mn-tk",
   "metadata": {
    "id": "5vXvC-8Mn-tk"
   },
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "ximp_mean = mean_imputer.fit_transform(xmiss)\n",
    "mse_mean = mse(ximp_mean,xfull)\n",
    "print(\"MSE imputation par la moyenne:\", mse_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hl0gPD6Aopg5",
   "metadata": {
    "id": "Hl0gPD6Aopg5"
   },
   "outputs": [],
   "source": [
    "ximp_ACP = iterative_ACP(xmiss, 5, 1000)\n",
    "mse_ACP = mse(ximp_ACP, xfull)\n",
    "print(\"MSE ACP itérative:\", mse_ACP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r0k9u_tJD5Hl",
   "metadata": {
    "id": "r0k9u_tJD5Hl"
   },
   "source": [
    "On obtient une MSE légèrement plus faible que celle obtenue en imputant par la moyenne. On peut se demander si le nombre de dimension $r$ à garder dans l'ACP a été bien choisi ici."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bRkD9L7ttvYI",
   "metadata": {
    "id": "bRkD9L7ttvYI"
   },
   "source": [
    "### Question 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VRKn2P00twsf",
   "metadata": {
    "id": "VRKn2P00twsf"
   },
   "source": [
    "Pour choisir le nombre optimal $r$ de dimensions à garder pour l'ACP, une solution est de parcourir une grille de valeurs possibles et choisir le $r_\\mathrm{opt}$ qui minimise la MSE sur cette grille.\n",
    "\n",
    "Implémentez cette stratégie en utilisant la fonction suivante `additional_na`. Elle permet d'ajouter des valeurs manquantes de type MCAR de telle sorte à ce qu'il y ait au moins encore une valeur observée dans chaque ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O1jpRJDKwGzA",
   "metadata": {
    "id": "O1jpRJDKwGzA"
   },
   "outputs": [],
   "source": [
    "def additional_na(xmiss, mask):\n",
    "\n",
    "    xmiss_addna = np.copy(xmiss)\n",
    "\n",
    "    for i in range(xmiss.shape[0]):\n",
    "        idx_obs = np.argwhere(mask[i, :] == 0).reshape((-1))\n",
    "        var_obs = np.random.choice(idx_obs) # choice of the observed variable\n",
    "        idx_candidates = idx_obs[idx_obs != var_obs]\n",
    "        var_miss = np.random.uniform(0, 1, size=len(idx_candidates)) < 0.5\n",
    "        missing_idx = idx_candidates[var_miss]\n",
    "        xmiss_addna[i, missing_idx] = np.nan\n",
    "    mask_addna = np.isnan(xmiss_addna)\n",
    "\n",
    "    return(xmiss_addna, mask_addna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uom5CJ47FnFJ",
   "metadata": {
    "id": "uom5CJ47FnFJ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PuZLSIkh4elz",
   "metadata": {
    "id": "PuZLSIkh4elz"
   },
   "outputs": [],
   "source": [
    "def choose_rank(xmiss, grid_r):\n",
    "\n",
    "    mask = np.isnan(xmiss)\n",
    "\n",
    "    list_error = []\n",
    "    for r in grid_r:\n",
    "        xmiss_addna, mask_addna = additional_na(xmiss, mask)\n",
    "        ximp_ACP = iterative_ACP(xmiss_addna, r, 1000)\n",
    "\n",
    "        list_error.append(np.sqrt(np.nanmean((ximp_ACP.flatten() - xmiss.flatten())**2)))\n",
    "\n",
    "    return list_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RVUfJSI45YP6",
   "metadata": {
    "id": "RVUfJSI45YP6"
   },
   "outputs": [],
   "source": [
    "grid_r = list(range(1, d))\n",
    "res_rank = choose_rank(xmiss, grid_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UmumP1M1BctC",
   "metadata": {
    "id": "UmumP1M1BctC"
   },
   "outputs": [],
   "source": [
    "r_opt = grid_r[np.argmin(res_rank)]\n",
    "ximp_ACP = iterative_ACP(xmiss, r_opt, 1000)\n",
    "mse_ACP = mse(ximp_ACP, xfull)\n",
    "print(\"MSE ACP itérative:\", mse_ACP)\n",
    "print(\"r optimal:\", r_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbQAnZkNGfqR",
   "metadata": {
    "id": "cbQAnZkNGfqR"
   },
   "source": [
    "Le $r$ optimal choisi est 1. On utilise donc une projection des données sur une seule dimension pour pouvoir prédire les valeurs manquantes. On perd beaucoup d'information et une piste serait de mieux traiter le bruit contenu dans les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4LE5sNc1cn2R",
   "metadata": {
    "id": "4LE5sNc1cn2R"
   },
   "source": [
    "## Question 2 : ACP itérative régularisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "od6ASyDQMSjz",
   "metadata": {
    "id": "od6ASyDQMSjz"
   },
   "source": [
    "Dans l'algorithme d'ACP itérative, les valeurs manquantes sont remplacées par : $$\\sum_{k=1}^{r}\\sigma_k(X)u_{ik}v_{jk},$$\n",
    "pour $i=1,\\dots,n$ et $j=1,\\dots,p$.\n",
    "\n",
    "Pour régulariser, l'algorithme `missMDA` propose de remplacer les valeurs manquantes par :   $$\\sum_{k=1}^{r}\\left(\\sigma_k(X)-\\frac{{\\sigma}^2}{\\sigma_k(X)}\\right)u_{ik}v_{jk}.$$\n",
    "\n",
    "Cette méthode est implémentée en langage `R` dans le package `missMDA`. Des exemples d'utilisation sont disponibles sur ce [lien](http://factominer.free.fr/missMDA/index.html) et l'article associé est disponible [ici](https://www.jstatsoft.org/article/view/v070i01).\n",
    "\n",
    "Faites une liste des hyperparamètres à choisir dans la méthode `missMDA`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NyEZeM2oPH_N",
   "metadata": {
    "id": "NyEZeM2oPH_N",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k9-sNqmlPJm2",
   "metadata": {
    "id": "k9-sNqmlPJm2"
   },
   "source": [
    "Les hyperparamètres sont $r$ (le nombre de dimensions à garder dans l'ACP) et $\\sigma^2$ (la variance du bruit supposé gaussien).\n",
    "\n",
    "Pour estimer $r$, on peut utiliser une méthode semblable à celle choisie dans la question 1. Pour estimer $\\sigma^2$, il a été proposé d'utiliser la somme des carrés des résidus divisé par $nd-\\#\\textrm{param}$, où $n$ et $d$ sont les dimensions de la matrice de données et $\\#\\textrm{param}=nr+pr-r^2$, le nombe de paramètres à estimer (ici nombre de paramètres dans une décomposition en valeurs singulières). Plus de détails sont disponibles [ici](https://arxiv.org/pdf/1602.01206).\n",
    "\n",
    "Notons qu'il y a une fonction `estim_ncpPCA` du package `missMDA`qui propose une validation croisée pour estimer le nombre de dimensions à garder dans l'ACP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hPZGRCyvcwdu",
   "metadata": {
    "id": "hPZGRCyvcwdu"
   },
   "source": [
    "## Question 3: ACP itérative à seuil doux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BUSdIGnuHAIs",
   "metadata": {
    "id": "BUSdIGnuHAIs"
   },
   "source": [
    "Une autre méthode qui permet de traiter le bruit dans les données est d'utiliser un seuillage doux avec l'algorithme `softImpute`. Tout comme `missMDA`, l'algorithme permet de mieux tenir compte du bruit contenu dans les données que l'ACP itérative classique, en remplaçant les valeurs manquantes par:\n",
    "$$\\sum_{k=1}^{d}\\max((\\sigma_k(X)-\\lambda),0)u_{ik}v_{jk},$$\n",
    "avec $\\lambda>0$.\n",
    "\n",
    "L'estimation de $\\theta$ est régularisée dans le sens où le problème d'optimisation devient ici:\n",
    "\n",
    "$$\\theta \\in \\textrm{argmin}_\\theta \\| (X - \\Theta) \\odot (\\mathbf{1}_{n\\times d}-M)\\| + \\lambda \\|\\Theta\\|_\\star,$$\n",
    "\n",
    "avec $\\|.\\|$ la norme nucléaire.\n",
    "\n",
    "L'article associé à cette méthode est disponible [ici](https://arxiv.org/pdf/1410.2596). L'algorithme est implémenté en langage `R` dans le package `softImpute`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A5pRKx4kILRh",
   "metadata": {
    "id": "A5pRKx4kILRh"
   },
   "source": [
    "### Question 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZWawufDTXKxx",
   "metadata": {
    "id": "ZWawufDTXKxx"
   },
   "source": [
    "Quels sont les hyperparamètres ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rfs-xNBIXLch",
   "metadata": {
    "id": "Rfs-xNBIXLch",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X69uIxZgXMRv",
   "metadata": {
    "id": "X69uIxZgXMRv"
   },
   "source": [
    "L'hyperparamètre est $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9l5v3HZ5XWq0",
   "metadata": {
    "id": "9l5v3HZ5XWq0"
   },
   "source": [
    "### Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cQcwSaSGIMhI",
   "metadata": {
    "id": "cQcwSaSGIMhI"
   },
   "source": [
    "Implémentez cette méthode en reprenant cette base de code. Vous utiliserez l'imputation par zéro codée en question 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qrisKEyDUQgi",
   "metadata": {
    "id": "qrisKEyDUQgi"
   },
   "outputs": [],
   "source": [
    "def softimpute(xmiss, lamb, maxit = 1000):\n",
    "\n",
    "    mask = np.isnan(xmiss)\n",
    "\n",
    "    ximp = ...\n",
    "\n",
    "    for i in range(maxit):\n",
    "        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n",
    "        d_thresh = ...\n",
    "        r = ...\n",
    "        d_thresh = d_thresh[:r]\n",
    "        U_thresh = U[:, :r]\n",
    "        V_thresh = V[:r, :]\n",
    "        D_thresh = np.diag(d_r)\n",
    "        svd_r = np.dot(U_thresh, np.dot(D_thresh, V_thresh))\n",
    "        ximp[mask] = svd_r[mask]\n",
    "\n",
    "    return ximp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eWe2k6N0UVGT",
   "metadata": {
    "id": "eWe2k6N0UVGT",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I-FDc2s9UWJj",
   "metadata": {
    "id": "I-FDc2s9UWJj"
   },
   "outputs": [],
   "source": [
    "def softimpute(xmiss, lamb, maxit = 1000):\n",
    "\n",
    "    mask = np.isnan(xmiss)\n",
    "\n",
    "    ximp = init_mean_imputation(xmiss, mask)\n",
    "\n",
    "    for i in range(maxit):\n",
    "        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n",
    "        d_thresh = np.maximum(d - lamb, 0)\n",
    "        r = (d_thresh > 0).sum()\n",
    "        d_thresh = d_thresh[:r]\n",
    "        U_thresh = U[:, :r]\n",
    "        V_thresh = V[:r, :]\n",
    "        D_thresh = np.diag(d_thresh)\n",
    "        svd_r = np.dot(U_thresh, np.dot(D_thresh, V_thresh))\n",
    "        ximp[mask] = svd_r[mask]\n",
    "\n",
    "    return ximp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "klLaOmRjpA44",
   "metadata": {
    "id": "klLaOmRjpA44"
   },
   "outputs": [],
   "source": [
    "ximp_soft = softimpute(xmiss, 100, maxit = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wca5WGezpbBt",
   "metadata": {
    "id": "Wca5WGezpbBt"
   },
   "outputs": [],
   "source": [
    "mse(ximp_soft,xfull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wC0b1tXbU80p",
   "metadata": {
    "id": "wC0b1tXbU80p"
   },
   "source": [
    "### Question 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zeID2Uu_U-Jm",
   "metadata": {
    "id": "zeID2Uu_U-Jm"
   },
   "source": [
    "Pour éviter de faire des itérations non nécessaires, on peut utiliser le critère de convergence implémenté ci-dessous, qui compare l'écart entre deux itérés. Si l'écart est plus faible qu'un seuil `conv_thresh`(fixé à $10^{-5}$ par défaut), l'algorithme a convergé.\n",
    "\n",
    "Incorporez ce critère de convergence dans la fonction `softimpute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FIOXwsp8VJDl",
   "metadata": {
    "id": "FIOXwsp8VJDl"
   },
   "outputs": [],
   "source": [
    "def converged(x_t, x_tplus1, mask, conv_thresh):\n",
    "\n",
    "    x_t_na = x_t[mask]\n",
    "    x_tplus1_na = x_tplus1[mask]\n",
    "    rmse = np.sqrt(np.sum((x_t_na - x_tplus1_na) ** 2))\n",
    "    denom = np.sqrt((x_t_na ** 2).sum())\n",
    "\n",
    "    return (rmse / denom) < conv_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diXZlQTOVuYk",
   "metadata": {
    "id": "diXZlQTOVuYk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7PlqFrVwzx",
   "metadata": {
    "id": "bd7PlqFrVwzx"
   },
   "outputs": [],
   "source": [
    "def softimpute(xmiss, lamb, maxit = 1000, conv_thresh = 1e-5):\n",
    "\n",
    "    mask = np.isnan(xmiss)\n",
    "\n",
    "    ximp = init_mean_imputation(xmiss, mask)\n",
    "\n",
    "    for i in range(maxit):\n",
    "        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n",
    "        d_thresh = np.maximum(d - lamb, 0)\n",
    "        r = (d_thresh > 0).sum()\n",
    "        d_thresh = d_thresh[:r]\n",
    "        U_thresh = U[:, :r]\n",
    "        V_thresh = V[:r, :]\n",
    "        D_thresh = np.diag(d_thresh)\n",
    "        svd_r = np.dot(U_thresh, np.dot(D_thresh, V_thresh))\n",
    "        if converged(ximp, svd_r, mask, conv_thresh):\n",
    "            break\n",
    "        ximp[mask] = svd_r[mask]\n",
    "\n",
    "    return ximp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JR6DKHosXagH",
   "metadata": {
    "id": "JR6DKHosXagH"
   },
   "source": [
    "### Question 3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cZ_KlySEXb8X",
   "metadata": {
    "id": "cZ_KlySEXb8X"
   },
   "source": [
    "Utilisez la fonction `choose_lambda` ci-dessous, qui parcourt une grille de valeurs pour $\\lambda$ et utilise la fonction `additional_na` définie en question 1d.\n",
    "\n",
    "`grid_len` permet de fixer la taille de la grille des valeurs.\n",
    "La grille de valeurs est définie dans la fonction `choose_lambda`, en calculant la valeur singulière maximale du jeu de données imputé par 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "br0CSmHQqBw8",
   "metadata": {
    "id": "br0CSmHQqBw8"
   },
   "outputs": [],
   "source": [
    "def choose_lambda(xmiss, grid_len = 15, maxit = 1000, conv_thresh = 1e-5):\n",
    "\n",
    "    mask = np.isnan(xmiss)\n",
    "    ximp_0 = init_zero_imputation(xmiss, mask)\n",
    "\n",
    "    # generate grid for lambda values\n",
    "    d = np.linalg.svd(ximp_0, compute_uv=False, full_matrices=False) # svd on imputed dataset with 0\n",
    "    lambda_max = np.max(d)\n",
    "    lambda_min = 0.001 * lambda_max\n",
    "    grid_lambda = np.exp(np.linspace(np.log(lambda_min), np.log(lambda_max), grid_len).tolist())\n",
    "\n",
    "    cv_error = []\n",
    "    for lamb in grid_lambda:\n",
    "        xmiss_addna, mask_maskna = additional_na(xmiss, mask)\n",
    "        ximp_soft = softimpute(xmiss_addna, lamb, maxit, conv_thresh)\n",
    "        cv_error.append(np.sqrt(np.nanmean((ximp_soft.flatten() - xmiss.flatten()) ** 2)))\n",
    "\n",
    "    return cv_error, grid_lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TMUwZwo5YTB1",
   "metadata": {
    "id": "TMUwZwo5YTB1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KasS9Pc0q2LM",
   "metadata": {
    "id": "KasS9Pc0q2LM"
   },
   "outputs": [],
   "source": [
    "res_lambda, grid_lambda = choose_lambda(xmiss, grid_len = 15)\n",
    "lambda_opt = grid_lambda[np.argmin(res_lambda)]\n",
    "ximp_soft = softimpute(xmiss, lambda_opt)\n",
    "mse_soft = mse(ximp_soft, xfull)\n",
    "print(\"MSE softImpute:\", mse_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SeS9FNXxQ3zV",
   "metadata": {
    "id": "SeS9FNXxQ3zV"
   },
   "source": [
    "# Exercice 4: méthode utilisant l'apprentissage profond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uxBbVz-ucWiG",
   "metadata": {
    "id": "uxBbVz-ucWiG"
   },
   "source": [
    "Dans cet exercice, vous allez prédire les valeurs manquantes du jeu de données *Breast Cancer Wisconsin* (de l'exercice 3) en utilisant la méthode `MIWAE`. L'article de recherche qui propose cette méthode est disponible [ici](https://arxiv.org/pdf/1812.02633). Des notebooks sont disponibles sur le répertoire Github [ici](https://github.com/pamattei/miwae/tree/master). Le code est très largement repris de [ce notebook](https://github.com/pamattei/miwae/blob/master/Tensorflow%202%20notebooks/MIWAE_UCI_single_multiple-imputation.ipynb).\n",
    "\n",
    "Il n'y a pas de questions d'implémentation dans cet exercice, le but est de mieux appréhender la méthode. Les calculs sont possibles sur CPU ou GPU mais seront plus rapides si un GPU est disponible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cC6jWkiVD478",
   "metadata": {
    "id": "cC6jWkiVD478"
   },
   "source": [
    "`MIWAE` utilise un modèle profond à variables latentes. Soit $Z \\in \\mathbb{R}^{d_{\\textrm{lat}}}$ les variables latentes. L'idée va être d'encoder l'information contenue dans les données dans l'espace latent (de plus petite dimension, $d_{\\textrm{lat}} < d$). L'information est ensuite décodée pour revenir dans l'espace des données, avec une distribution $p_\\theta(X|Z)$ paramétrée par $f_\\theta(Z)$, entrainé par un réseau de neurones.\n",
    "\n",
    "`MIWAE` propose d'estimer le paramètre $\\theta$ en calculant une borne $L_K$ de la vraisemblance observée, telle que  $L_K \\leq L_{{\\mathrm{{obs}}}}$. Cette borne tire des échantillons pour les variables latentes selon une distribution $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$, qui encode les données dans l'espace latent, et qui est paramétré par $g_\\gamma(X_{\\mathrm{obs}(M)})$, entrainé par un autre réseau de neurones.\n",
    "$$L_K(\\theta,\\gamma)=\\sum_{i=1}^n \\mathbb{E}_{Z_{i1},\\dots,Z_{iK}\\sim p_\\gamma(Z|X_{\\mathrm{obs}(M)})}\\left[\\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_\\theta(X_{\\mathrm{obs}(M)}|Z_{ik})p(Z_{ik})}{{p_\\gamma(Z_{ik}|X_{\\mathrm{obs}(M)})}}\\right].$$\n",
    "\n",
    "Finalement, le modèle génératif est le suivant:\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        Z\\sim p_\\gamma(Z|X_{\\mathrm{obs}(M)};{g_\\gamma(X_{\\mathrm{obs}(M)})}) &  \\textrm{(encodeur)} \\\\\n",
    "        X_{\\mathrm{obs}(M)} \\sim p_\\theta(X_{\\mathrm{obs}(M)}|Z;{f_\\theta(Z)}) & \\textrm{(décodeur)}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "avec $g_\\gamma(X_{\\mathrm{obs}(M)})$ et $f_\\theta(Z)$ entrainés par des réseaux de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1QhlChoGEzJ1",
   "metadata": {
    "id": "1QhlChoGEzJ1"
   },
   "source": [
    "## Question 1: hyperparamètres et modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcUA4oG7E1J7",
   "metadata": {
    "id": "qcUA4oG7E1J7"
   },
   "source": [
    "Quels sont les hyperparamètres ? Quels sont les choix à réaliser pour cette méthode d'imputation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zW5tIUL9E3AV",
   "metadata": {
    "id": "zW5tIUL9E3AV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RqysqEvlE4qv",
   "metadata": {
    "id": "RqysqEvlE4qv"
   },
   "source": [
    "Les hyperparamètres sont $d_{\\textrm{lat}}$ (la dimension de l'espace latent) et $K$ (le nombre d'échantillons pour l'échantillonnage préférentiel).\n",
    "\n",
    "Les distributions $p(Z)$, $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$ et $p_\\theta(X_{\\mathrm{obs}(M)}|Z)$ peuvent être des Gaussiennes, mais un choix plus pertinent peut être fait en fonction des cas et également en fonction du type des variables (quantitatives, catégorielles).\n",
    "\n",
    "Enfin, on doit choisir l'architecture du réseau de neurones, avec des paramètres associés, notamment :\n",
    "* le nombre de couches cachés,\n",
    "* la dimension des couches cachées dans les réseaux de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9yt8TUvdF95j",
   "metadata": {
    "id": "9yt8TUvdF95j"
   },
   "source": [
    "Dans la suite, pour les variables latentes, on va choisir une distribution gausienne standard (cette distribution a priori convient la plupart du temps) : $p(Z)=N(0_d,I_{d\\times d})$. Pour $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$, on choisit également une distribution Gaussienne avec une moyenne et une matrice de covariances diagonale. Pour $p_\\theta(X_{\\mathrm{obs}(M)}|Z)$, on prendra une distribution de Student.\n",
    "\n",
    "Les réseaux de neurones pour l'encodeur et le décodeur ont la même architecture, avec 3 couches. Par exemple, pour $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$, on suppose:\n",
    "$f_\\theta(Z)=\\sigma(W_1\\sigma(W_0 Z + b_0)+ b_1)$, où $\\sigma$ est la fonction d'activation ReLU. Les paramètres de la loi de Student sont ensuite estimés avec :\n",
    "\\begin{align*}\n",
    "\\mu_\\theta&=W_\\mu f_\\theta(Z) + b_\\mu \\quad \\textrm{(moyenne)} \\\\\n",
    "\\Sigma_\\theta&=\\textrm{Softplus}(\\textrm{Diag}(W_\\sigma f_\\theta(Z) + b_\\sigma))+10^{-3} \\quad \\textrm{(matrice de covariances)} \\\\\n",
    "\\nu_\\theta &= \\textrm{Softplus}(W_\\nu f_\\theta(Z) + b_\\nu)+3 \\quad \\textrm{(nombre de degrés de liberté)},\n",
    "\\end{align*}\n",
    "On rajoute $10^{-3}$ pour effectivement obtenir une matrice de covariances, et $3$ pour avoir au moins trois degrés de liberté et ne pas avoir des queues trop lourdes. Ces choix sont expliqués plus en détail dans le notebook de la méthode `MIWAE` cité ci-dessus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab240f8",
   "metadata": {
    "id": "0ab240f8"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # determine si CUDA est disponible (utilisation GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5PrFGmwfDWI",
   "metadata": {
    "id": "a5PrFGmwfDWI"
   },
   "outputs": [],
   "source": [
    "h = 128 # number of hidden units in (same for all MLPs)\n",
    "d_lat = 10 # dimension of the latent space\n",
    "K = 20 # number of IS during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OP6bcHpLPzvA",
   "metadata": {
    "id": "OP6bcHpLPzvA"
   },
   "outputs": [],
   "source": [
    "p_z = td.Independent(td.Normal(loc=torch.zeros(d_lat).to(device), scale=torch.ones(d_lat).to(device)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9LKFRenSQJg_",
   "metadata": {
    "id": "9LKFRenSQJg_"
   },
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "    torch.nn.Linear(d_lat, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3 * d),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lg7aPxSZQLYl",
   "metadata": {
    "id": "lg7aPxSZQLYl"
   },
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2 * d_lat),  # the encoder will output both the mean and the diagonal covariance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8kTRF_DBQqkM",
   "metadata": {
    "id": "8kTRF_DBQqkM"
   },
   "outputs": [],
   "source": [
    "def weights_init(layer): # this function will be used for initializing the neural networks\n",
    "  if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oQ_jqXSeFuu5",
   "metadata": {
    "id": "oQ_jqXSeFuu5"
   },
   "source": [
    "Ci-dessous, voici la fonction qui permet de calculer la borne $L_K$ à maximiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZZnZ-tecF22j",
   "metadata": {
    "id": "ZZnZ-tecF22j"
   },
   "outputs": [],
   "source": [
    "def miwae_loss(iota_x, mask):\n",
    "  batch_size = iota_x.shape[0]\n",
    "  out_encoder = encoder(iota_x)\n",
    "  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d_lat], scale=torch.nn.Softplus()(out_encoder[..., d_lat:(2*d_lat)])), 1)\n",
    "\n",
    "  zgivenx = q_zgivenxobs.rsample([K])\n",
    "  zgivenx_flat = zgivenx.reshape([K*batch_size, d_lat])\n",
    "\n",
    "  out_decoder = decoder(zgivenx_flat)\n",
    "  all_means_obs_model = out_decoder[..., :d]\n",
    "  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., d:(2*d)]) + 0.001\n",
    "  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*d):(3*d)]) + 3\n",
    "\n",
    "  data_flat = torch.Tensor.repeat(iota_x,[K, 1]).reshape([-1, 1])\n",
    "  tiledmask = torch.Tensor.repeat(mask,[K, 1])\n",
    "\n",
    "  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1, 1]), scale=all_scales_obs_model.reshape([-1, 1]), df=all_degfreedom_obs_model.reshape([-1, 1])).log_prob(data_flat)\n",
    "  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K * batch_size, d])\n",
    "\n",
    "  logpxobsgivenz = torch.sum(all_log_pxgivenz * (1 - tiledmask), 1).reshape([K, batch_size])\n",
    "  logpz = p_z.log_prob(zgivenx)\n",
    "  logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "  neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "  return neg_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vjO-lPVaGw89",
   "metadata": {
    "id": "vjO-lPVaGw89"
   },
   "source": [
    "## Question 2: standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gAkNXBRqGx8s",
   "metadata": {
    "id": "gAkNXBRqGx8s"
   },
   "source": [
    "Comme beaucoup de méthodes d'apprentissage profond, il faut standardiser les données.\n",
    "\n",
    "On standardise le jeu de données qui contient des valeurs manquantes avec le code suivant. Quel peut être le problème ici ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAQnVGACHK_1",
   "metadata": {
    "id": "wAQnVGACHK_1"
   },
   "outputs": [],
   "source": [
    "mean_xmiss = np.nanmean(xmiss, 0)\n",
    "std_xmiss = np.nanstd(xmiss, 0)\n",
    "xmiss = (xmiss - mean_xmiss) / std_xmiss\n",
    "\n",
    "ximp_0 = init_zero_imputation(xmiss, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mOZV6TFHIwZO",
   "metadata": {
    "id": "mOZV6TFHIwZO",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtpJg96LIzuz",
   "metadata": {
    "id": "mtpJg96LIzuz"
   },
   "source": [
    "On standardise les données en calculant moyenne et un écart-type sur les données imputées par 0. Ces quantités peuvent donc être biaisées, mais on peut faire l'hypothèse que cela n'aura pas de répercussion à la fin de l'entrainement du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L8Qto5_vJD3P",
   "metadata": {
    "id": "L8Qto5_vJD3P"
   },
   "source": [
    "## Question 3: entraînement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emwWKh2sJLLr",
   "metadata": {
    "id": "emwWKh2sJLLr"
   },
   "source": [
    "Avec quel algorithme optimiser la borne $L_K$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vY68tnuZPp-F",
   "metadata": {
    "id": "vY68tnuZPp-F",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-d72Ed3RPrDQ",
   "metadata": {
    "id": "-d72Ed3RPrDQ"
   },
   "source": [
    "On utilise l'algorithme de descente de gradient stochastique (SGD) par lots (batchs), que l'on rappelle ci-dessous en pseudo-code:\n",
    "\n",
    "`for epoch=0 in NB_epochs:` # boucle sur les époques\n",
    "*   `Tirer une permutation aléatoire des données et construire les lots de taille n_batch`\n",
    "*   `Pour chaque lot`: # boucle sur les lots\n",
    "  + `Mettre à jour les paramètres`\n",
    "$$ (\\theta^{(t+1)},\\gamma^{(t+1)})=(\\theta^{(t)},\\gamma^{(t)})-\\lambda \\: \\partial_{(\\theta,\\gamma)} L_K(\\theta^{(t)},\\gamma^{(t)}), \\quad \\textrm{avec $\\lambda$ est le pas du gradient.}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "L'utilisateur choisit le pas de gradient, le nombre d'époque et la taille des lots.\n",
    "\n",
    "En pratique, on va préférer l'utilisation de l'algorithme d'optimisation `Adam`, une extension du `SGD` avec un pas de gradient adaptatif et un momentum.\n",
    "\n",
    "Voici le code pour l'entaînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G-AMdi54Qkni",
   "metadata": {
    "id": "G-AMdi54Qkni"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1AaoODSWQeor",
   "metadata": {
    "id": "1AaoODSWQeor"
   },
   "outputs": [],
   "source": [
    "encoder.to(device) # we'll use the GPU or CPU depending on the available device\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zlfVKDSfQuQ-",
   "metadata": {
    "id": "zlfVKDSfQuQ-"
   },
   "outputs": [],
   "source": [
    "miwae_loss_train = np.array([])\n",
    "mse_train = np.array([])\n",
    "mse_train2 = np.array([])\n",
    "bs = 64 # batch size\n",
    "n_epochs = 2002 # 1 epoch = all the data are used once\n",
    "ximp_MIWAE = np.copy(ximp_0) # This will be out imputed data matrix\n",
    "\n",
    "encoder.apply(weights_init)\n",
    "decoder.apply(weights_init)\n",
    "\n",
    "for ep in range(1, n_epochs):\n",
    "  perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "  batches_data = np.array_split(ximp_0[perm,], n/bs)\n",
    "  batches_mask = np.array_split(mask[perm,], n/bs)\n",
    "  for it in range(len(batches_data)):\n",
    "    optimizer.zero_grad()\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    b_data = torch.from_numpy(batches_data[it]).float().to(device)\n",
    "    b_mask = torch.from_numpy(batches_mask[it]).float().to(device)\n",
    "    loss = miwae_loss(iota_x=b_data, mask=b_mask)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  if ep % 100 == 1:\n",
    "    print('Epoch %g' %ep)\n",
    "    print('MIWAE likelihood bound  %g' %(-np.log(K) - miwae_loss(iota_x=torch.from_numpy(ximp_0).float().to(device), mask=torch.from_numpy(mask).float().to(device)).cpu().data.numpy())) # Gradient step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YyK5Q9jANMfR",
   "metadata": {
    "id": "YyK5Q9jANMfR"
   },
   "source": [
    "## Question 4: imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dWf1PKKEXrRW",
   "metadata": {
    "id": "dWf1PKKEXrRW"
   },
   "source": [
    "La dernière étape consiste à prédire les valeurs manquantes. On va réaliser une imputation simple, mais une imputation multiple est également possible (cf [notebook spécifique](https://github.com/pamattei/miwae/blob/master/Tensorflow%202%20notebooks/MIWAE_UCI_single_multiple-imputation.ipynb)).\n",
    "\n",
    "Quelle quantité calculer pour prédire les valeurs manquantes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SOAvJYhw3Qlu",
   "metadata": {
    "id": "SOAvJYhw3Qlu",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BZx0tQ8LbdPI",
   "metadata": {
    "id": "BZx0tQ8LbdPI"
   },
   "source": [
    "   L'idée de `MIWAE` est de calculer l'espérance conditionnelle de $X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)}$ ci-dessous:\n",
    "\n",
    "   \n",
    "   \\begin{align*}\n",
    "        &\\mathbb{E}[X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)}] \\\\\n",
    "        &=\\int X_{\\mathrm{mis}(M)} p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)})dX_{\\mathrm{mis}(M)} \\\\\n",
    "        &= \\int \\int X_{\\mathrm{mis}(M)} p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)},Z)p_\\theta(Z|X_{\\mathrm{obs}(M)})dZdX_{\\mathrm{mis}(M)} \\\\\n",
    "        &= \\int \\int X_{\\mathrm{mis}(M)} \\frac{p_\\theta(Z|X_{\\mathrm{obs}(M)})}{{p_\\gamma(Z|X_{\\mathrm{obs}(M)})}}{p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)},Z)p_\\gamma(Z|X_{\\mathrm{obs}(M)})}dZdX_{\\mathrm{mis}(M)}\n",
    "    \\end{align*}\n",
    "Elle n'est pas explicite, donc on utilise un échantillonnage préférentiel dit auto-normalisé (plus de détails sur cette méthode d'échantillonnage sont disponible [ici, Section 9.2](https://artowen.su.domains/mc/)):\n",
    "$$\\sum_{l=1}^L w_l X_{\\mathrm{mis}(M)}^{(l)}$$\n",
    "avec $w_l=\\frac{r_l}{\\sum_{l=1}^L r_l}$, $r_l=\\frac{p_\\theta(X_{\\mathrm{obs}(M)}|Z^{(l)})p(Z^{(l)})}{{p_\\gamma(Z^{(l)}|X_{\\mathrm{obs}(M)})}}$, et\n",
    "$(X_{\\mathrm{mis}(M)}^{(l)},Z^{(l)}) \\overset{\\mathrm{iid}}{\\sim} {p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)},Z)p_\\gamma(Z|X_{\\mathrm{obs}(M)})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vI30MswoQnuw",
   "metadata": {
    "id": "vI30MswoQnuw"
   },
   "outputs": [],
   "source": [
    "def miwae_impute(iota_x,mask, L):\n",
    "  batch_size = iota_x.shape[0]\n",
    "  out_encoder = encoder(iota_x)\n",
    "  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d_lat], scale=torch.nn.Softplus()(out_encoder[..., d_lat:(2*d_lat)])), 1)\n",
    "\n",
    "  zgivenx = q_zgivenxobs.rsample([L])\n",
    "  zgivenx_flat = zgivenx.reshape([L * batch_size, d_lat])\n",
    "\n",
    "  out_decoder = decoder(zgivenx_flat)\n",
    "  all_means_obs_model = out_decoder[..., :d]\n",
    "  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., d:(2*d)]) + 0.001\n",
    "  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*d):(3*d)]) + 3\n",
    "\n",
    "  data_flat = torch.Tensor.repeat(iota_x,[L, 1]).reshape([-1, 1]).to(device)\n",
    "  tiledmask = torch.Tensor.repeat(mask,[L, 1]).to(device)\n",
    "\n",
    "  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1, 1]), scale=all_scales_obs_model.reshape([-1, 1]), df=all_degfreedom_obs_model.reshape([-1, 1])).log_prob(data_flat)\n",
    "  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L * batch_size, d])\n",
    "\n",
    "  logpxobsgivenz = torch.sum(all_log_pxgivenz * (1 - tiledmask), 1).reshape([L, batch_size])\n",
    "  logpz = p_z.log_prob(zgivenx)\n",
    "  logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "  xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model), 1)\n",
    "\n",
    "  imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq, 0) # these are w_1,....,w_L for all observations in the batch\n",
    "  xms = xgivenz.sample().reshape([L, batch_size, d])\n",
    "  xm = torch.einsum('ki,kij->ij', imp_weights, xms)\n",
    "\n",
    "  return xm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1qGbxEUSNFd8",
   "metadata": {
    "id": "1qGbxEUSNFd8"
   },
   "outputs": [],
   "source": [
    "ximp_MIWAE[mask] = miwae_impute(iota_x=torch.from_numpy(ximp_0).float().to(device), mask=torch.from_numpy(mask).float().to(device), L=10).cpu().data.numpy()[mask]\n",
    "ximp_MIWAE_destandardized = ximp_MIWAE * std_xmiss + mean_xmiss\n",
    "mse_MIWAE = mse(ximp_MIWAE_destandardized, xfull)\n",
    "print(\"MSE MIWAE:\", mse_MIWAE)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lMRqaWG2T_DZ",
    "5wQWOPI2Ql-v",
    "Auu9PnIpmQT_",
    "0692SnGY4qBB",
    "4t51EgEM4oA3",
    "mOZV6TFHIwZO"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "semipy-venv",
   "language": "python",
   "name": "semipy-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
