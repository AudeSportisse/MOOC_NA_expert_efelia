{"cells":[{"cell_type":"markdown","id":"wPsCwp6aKTnD","metadata":{"id":"wPsCwp6aKTnD"},"source":["# Lab 3"]},{"cell_type":"markdown","id":"KqupnOnON5Iz","metadata":{"id":"KqupnOnON5Iz"},"source":["In this lab, you will work with estimation methods based on likelihood.\n","\n","In the presence of missing values, we maximize the observed likelihood :\n","$$\\theta^{ML} \\in \\textrm{argmax}_\\theta \\: L_{{\\mathrm{{obs}}}}(\\theta;X_{\\mathrm{obs}(M)}),$$\n","\n","$$\\mathrm{with} \\: L_{{\\mathrm{{obs}}}}(\\theta;X_{\\mathrm{obs}(M)})={\\int} p(X;\\theta) {dX_{\\mathrm{mis}(M)}}.$$\n","\n","This makes it possible to estimate the parameter $\\theta$ of the data distribution $p(X; \\theta)$, and potentially to predict missing values.\n","\n","\n","Exercise 1 will introduce you to the Expectation-Maximization algorithm in the case of a bivariate Gaussian distribution, to estimate parameters in the presence of missing values. In Exercise 2, you will see why and how these methods can also be used for imputation. In Exercise 3, you will work with low-rank methods, and in Exercise 4, with a method based on deep learning."]},{"cell_type":"markdown","id":"lMRqaWG2T_DZ","metadata":{"id":"lMRqaWG2T_DZ"},"source":["# Importing libraries"]},{"cell_type":"code","execution_count":null,"id":"TIz9NU5chuBK","metadata":{"id":"TIz9NU5chuBK"},"outputs":[],"source":["### Classical libraries\n","import numpy as np\n","import pandas as pd\n","\n","### Specific for missing values\n","from sklearn.impute import SimpleImputer\n","\n","### Dataset\n","from sklearn.datasets import load_breast_cancer\n","\n","### Data visualisation\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","\n","### Pytorch for exercice 4 (deep learning)\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.distributions as td\n","from torch import optim"]},{"cell_type":"markdown","id":"adK13Y2BQdDq","metadata":{"id":"adK13Y2BQdDq"},"source":["# Exercise 1: Expectation-Maximization algorithm"]},{"cell_type":"markdown","id":"y_oI1M3oUeHX","metadata":{"id":"y_oI1M3oUeHX"},"source":["Consider a bivariate Gaussian sample (*i.e.*, with \\$d=2\\$ variables),\n","\n","$$\n","\\left( X_{i.} \\right)_{1\\leq i\\leq n}\n","=\\left( X_{i0}, X_{i1} \\right)_{1\\leq i\\leq n},\n","$$\n","\n","of size \\$n\\$, with mean vector \\$\\mu\\$ and covariance matrix \\$\\Sigma\\$, where\n","\n","$$\n","\\mu = \\begin{bmatrix}\n","  \\mu_0 \\\\[6pt] \\mu_1\n","\\end{bmatrix}, \\quad\n","\\Sigma = \\begin{bmatrix}\n","  \\sigma_{0} & \\sigma_{01} \\\\[6pt]\n","  \\sigma_{01} & \\sigma_{1}\n","\\end{bmatrix}.\n","$$\n","\n","Suppose further that there are missing values of type Missing Completely At Random (MCAR) in \\$X\\_{.1}\\$: that is, the probability of missingness does not depend on the data values themselves, so the missing-data mechanism can be ignored in the statistical analysis."]},{"cell_type":"code","execution_count":null,"id":"zOpveSthgSIS","metadata":{"id":"zOpveSthgSIS"},"outputs":[],"source":["np.random.seed(0)\n","\n","n = 100\n","d = 2\n","mu0 = 5.\n","mu1 = 1.\n","sig0 = 1.\n","sig1 = 1.\n","sig01 = 0.5\n","\n","mean = np.array([mu0, mu1])\n","cov = np.array([\n","    [sig0, sig01],\n","    [sig01, sig1]\n","    ])\n","\n","xfull = np.random.multivariate_normal(mean, cov, size=n)"]},{"cell_type":"code","execution_count":null,"id":"5nh7dMK8glAT","metadata":{"id":"5nh7dMK8glAT"},"outputs":[],"source":["p = 0.4\n","\n","xmiss = np.copy(xfull)\n","miss_id = (np.random.uniform(0, 1, size=n) < p)\n","xmiss[miss_id, 1] = np.nan\n","\n","M = np.isnan(xmiss)"]},{"cell_type":"code","execution_count":null,"id":"C-cdOVebUEJo","metadata":{"id":"C-cdOVebUEJo"},"outputs":[],"source":["ax = sns.scatterplot(x=xfull[:, 0], y=xfull[:, 1], hue=M[:, 1], palette=['#d1e5f0', '#2166ac'])\n","handles, labels  =  ax.get_legend_handles_labels()\n","ax.set_title('MCAR')\n","ax.set_xlabel(r'$X_{.0}$')\n","ax.set_ylabel(r'$X_{.1}$')\n","ax.legend(handles, ['Observed', 'Missing'], loc='lower right', fontsize='13')\n",";"]},{"cell_type":"markdown","id":"0UwnUw9Ctv-_","metadata":{"id":"0UwnUw9Ctv-_"},"source":["## Question 1 : maximum likelihood estimator"]},{"cell_type":"markdown","id":"o3Skv7TIinsb","metadata":{"id":"o3Skv7TIinsb"},"source":["The observed log-likelihood to be maximized is\n","$$\\ell_{\\textrm{obs}}(\\theta;X_{.0},X_{\\mathrm{obs}(M_{.1})1})=\\sum_{i=1}^n \\int \\log p(X_{i.};\\theta) dX_{\\mathrm{mis}(M_{i1})1},$$\n","where $\\theta=(\\mu,\\Sigma)$. By default, we have $X_{i1}=X_{\\mathrm{mis}(M_{i1})1}$ if $M_{i1}=1$, that is $X_{i1}$ is missing. Similarly, we have $X_{i1}=X_{\\mathrm{obs}(M_{i1})1}$ if $M_{i1}=0$, that is when $X_{i1}$ is observed.\n","In the following, you can consider, without loss of generality, that the first $r$ observations are observed and the remaining $n-r$ are missing, that is, $M_{i1}=0$ for $i=1,\\dots,r$ and $M_{i1}=1$ for $i=r+1,\\dots,n$.\n","\n","In Question 1a, you will show that its expression is, up to some constants:\n","$$\\ell(\\mu,\\Sigma;X_{.0},X_{\\mathrm{obs}(M_{.1})1})=-\\frac{n}{2}\\log(\\sigma_{0}^2)-\\frac{1}{2}\\sum_{i=1}^{n}\\frac{(X_{i0}-\\mu_0)^2}{\\sigma_{0}^2}\n","-\\frac{r}{2}\\log\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2 \\\\\n","-\\frac{1}{2}\\sum_{i=1}^{r}\\frac{(X_{i1}-\\mu_1-\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0))^2}{\\left(\\sigma_{11}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2}$$\n","and in Question 1b, you will provide the expression of the maximum likelihood estimator. Question 1c involves the implementation."]},{"cell_type":"markdown","id":"VUc5EUDot_jl","metadata":{"id":"VUc5EUDot_jl"},"source":["### Question 1a"]},{"cell_type":"markdown","id":"1zK9CTo5i8Qy","metadata":{"id":"1zK9CTo5i8Qy"},"source":["Here are the first steps to obtain the observed likelihood.\n","\n","\\begin{align}\n","\\ell_{\\textrm{obs}}(\\theta;X_{.0},X_{\\mathrm{obs}(M_{.1})1})&=\\sum_{i=1}^n \\int \\log p(X_{i.};\\theta) dX_{\\mathrm{mis}(M_{.1})1} \\\\\n","&=\\sum_{i=1}^n \\int \\log p(X_{i0})p(X_{i0}|X_{i1};\\theta) dX_{\\mathrm{mis}(M_{.1})1} \\\\\n","&= \\sum_{i=1}^n \\log p(X_{i0}) + \\sum_{i=1}^n \\int \\log p(X_{i0}|X_{i1};\\theta) dX_{\\mathrm{mis}(M_{.1})1}\n","\\end{align}\n","\n","Detail the two terms, using the classical formulas for a Gaussian vector recalled below for the second term :\n","\n","$X_{i1}|X_{i0} \\sim N(\\mathbb{E}[X_{i1}|X_{i0}],\\mathrm{Var}(X_{i1}|X_{i0}))$ avec\n","\\begin{align*}\n","\\mathbb{E}[X_{i1}|X_{i0}]&=\\mu_1+\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0) \\\\\n","\\mathrm{Var}(X_{i1}|X_{i0})&=\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\n","\\end{align*}"]},{"cell_type":"markdown","id":"43G84aijV_nb","metadata":{"id":"43G84aijV_nb"},"source":["### Solution"]},{"cell_type":"markdown","id":"vhCu8O9CV-5w","metadata":{"id":"vhCu8O9CV-5w"},"source":["For the first term, we have $X_{.0}\\sim N(\\mu_0,\\sigma_{0})$, then\n","$\\sum_{i=1}^n \\log p(X_{i0};\\theta)= -\\sum_{i=1}^n \\frac{(X_{i0}-\\mu_0)^2}{2\\sigma_{0}^2}-\\frac{n}{2}\\log\\sigma_{0}^2$.\n","\n","For the second term, there are two cases. If $X_{i1}$ is missing, the integral is equal to 1 (integral of a density on the whole space).\n","If $X_{i1}$ is observed (that is for $X_{i1}$ with $i=1,\\dots,r$), the term $p(X_{i1}|X_{i1};\\theta)$ can be taken out of the integral, and standard formulas are then used to obtain : $-\\frac{r}{2}\\log\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2\n","-\\frac{1}{2}\\sum_{i=1}^{r}\\frac{(X_{i1}-\\mu_1-\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0))^2}{\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2}$.\n","\n"]},{"cell_type":"markdown","id":"Tl25WZuLi9eK","metadata":{"id":"Tl25WZuLi9eK"},"source":["### Question 1b"]},{"cell_type":"markdown","id":"CmvOgca7dBij","metadata":{"id":"CmvOgca7dBij"},"source":["The maximum likelihood estimator is\n","$\\theta^{ML} \\in \\textrm{argmin}_\\theta \\: \\ell_{\\textrm{obs}}(\\theta;X_{.0},X_{\\mathrm{obs}(M_{.1})1})$. Give its expression to estimate the mean, assuming $\\Sigma$ is known to simplify the calculations."]},{"cell_type":"markdown","id":"7GL1UCXJdeeE","metadata":{"id":"7GL1UCXJdeeE"},"source":["### Solution"]},{"cell_type":"markdown","id":"53asbJZ0dlSc","metadata":{"id":"53asbJZ0dlSc"},"source":["With $\\Sigma$ fixed (known), differentiating the observed likelihood with respect to $\\mu$ yields the following expressions:\n","\n","\\begin{align*}\n","    \\nabla_{\\mu_0} \\ell(\\mu,\\Sigma;X_{.0},X_{.1})&=\\sum_{i=1}^{n}\\frac{X_{i0}-\\mu_0}{\\sigma_{0}^2}\n"," \\\\\n"," \\nabla_{\\mu_1}\\ell(\\mu,\\Sigma;X_{.0},X_{.1})&=\\sum_{i=1}^{r}\\frac{X_{i1}-\\mu_1-\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0)}{\\left(\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\\right)^2}\n","\\end{align*}\n","\n","\\begin{align*}\n","\\nabla_{\\mu_0} \\ell(\\mu,\\Sigma;X_{.0},X_{.1})=0 &\\Longleftrightarrow {\\mu}^{ML}_0 = \\frac{1}{n}\\sum_{i=1}^n X_{i0} \\\\\n","\\nabla_{\\mu_1} \\ell(\\mu,\\Sigma;X_{.0},X_{.1})=0 &\\Longleftrightarrow {\\mu}^{ML}_1 = \\frac{1}{r}\\sum_{i=1}^r X_{i1} + \\frac{\\sigma_{01}}{\\sigma_{0}}\\left({\\mu}_0^{ML}-\\frac{1}{r} \\sum_{i=1}^r X_{i0}\\right),\n","\\end{align*}\n","where $\\mu_0$ is replaced by its estimator ${\\mu}^{ML}_0$ in the expression of ${\\mu}^{ML}_1$ (plug-in method). The proof of concavity for the likelihood $\\ell$ is not provided."]},{"cell_type":"markdown","id":"XvRn8oKpjYkm","metadata":{"id":"XvRn8oKpjYkm"},"source":["### Question 1c"]},{"cell_type":"markdown","id":"e5lzAV5rjZx8","metadata":{"id":"e5lzAV5rjZx8"},"source":["Implement the maximum likelihood estimator for the mean based on the expression derived in question 1b."]},{"cell_type":"markdown","id":"682pwD8xhSll","metadata":{"id":"682pwD8xhSll"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"-EwGdAqzf7Yo","metadata":{"id":"-EwGdAqzf7Yo"},"outputs":[],"source":["mu0_ML = np.mean(xmiss[:, 0])\n","mu1_ML = np.mean(xmiss[~miss_id, 1]) + sig01 / sig0 * (mu0_ML - np.mean(xmiss[~miss_id, 0]))"]},{"cell_type":"code","execution_count":null,"id":"DJuCn9b1hD1Z","metadata":{"id":"DJuCn9b1hD1Z"},"outputs":[],"source":["print(\"Estimator of mu0 :\", mu0_ML)\n","print(\"Estimator of mu1 :\", mu1_ML)"]},{"cell_type":"markdown","id":"LJcZT6pitwrg","metadata":{"id":"LJcZT6pitwrg"},"source":["## Question 2 : E-step"]},{"cell_type":"markdown","id":"ntjDpmf_Vjfu","metadata":{"id":"ntjDpmf_Vjfu"},"source":["As seen in the video from Module 3, the parameter $\\theta$ can also be estimated iteratively using the EM algorithm.\n","\n","There is an initialization step where one obtains $\\theta^{(0)}$, followed by two steps repeated until convergence:\n","* the E-step, where the following conditional expectation is computed (at iteration $t$):\n","$$Q(\\theta;{\\theta}^{(t)})=\\mathbb{E}[\\quad \\ell(\\theta;X)\\quad\\big|X_{.0},X_{\\mathrm{obs}(M_{.1})1},{\\theta}^{(t)}],$$\n","with $\\ell(\\theta;X)=\\sum_{i=1}^n\\log p(X;\\theta)$ being the complete-data log-likelihood.\n","\n","* the M-step, where the conditional expectation is maximized to update the parameters\n","$${\\theta}^{(t+1)}\\in\\textrm{argmax}_\\theta \\: Q(\\theta;{\\theta}^{(t)})$$\n","\n","You will detail the E-step.\n","\n"]},{"cell_type":"markdown","id":"mCu0XPjVbZvq","metadata":{"id":"mCu0XPjVbZvq"},"source":["The complete-data log-likelihood is given below:\n","\n","\\begin{align*}\n","\\ell(\\theta;X)&=-\\frac{n}{2}\\log(\\mathrm{det}(\\Sigma))-\\frac{1}{2}\\sum_{i=1}^n\\begin{pmatrix} X_{i0}-\\mu_0 & X_{i1}-\\mu_1\n","\\end{pmatrix}\\Sigma^{-1}\\begin{pmatrix} X_{i0}-\\mu_0 & X_{i1}-\\mu_1\n","\\end{pmatrix}^T \\\\\n","&= \\frac{n}{2}\\log(\\mathrm{det}(\\Sigma))-\\frac{1}{2}\\sum_{i=1}^n (X_{i0}-\\mu_0)^2\\tilde{\\sigma}_{0} + 2(X_{i0}-\\mu_0)(X_{i1}-\\mu_1)(\\tilde{\\sigma}_{01})^2 + (X_{i1}-\\mu_1)^2\\tilde{\\sigma}_{1},\n","\\end{align*}\n","avec $\\Sigma^{-1}=\\begin{pmatrix} \\tilde{\\sigma}_{0} & \\tilde{\\sigma}_{01} \\\\\n","\\tilde{\\sigma}_{01} & \\tilde{\\sigma}_{1}  \n","\\end{pmatrix}$."]},{"cell_type":"markdown","id":"UrxiQdLNcQN-","metadata":{"id":"UrxiQdLNcQN-"},"source":["$\\ell(\\theta;X)$ is linear in the following quantities: $\\sum_{i=1}^n X_{i0}$, $\\sum_{i=1}^n X_{i0}^2$, $\\sum_{i=1}^n X_{i1}$, $\\sum_{i=1}^n X_{i1}^2$ and $\\sum_{i=1}^n X_{i0}X_{i1}$.\n","To obtain the expression of the conditional expectation, it is therefore sufficient to compute the following quantities:\n","\\begin{align*}\n","s_j&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{ij}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right], j=0,1 \\\\\n","s_{jj}&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{ij}^2|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right], j=0,1 \\\\\n","s_{jk}&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{ij}X_{ik}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right], j=0, k=1\n","\\end{align*}\n"]},{"cell_type":"markdown","id":"z-_B9FnccNJ2","metadata":{"id":"z-_B9FnccNJ2"},"source":["### Question 2a"]},{"cell_type":"markdown","id":"bZLA2SJKehHc","metadata":{"id":"bZLA2SJKehHc"},"source":["Calculate $s_0$ and $s_{00}$."]},{"cell_type":"markdown","id":"T93oZU0demjM","metadata":{"id":"T93oZU0demjM"},"source":["### Solution"]},{"cell_type":"markdown","id":"2F-piYyXenjX","metadata":{"id":"2F-piYyXenjX"},"source":["The expressions for $s_0$ and $s_{00}$ are the easiest to obtain. Indeed, since we are conditioning on $X_{.0}$, we directly get:\n","$$s_0 = \\sum_{i=1}^n X_{i0}, \\quad  s_{00} = \\sum_{i=1}^n X_{i0}^2.$$"]},{"cell_type":"markdown","id":"Fxt2t2ldfUtB","metadata":{"id":"Fxt2t2ldfUtB"},"source":["### Question 2b"]},{"cell_type":"markdown","id":"br58u7L4fV-4","metadata":{"id":"br58u7L4fV-4"},"source":["Calculate $s_1$. The technique is the same as the one used for computing the second term in question 1a."]},{"cell_type":"markdown","id":"J7iK937Nfex9","metadata":{"id":"J7iK937Nfex9"},"source":["### Solution"]},{"cell_type":"markdown","id":"vCVSSsWMhwgm","metadata":{"id":"vCVSSsWMhwgm"},"source":["\\begin{align*}\n","        \\mathbb{E}\\left[ X_{i1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]&=  \\int X_{i1} p(X_{\\textrm{mis}(M_{i1})1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)})dX_{\\textrm{mis}(M_{i1})1} \\\\\n","        &=\\begin{cases}\n","\t\tX_{\\textrm{obs}(M_{i1})1}  &\\textrm{ if } X_{i1} \\textrm{ is observed} \\\\\n","\t\t\\int X_{\\textrm{mis}(M_{i1})1} p(X_{\\textrm{mis}(M_{i1})1}|X_{i0},{\\theta}^{(t)})dX_{\\textrm{mis}(M_{i1})1} &\\textrm{ otherwise.}\n","\t\\end{cases}\n","\\end{align*}\n","In the first case, if $X_{i1}$ is observed, then $X_{i1} = X_{\\textrm{obs}(M_{i1})1}$ and can be taken out of the integral, which equals 1. In the second case, it is simply the expectation of the conditional distribution of $X_{i1}$ given $X_{i0}$. To compute this, one can use the standard formulas for Gaussian vectors (as recalled in question 1a), and obtain:\n","\\begin{align*}\n","        \\mathbb{E}\\left[ X_{i1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]&= \\begin{cases}\n","\t\tX_{i1} &\\textrm{ if } X_{i1} \\textrm{ is observed} \\\\\n","\t\t\\mu_1^{(t)} + \\frac{\\sigma_{01}^{(t)}}{\\sigma_{0}^{(t)}}( X_{i0}-\\mu_0^{(t)}) &\\textrm{ otherwise.}\n","\t\\end{cases}\n","\\end{align*}"]},{"cell_type":"markdown","id":"3gOGHjxNgIDt","metadata":{"id":"3gOGHjxNgIDt"},"source":["For the terms $s_{11}$ and $s_{01}$, exactly the same strategy as in question 2b is used to obtain:\n","\n","\\begin{align}\n","s_{11}&=\\mathbb{E}\\left[ \\sum_{i=1}^nX_{i1}^2|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]=\\sum_{i=1}^{r} X_{i1}^2 + \\sum_{i=r+1}^n \\left(\\left(\\mu_1^{(t)} + \\frac{\\sigma^{(t)}_{01}}{\\sigma^{(t)}_{0}}( X_{i0}-\\mu^{(t)}_0)\\right)^2+\\sigma^{(t)}_{1}-\\frac{(\\sigma^{(t)}_{01})^2}{\\sigma^{(t)}_{0}}\\right) \\\\\n","s_{01}&=\\mathbb{E}\\left[ \\sum_{i=1}^n X_{i0}X_{i1}|X_{i0},X_{\\mathrm{obs}(M_{i1})1},{\\theta}^{(t)}\\right]=\\sum_{i=1}^{r} X_{i0}X_{i1} + \\sum_{i=r+1}^n X_{i0}\\left(\\mu^{(t)}_1 +\\frac{\\sigma^{(t)}_{01}}{\\sigma^{(t)}_{0}}\\left( X_{i0}-\\mu^{(t)}_0\\right)\\right)\n","\\end{align}"]},{"cell_type":"markdown","id":"4FjGov4Zt1pg","metadata":{"id":"4FjGov4Zt1pg"},"source":["## Question 3: M-step"]},{"cell_type":"markdown","id":"wAzDO4v08LLs","metadata":{"id":"wAzDO4v08LLs"},"source":["Without detailing the calculations, how can the likelihood be maximized in the M-step?"]},{"cell_type":"markdown","id":"j7EkJAXQ8a2s","metadata":{"id":"j7EkJAXQ8a2s"},"source":["### Solution"]},{"cell_type":"markdown","id":"lVP67ln78b9G","metadata":{"id":"lVP67ln78b9G"},"source":["The classical maximum likelihood estimator in the Gaussian case is recovered by replacing the quantities involving missing values with the conditional expectations computed in question 2 (see for example Section 2.3.4 of\n","[the book of Bishop, 2006](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)).\n","\n","\\begin{align*}\n","    \\mu_0^{(t+1)}&=\\frac{s_0}{n} \\\\\n","    \\mu_1^{(t+1)}&=\\frac{s_1}{n} \\\\\n","    \\sigma_{0}^{(t+1)}&=\\frac{s_{00}}{n}-(\\mu_0^{(t+1)})^2 \\\\\n","    \\sigma_{1}^{(t+1)}&=\\frac{s_{11}}{n}-(\\mu_1^{(t+1)})^2 \\\\\n","    \\sigma_{01}^{(t+1)}&=\\frac{s_{01}}{n}-(\\mu_0^{(t+1)}\\mu_1^{(t+1)})\n","\\end{align*}"]},{"cell_type":"markdown","id":"AmnHPKJha7ko","metadata":{"id":"AmnHPKJha7ko"},"source":["## Question 4: Practical implementation"]},{"cell_type":"markdown","id":"DZzGWofb-v17","metadata":{"id":"DZzGWofb-v17"},"source":["### Question 4a"]},{"cell_type":"markdown","id":"3pKRaqjp_JDn","metadata":{"id":"3pKRaqjp_JDn"},"source":["First, an initialization for $\\theta^{(0)}$ must be found. The algorithm will be initialized with empirical quantities (mean and covariance matrix) computed from the complete cases. This is not always a viable solution (for example, when there are too many missing values). Another approach can be to impute the missing values using a computationally inexpensive method and then compute the empirical quantities for $\\theta^{(0)}$."]},{"cell_type":"markdown","id":"_KW5e5zw-xHu","metadata":{"id":"_KW5e5zw-xHu"},"source":["Write the code that performs the initialization. What do you observe?"]},{"cell_type":"markdown","id":"cBr1HgRlBd8B","metadata":{"id":"cBr1HgRlBd8B"},"source":["### Solution"]},{"cell_type":"markdown","id":"CLPDIZBiBe0_","metadata":{"id":"CLPDIZBiBe0_"},"source":["The results for the covariance matrix $\\Sigma$ are biased."]},{"cell_type":"code","execution_count":null,"id":"NgHU5w4y_z7c","metadata":{"id":"NgHU5w4y_z7c"},"outputs":[],"source":["mu_init = np.nanmean(xmiss,axis=0)\n","mu_init"]},{"cell_type":"code","execution_count":null,"id":"Ev5PxGdOAdnk","metadata":{"id":"Ev5PxGdOAdnk"},"outputs":[],"source":["Sigma_init = np.array(pd.DataFrame(xmiss).cov())\n","Sigma_init"]},{"cell_type":"markdown","id":"ef8tWo0U_GCo","metadata":{"id":"ef8tWo0U_GCo"},"source":["### Question 4b"]},{"cell_type":"markdown","id":"gX8aEs-QBma5","metadata":{"id":"gX8aEs-QBma5"},"source":["Write two functions for the E-step and the M-step of the algorithm."]},{"cell_type":"code","execution_count":null,"id":"uWzUgQ23CAwE","metadata":{"id":"uWzUgQ23CAwE"},"outputs":[],"source":["def Estep(xmiss, mu, Sigma, miss_id):\n","\n","    ### TO COMPLETE ###\n","\n","    return {\n","        's0': np.sum(s0),\n","        's1': np.sum(s1),\n","        's00': np.sum(s00),\n","        's11': np.sum(s11),\n","        's01': np.sum(s01)\n","    }"]},{"cell_type":"code","execution_count":null,"id":"WhU24guzDPxs","metadata":{"id":"WhU24guzDPxs"},"outputs":[],"source":["def Mstep(xmiss, s0, s1, s00, s11, s01):\n","\n","    ### TO COMPLETE ###\n","\n","    return {'mu': mu, 'Sigma': Sigma}"]},{"cell_type":"markdown","id":"U7UcirzeBzPX","metadata":{"id":"U7UcirzeBzPX"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"Xv7IgbytCBlX","metadata":{"id":"Xv7IgbytCBlX"},"outputs":[],"source":["def Estep(xmiss, mu, Sigma, miss_id):\n","\n","    n = xmiss.shape[0]\n","\n","    # For the variable X0\n","    s0 = xmiss[:, 0]\n","    s00 = xmiss[:, 0] ** 2\n","\n","    s1 = np.zeros(n)\n","    s11 = np.zeros(n)\n","\n","    # For observed values of X1\n","    s1[~miss_id] = xmiss[~miss_id, 1]\n","    s11[~miss_id] = xmiss[~miss_id, 1] ** 2\n","\n","    # For missing values of X1\n","    s1[miss_id] = mu[1] + (Sigma[0,1] / Sigma[0,0]) * (xmiss[miss_id, 0] - mu[0])\n","    s11[miss_id] = (\n","        s1[miss_id] ** 2 +\n","        Sigma[1,1] - (Sigma[0,1] ** 2) / Sigma[0,0]\n","    )\n","\n","    s01 = s0 * s1\n","\n","    return {\n","        's0': np.sum(s0),\n","        's1': np.sum(s1),\n","        's00': np.sum(s00),\n","        's11': np.sum(s11),\n","        's01': np.sum(s01)\n","    }"]},{"cell_type":"code","execution_count":null,"id":"bmZVfRYmDW08","metadata":{"id":"bmZVfRYmDW08"},"outputs":[],"source":["def Mstep(xmiss, s0, s1, s00, s11, s01):\n","\n","    n = xmiss.shape[0]\n","\n","    mu0 = s0 / n\n","    mu1 = s1 / n\n","\n","    sig0 = s00 / n - mu0 ** 2\n","    sig1 = s11 / n - mu1 ** 2\n","    sig01 = s01 / n - mu0 * mu1\n","\n","    mu = np.array([mu0, mu1])\n","    Sigma = np.array([\n","        [sig0, sig01],\n","        [sig01, sig1]\n","    ])\n","\n","    return {'mu': mu, 'Sigma': Sigma}"]},{"cell_type":"markdown","id":"03TBNn4gF2sw","metadata":{"id":"03TBNn4gF2sw"},"source":["### Question 4c"]},{"cell_type":"markdown","id":"WYD9skLlF4Dy","metadata":{"id":"WYD9skLlF4Dy"},"source":["Run the algorithm for 50 iterations."]},{"cell_type":"markdown","id":"hb5Kzjc8JUIj","metadata":{"id":"hb5Kzjc8JUIj"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"MTEK-wtpF_X-","metadata":{"id":"MTEK-wtpF_X-"},"outputs":[],"source":["mu_EM = mu_init\n","Sigma_EM = Sigma_init\n","\n","for i in range(50):\n","\n","  # E-step\n","  res_Estep = Estep(xmiss, mu_EM, Sigma_EM, miss_id)\n","  s0 = res_Estep[\"s0\"]\n","  s1 = res_Estep[\"s1\"]\n","  s00 = res_Estep[\"s00\"]\n","  s11 = res_Estep[\"s11\"]\n","  s01 = res_Estep[\"s01\"]\n","\n","  # M-step\n","  res_Mstep = Mstep(xmiss, s0, s1, s00, s11, s01)\n","  mu_EM = res_Mstep[\"mu\"]\n","  Sigma_EM = res_Mstep[\"Sigma\"]\n"]},{"cell_type":"code","execution_count":null,"id":"px5dKIQjGtzT","metadata":{"id":"px5dKIQjGtzT"},"outputs":[],"source":["mu_EM"]},{"cell_type":"code","execution_count":null,"id":"6qpbaMDcGgGm","metadata":{"id":"6qpbaMDcGgGm"},"outputs":[],"source":["Sigma_EM"]},{"cell_type":"markdown","id":"5wQWOPI2Ql-v","metadata":{"id":"5wQWOPI2Ql-v"},"source":["# Exercise 2: estimate in order to impute"]},{"cell_type":"markdown","id":"qxJU8XQdQxtx","metadata":{"id":"qxJU8XQdQxtx"},"source":["In this short exercise, you will impute the missing values based on the estimation of the parameter $\\theta$ of the data distribution $p(X)$."]},{"cell_type":"markdown","id":"Auu9PnIpmQT_","metadata":{"id":"Auu9PnIpmQT_"},"source":["### Question 1"]},{"cell_type":"markdown","id":"3GegQYVSmSQs","metadata":{"id":"3GegQYVSmSQs"},"source":["In the case of Exercise 1 (bivariate Gaussian), propose a strategy to impute, that is, to draw a sample following the distribution $p(X_{i1}|X_{i0})$."]},{"cell_type":"markdown","id":"0692SnGY4qBB","metadata":{"id":"0692SnGY4qBB"},"source":["### Solution"]},{"cell_type":"markdown","id":"TlZ19TMP4rCb","metadata":{"id":"TlZ19TMP4rCb"},"source":["In the bivariate Gaussian case, this distribution is explicit. The classic formulas for a Gaussian vector, recalled in question 1a, can be used again.\n","\n","We have:\n","$X_{i1}|X_{i0} \\sim N(\\mathbb{E}[X_{i1}|X_{i0}],\\mathrm{Var}(X_{i1}|X_{i0}))$ with\n","\\begin{align*}\n","\\mathbb{E}[X_{i1}|X_{i0}]&=\\mu_1+\\frac{\\sigma_{01}}{\\sigma_{0}}(X_{i0}-\\mu_0) \\\\\n","\\mathrm{Var}(X_{i1}|X_{i0})&=\\sigma_{1}-\\frac{\\sigma_{01}^2}{\\sigma_{0}}\n","\\end{align*}"]},{"cell_type":"markdown","id":"UvgaqyQI4k2e","metadata":{"id":"UvgaqyQI4k2e"},"source":["## Question 2"]},{"cell_type":"markdown","id":"TG1cTAKG4mRC","metadata":{"id":"TG1cTAKG4mRC"},"source":["Implement the chosen strategy."]},{"cell_type":"markdown","id":"4t51EgEM4oA3","metadata":{"id":"4t51EgEM4oA3"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"c6sBjQqF4pUJ","metadata":{"id":"c6sBjQqF4pUJ"},"outputs":[],"source":["def imputation_EM(mu, Sigma, xmiss, miss_id):\n","\n","  ximp = np.copy(xmiss)\n","\n","  for i in range(xmiss.shape[0]):\n","    if miss_id[i]:\n","      mean = mu[1] + (Sigma[0, 1] / Sigma[0, 0]) * (xmiss[i, 0] - mu[0])\n","      cov = Sigma[1, 1] - (Sigma[0, 1] ** 2) / Sigma[0,0]\n","      ximp[i, 1] = np.random.normal(mean, cov, size=1)\n","\n","  return ximp"]},{"cell_type":"code","execution_count":null,"id":"R5uJ33muQKPc","metadata":{"id":"R5uJ33muQKPc"},"outputs":[],"source":["ximp_EM = imputation_EM(mu_EM, Sigma_EM, xmiss, miss_id)"]},{"cell_type":"code","execution_count":null,"id":"BdWwpIbjScB1","metadata":{"id":"BdWwpIbjScB1"},"outputs":[],"source":["ax = sns.scatterplot(x=ximp_EM[:, 0], y=ximp_EM[:, 1], hue=M[:, 1], palette=['#d1e5f0', '#2166ac'])\n","handles, labels  =  ax.get_legend_handles_labels()\n","ax.set_title('MCAR values imputed with the EM algorithm')\n","ax.set_xlabel(r'$X_{.0}$')\n","ax.set_ylabel(r'$X_{.1}$')\n","ax.legend(handles, ['Observed', 'Imputed'], loc='lower right', fontsize='13')\n",";"]},{"cell_type":"markdown","id":"Es24dccORfo3","metadata":{"id":"Es24dccORfo3"},"source":["# Exercise 3: low-rank methods"]},{"cell_type":"markdown","id":"XBTaPtoEVy3W","metadata":{"id":"XBTaPtoEVy3W"},"source":["In this exercise, you will implement the algorithm presented in the module video, which predicts missing values using iterative Principal Component Analysis (PCA). In question 2, more details will be provided on one of its variants, which better handles noise in the data, called `missMDA`: this method is implemented in a widely used R package. In question 3, you will implement another of its variants yourselves, the `softImpute` algorithm.\n","\n","In low-rank methods, it is assumed that the data matrix can be decomposed into a low-rank matrix plus Gaussian noise.\n","$$X = \\Theta + \\epsilon,\n","$$\n","with $\\epsilon\\sim N(0_d,\\sigma^2I_{d\\times d})$.\n","\n","To predict the missing values in $X$, the parameter $\\Theta$ will be estimated using the iterative PCA algorithm (presented in the video). The algorithm takes as input $r$, the number of dimensions to retain for PCA, as well as the incomplete dataset $X^\\textrm{NA}$.\n","\n","- There is a step of **naive initialization** for imputation (for example, by the mean or even imputing with 0). This yields a complete dataset $X^{(0)}$.\n","\n","Two steps are performed iteratively:\n","\n","- the **estimation step**: $\\theta$ is estimated using the singular value decomposition, keeping only the first $r$ dimensions.\n","$$\\Theta^{(t)}=\\text{SVD}_{\\textbf{r}}({X}^{(t)})=U_{{\\textbf{r}}}D_{{\\textbf{r}}}V_{{\\textbf{r}}}^t,$$\n","\n","- the **imputation step**: the missing values are predicted by the value of $\\Theta^{(t)}$.\n","\n","$$X^{(t+1)}=X \\odot (\\mathbf{1}_{n\\times d}-{M}) + \\: \\Theta^{(t)} \\odot M.$$\n"]},{"cell_type":"markdown","id":"SaFTqIRXhTuD","metadata":{"id":"SaFTqIRXhTuD"},"source":["In this exercise, the same complete real dataset *Breast Cancer Wisconsin* as in the previous modules (Exercise 4 of Lab 1 and Exercise 3 of Lab 2) will be considered. We introduce 30% missing values of the MCAR type."]},{"cell_type":"code","execution_count":null,"id":"igHdsiBZhxOI","metadata":{"id":"igHdsiBZhxOI"},"outputs":[],"source":["data = load_breast_cancer()\n","xfull = data['data'] # covariates, without missing values\n","diagnosis = data['target']   # target variable to predict, when the learning task is prediction\n","features_names = data['feature_names']"]},{"cell_type":"code","execution_count":null,"id":"Hnk70tSwh27O","metadata":{"id":"Hnk70tSwh27O"},"outputs":[],"source":["pd.DataFrame(xfull, columns=features_names).head()"]},{"cell_type":"code","execution_count":null,"id":"SgXNTcB1h4V8","metadata":{"id":"SgXNTcB1h4V8"},"outputs":[],"source":["n, d = xfull.shape"]},{"cell_type":"code","execution_count":null,"id":"hiBKVibIh58C","metadata":{"id":"hiBKVibIh58C"},"outputs":[],"source":["np.random.seed(123)\n","\n","p = 0.3\n","xmiss = np.copy(xfull)\n","for j in range(d):\n","  miss_id = np.random.uniform(0, 1, size=n) < p\n","  xmiss[miss_id, j] = np.nan\n","mask = np.isnan(xmiss)"]},{"cell_type":"markdown","id":"M5Q_k5Zam9vv","metadata":{"id":"M5Q_k5Zam9vv"},"source":["The focus will be on the imputation task, and the mean squared errors (MSE) will be calculated to evaluate the methods (score introduced in Lab 2)."]},{"cell_type":"code","execution_count":null,"id":"iOOqZ0AmnPOY","metadata":{"id":"iOOqZ0AmnPOY"},"outputs":[],"source":["def mse(x_imp, x_true):\n","  n = len(x_true)\n","  return (1 / n) * np.sum((x_imp - x_true) ** 2)"]},{"cell_type":"markdown","id":"W4YeAyljcmtN","metadata":{"id":"W4YeAyljcmtN"},"source":["## Question 1: iterative PCA"]},{"cell_type":"markdown","id":"if7K23CZKnp1","metadata":{"id":"if7K23CZKnp1"},"source":["### Question 1a"]},{"cell_type":"markdown","id":"3fVNPjq8f_OD","metadata":{"id":"3fVNPjq8f_OD"},"source":["Propose a naive imputation by the mean and another by 0 in functions called `init_mean_imputation` and `init_zero_imputation`."]},{"cell_type":"markdown","id":"qR8av2eAgDCS","metadata":{"id":"qR8av2eAgDCS"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"OvkKLP3ngRsv","metadata":{"id":"OvkKLP3ngRsv"},"outputs":[],"source":["def init_mean_imputation(xmiss, mask):\n","    mean_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n","    ximp = mean_imputer.fit_transform(xmiss)\n","    return ximp"]},{"cell_type":"code","execution_count":null,"id":"2SyvZKiRKqlc","metadata":{"id":"2SyvZKiRKqlc"},"outputs":[],"source":["def init_zero_imputation(xmiss, mask):\n","    ximp = xmiss.copy()\n","    ximp[mask] = 0\n","    return ximp"]},{"cell_type":"code","execution_count":null,"id":"Fs-JFJYfKUpe","metadata":{"id":"Fs-JFJYfKUpe"},"outputs":[],"source":["pd.DataFrame(init_mean_imputation(xmiss, mask)).head()"]},{"cell_type":"code","execution_count":null,"id":"MDjUNAWFKrlB","metadata":{"id":"MDjUNAWFKrlB"},"outputs":[],"source":["pd.DataFrame(init_zero_imputation(xmiss, mask)).head()"]},{"cell_type":"markdown","id":"Cf4_C2bKgh_A","metadata":{"id":"Cf4_C2bKgh_A"},"source":["### Question 1b"]},{"cell_type":"markdown","id":"tTISLuG0gjMw","metadata":{"id":"tTISLuG0gjMw"},"source":["Implement the algorithm of iterative PCA in the function `iterative_ACP` which takes as arguments:\n","* `xmiss`: the incomplete dataset,\n","* `r`: the number of dimensions to keep for PCA,\n","* `maxit`: the maximum number of iterations of the algorithm. A convergence criterion will be defined in question 3.\n","\n","The mean imputation will be used as initialization.\n","\n","For the singular value decomposition, you can use the function `linalg.svd` of `numpy` (available documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html))."]},{"cell_type":"code","execution_count":null,"id":"QSSmV_cfmXLp","metadata":{"id":"QSSmV_cfmXLp"},"outputs":[],"source":["def iterative_ACP(xmiss, r, maxit):\n","\n","    ximp = ...\n","\n","    return ximp"]},{"cell_type":"markdown","id":"gncdymG7l0D_","metadata":{"id":"gncdymG7l0D_"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"PpNzqQauXX9y","metadata":{"id":"PpNzqQauXX9y"},"outputs":[],"source":["def iterative_ACP(xmiss, r, maxit):\n","\n","    mask = np.isnan(xmiss)\n","\n","    ximp = init_mean_imputation(xmiss, mask)\n","\n","    for i in range(maxit):\n","        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n","        d_r = d[:r]\n","        U_r = U[:, :r]\n","        V_r = V[:r, :]\n","        D_r = np.diag(d_r)\n","        svd_r = np.dot(U_r, np.dot(D_r, V_r))\n","        ximp[mask] = svd_r[mask]\n","\n","    return ximp"]},{"cell_type":"code","execution_count":null,"id":"GwYgnGn8l3Fi","metadata":{"id":"GwYgnGn8l3Fi"},"outputs":[],"source":["pd.DataFrame(iterative_ACP(xmiss, 5, 50)).head()"]},{"cell_type":"markdown","id":"iM2-YseKn1h0","metadata":{"id":"iM2-YseKn1h0"},"source":["### Question 1c"]},{"cell_type":"markdown","id":"Us2xw8PGn3A9","metadata":{"id":"Us2xw8PGn3A9"},"source":["Calculate the MSE, and compare it with the one obtained using mean imputation."]},{"cell_type":"markdown","id":"DwsjE2hzn9cL","metadata":{"id":"DwsjE2hzn9cL"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"5vXvC-8Mn-tk","metadata":{"id":"5vXvC-8Mn-tk"},"outputs":[],"source":["mean_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n","ximp_mean = mean_imputer.fit_transform(xmiss)\n","mse_mean = mse(ximp_mean,xfull)\n","print(\"MSE imputation par la moyenne:\", mse_mean)"]},{"cell_type":"code","execution_count":null,"id":"Hl0gPD6Aopg5","metadata":{"id":"Hl0gPD6Aopg5"},"outputs":[],"source":["ximp_ACP = iterative_ACP(xmiss, 5, 1000)\n","mse_ACP = mse(ximp_ACP, xfull)\n","print(\"MSE ACP itérative:\", mse_ACP)"]},{"cell_type":"markdown","id":"r0k9u_tJD5Hl","metadata":{"id":"r0k9u_tJD5Hl"},"source":["A slightly lower MSE is obtained compared to mean imputation. It is worth questioning whether the number of dimensions $r$ to keep in the PCA was well chosen here."]},{"cell_type":"markdown","id":"bRkD9L7ttvYI","metadata":{"id":"bRkD9L7ttvYI"},"source":["### Question 1d"]},{"cell_type":"markdown","id":"VRKn2P00twsf","metadata":{"id":"VRKn2P00twsf"},"source":["To choose the optimal number $r$ of dimensions to keep for PCA, one solution is to search over a grid of possible values and select the $r_{\\mathrm{opt}}$ that minimizes the MSE on this grid.\n","\n","Implement this strategy using the following function `additional_na`. It allows adding MCAR-type missing values such that there is at least one observed value remaining in each row."]},{"cell_type":"code","execution_count":null,"id":"O1jpRJDKwGzA","metadata":{"id":"O1jpRJDKwGzA"},"outputs":[],"source":["def additional_na(xmiss, mask):\n","\n","    xmiss_addna = np.copy(xmiss)\n","\n","    for i in range(xmiss.shape[0]):\n","        idx_obs = np.argwhere(mask[i, :] == 0).reshape((-1))\n","        var_obs = np.random.choice(idx_obs) # choice of the observed variable\n","        idx_candidates = idx_obs[idx_obs != var_obs]\n","        var_miss = np.random.uniform(0, 1, size=len(idx_candidates)) < 0.5\n","        missing_idx = idx_candidates[var_miss]\n","        xmiss_addna[i, missing_idx] = np.nan\n","    mask_addna = np.isnan(xmiss_addna)\n","\n","    return(xmiss_addna, mask_addna)"]},{"cell_type":"markdown","id":"uom5CJ47FnFJ","metadata":{"id":"uom5CJ47FnFJ"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"PuZLSIkh4elz","metadata":{"id":"PuZLSIkh4elz"},"outputs":[],"source":["def choose_rank(xmiss, grid_r):\n","\n","    mask = np.isnan(xmiss)\n","\n","    list_error = []\n","    for r in grid_r:\n","        xmiss_addna, mask_addna = additional_na(xmiss, mask)\n","        ximp_ACP = iterative_ACP(xmiss_addna, r, 1000)\n","\n","        list_error.append(np.sqrt(np.nanmean((ximp_ACP.flatten() - xmiss.flatten())**2)))\n","\n","    return list_error"]},{"cell_type":"code","execution_count":null,"id":"RVUfJSI45YP6","metadata":{"id":"RVUfJSI45YP6"},"outputs":[],"source":["grid_r = list(range(1, d))\n","res_rank = choose_rank(xmiss, grid_r)"]},{"cell_type":"code","execution_count":null,"id":"UmumP1M1BctC","metadata":{"id":"UmumP1M1BctC"},"outputs":[],"source":["r_opt = grid_r[np.argmin(res_rank)]\n","ximp_ACP = iterative_ACP(xmiss, r_opt, 1000)\n","mse_ACP = mse(ximp_ACP, xfull)\n","print(\"MSE ACP itérative:\", mse_ACP)\n","print(\"r optimal:\", r_opt)"]},{"cell_type":"markdown","id":"cbQAnZkNGfqR","metadata":{"id":"cbQAnZkNGfqR"},"source":["The chosen optimal $r$ is 1. Thus, a projection of the data onto a single dimension is used to predict the missing values. This results in a significant loss of information, and one possible approach would be to better handle the noise contained in the data."]},{"cell_type":"markdown","id":"4LE5sNc1cn2R","metadata":{"id":"4LE5sNc1cn2R"},"source":["## Question 2 : regularized iterative PCA"]},{"cell_type":"markdown","id":"od6ASyDQMSjz","metadata":{"id":"od6ASyDQMSjz"},"source":["In the algorithm of iterative PCA, the missing values are replaced by : $$\\sum_{k=1}^{r}\\sigma_k(X)u_{ik}v_{jk},$$\n","for $i=1,\\dots,n$ and $j=1,\\dots,p$.\n","\n","To regularize, the `missMDA` algorithm proposes to replace the missing values by: $$\\sum_{k=1}^{r}\\left(\\sigma_k(X)-\\frac{{\\sigma}^2}{\\sigma_k(X)}\\right)u_{ik}v_{jk}.$$\n","\n","This method is implemented in `R` in the `missMDA` package. Other examples of applications are available in this [link](http://factominer.free.fr/missMDA/index.html) and the research paper can be found [here](https://www.jstatsoft.org/article/view/v070i01).\n","\n","Make a list of the hyperparameters to choose in the `missMDA` method."]},{"cell_type":"markdown","id":"NyEZeM2oPH_N","metadata":{"id":"NyEZeM2oPH_N"},"source":["### Solution"]},{"cell_type":"markdown","id":"k9-sNqmlPJm2","metadata":{"id":"k9-sNqmlPJm2"},"source":["The hyperparameters are $r$ (the number of dimensions to keep in the PCA) and $\\sigma^2$ (the variance of the assumed Gaussian noise).\n","\n","To estimate $r$, a method similar to the one chosen in question 1 can be used. To estimate $\\sigma^2$, it has been proposed to use the sum of squared residuals divided by $nd - \\#\\textrm{param}$, where $n$ and $d$ are the dimensions of the data matrix, and $\\#\\textrm{param} = nr + pr - r^2$ is the number of parameters to estimate (here, the number of parameters in a singular value decomposition). More details are available [here](https://arxiv.org/pdf/1602.01206).\n","\n","Note that there is a function `estim_ncpPCA` in the `missMDA` package which proposes a cross-validation procedure to estimate the number of dimensions to keep in the PCA."]},{"cell_type":"markdown","id":"hPZGRCyvcwdu","metadata":{"id":"hPZGRCyvcwdu"},"source":["## Question 3: soft-thresholded iterative PCA"]},{"cell_type":"markdown","id":"BUSdIGnuHAIs","metadata":{"id":"BUSdIGnuHAIs"},"source":["Another method that allows handling noise in the data is to use soft-thresholding with the `softImpute` algorithm. Like `missMDA`, this algorithm better accounts for the noise present in the data compared to classical iterative PCA, by replacing missing values with:\n","$$\\sum_{k=1}^{d}\\max((\\sigma_k(X)-\\lambda),0)u_{ik}v_{jk},$$\n","with $\\lambda>0$.\n","\n","The estimation of $\\theta$ is regularized in the sense that the optimization problem becomes:\n","\n","$$\\theta \\in \\textrm{argmin}_\\theta \\| (X - \\Theta) \\odot (\\mathbf{1}_{n\\times d}-M)\\| + \\lambda \\|\\Theta\\|_\\star,$$\n","\n","with $\\|.\\|$ the nuclear norm.\n","\n","The paper associated with this method is available [here](https://arxiv.org/pdf/1410.2596). The algorithm is implemented in `R` in the `softImpute` package."]},{"cell_type":"markdown","id":"A5pRKx4kILRh","metadata":{"id":"A5pRKx4kILRh"},"source":["### Question 3a"]},{"cell_type":"markdown","id":"ZWawufDTXKxx","metadata":{"id":"ZWawufDTXKxx"},"source":["What are the hyperparameters?"]},{"cell_type":"markdown","id":"Rfs-xNBIXLch","metadata":{"id":"Rfs-xNBIXLch"},"source":["### Solution"]},{"cell_type":"markdown","id":"X69uIxZgXMRv","metadata":{"id":"X69uIxZgXMRv"},"source":["The hyperparameter is $\\lambda$."]},{"cell_type":"markdown","id":"9l5v3HZ5XWq0","metadata":{"id":"9l5v3HZ5XWq0"},"source":["### Question 3b"]},{"cell_type":"markdown","id":"cQcwSaSGIMhI","metadata":{"id":"cQcwSaSGIMhI"},"source":["Implement this method by building on the following base code. Use the zero imputation coded in question 1a."]},{"cell_type":"code","execution_count":null,"id":"qrisKEyDUQgi","metadata":{"id":"qrisKEyDUQgi"},"outputs":[],"source":["def softimpute(xmiss, lamb, maxit = 1000):\n","\n","    mask = np.isnan(xmiss)\n","\n","    ximp = ...\n","\n","    for i in range(maxit):\n","        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n","        d_thresh = ...\n","        r = ...\n","        d_thresh = d_thresh[:r]\n","        U_thresh = U[:, :r]\n","        V_thresh = V[:r, :]\n","        D_thresh = np.diag(d_r)\n","        svd_r = np.dot(U_thresh, np.dot(D_thresh, V_thresh))\n","        ximp[mask] = svd_r[mask]\n","\n","    return ximp"]},{"cell_type":"markdown","id":"eWe2k6N0UVGT","metadata":{"id":"eWe2k6N0UVGT"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"I-FDc2s9UWJj","metadata":{"id":"I-FDc2s9UWJj"},"outputs":[],"source":["def softimpute(xmiss, lamb, maxit = 1000):\n","\n","    mask = np.isnan(xmiss)\n","\n","    ximp = init_mean_imputation(xmiss, mask)\n","\n","    for i in range(maxit):\n","        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n","        d_thresh = np.maximum(d - lamb, 0)\n","        r = (d_thresh > 0).sum()\n","        d_thresh = d_thresh[:r]\n","        U_thresh = U[:, :r]\n","        V_thresh = V[:r, :]\n","        D_thresh = np.diag(d_thresh)\n","        svd_r = np.dot(U_thresh, np.dot(D_thresh, V_thresh))\n","        ximp[mask] = svd_r[mask]\n","\n","    return ximp"]},{"cell_type":"code","execution_count":null,"id":"klLaOmRjpA44","metadata":{"id":"klLaOmRjpA44"},"outputs":[],"source":["ximp_soft = softimpute(xmiss, 100, maxit = 1000)"]},{"cell_type":"code","execution_count":null,"id":"Wca5WGezpbBt","metadata":{"id":"Wca5WGezpbBt"},"outputs":[],"source":["mse(ximp_soft,xfull)"]},{"cell_type":"markdown","id":"wC0b1tXbU80p","metadata":{"id":"wC0b1tXbU80p"},"source":["### Question 3c"]},{"cell_type":"markdown","id":"zeID2Uu_U-Jm","metadata":{"id":"zeID2Uu_U-Jm"},"source":["To avoid unnecessary iterations, the convergence criterion implemented below can be used, which compares the difference between two successive iterates. If the difference is smaller than a threshold `conv_thresh` (set to $10^{-5}$ by default), the algorithm is considered to have converged.\n","\n","Incorporate this convergence criterion into the function `softimpute`."]},{"cell_type":"code","execution_count":null,"id":"FIOXwsp8VJDl","metadata":{"id":"FIOXwsp8VJDl"},"outputs":[],"source":["def converged(x_t, x_tplus1, mask, conv_thresh):\n","\n","    x_t_na = x_t[mask]\n","    x_tplus1_na = x_tplus1[mask]\n","    rmse = np.sqrt(np.sum((x_t_na - x_tplus1_na) ** 2))\n","    denom = np.sqrt((x_t_na ** 2).sum())\n","\n","    return (rmse / denom) < conv_thresh"]},{"cell_type":"markdown","id":"diXZlQTOVuYk","metadata":{"id":"diXZlQTOVuYk"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"bd7PlqFrVwzx","metadata":{"id":"bd7PlqFrVwzx"},"outputs":[],"source":["def softimpute(xmiss, lamb, maxit = 1000, conv_thresh = 1e-5):\n","\n","    mask = np.isnan(xmiss)\n","\n","    ximp = init_mean_imputation(xmiss, mask)\n","\n","    for i in range(maxit):\n","        U, d, V = np.linalg.svd(ximp, compute_uv = True, full_matrices=False)\n","        d_thresh = np.maximum(d - lamb, 0)\n","        r = (d_thresh > 0).sum()\n","        d_thresh = d_thresh[:r]\n","        U_thresh = U[:, :r]\n","        V_thresh = V[:r, :]\n","        D_thresh = np.diag(d_thresh)\n","        svd_r = np.dot(U_thresh, np.dot(D_thresh, V_thresh))\n","        if converged(ximp, svd_r, mask, conv_thresh):\n","            break\n","        ximp[mask] = svd_r[mask]\n","\n","    return ximp"]},{"cell_type":"markdown","id":"JR6DKHosXagH","metadata":{"id":"JR6DKHosXagH"},"source":["### Question 3d"]},{"cell_type":"markdown","id":"cZ_KlySEXb8X","metadata":{"id":"cZ_KlySEXb8X"},"source":["Use the function `choose_lambda` below, which explores a grid of values for $\\lambda$ and uses the function additional_na defined in question 1d.\n","\n","`grid_len` sets the size of the grid of values.\n","The grid of values is defined inside the `choose_lambda` function by computing the maximum singular value of the dataset imputed with zeros."]},{"cell_type":"code","execution_count":null,"id":"br0CSmHQqBw8","metadata":{"id":"br0CSmHQqBw8"},"outputs":[],"source":["def choose_lambda(xmiss, grid_len = 15, maxit = 1000, conv_thresh = 1e-5):\n","\n","    mask = np.isnan(xmiss)\n","    ximp_0 = init_zero_imputation(xmiss, mask)\n","\n","    # generate grid for lambda values\n","    d = np.linalg.svd(ximp_0, compute_uv=False, full_matrices=False) # svd on imputed dataset with 0\n","    lambda_max = np.max(d)\n","    lambda_min = 0.001 * lambda_max\n","    grid_lambda = np.exp(np.linspace(np.log(lambda_min), np.log(lambda_max), grid_len).tolist())\n","\n","    cv_error = []\n","    for lamb in grid_lambda:\n","        xmiss_addna, mask_maskna = additional_na(xmiss, mask)\n","        ximp_soft = softimpute(xmiss_addna, lamb, maxit, conv_thresh)\n","        cv_error.append(np.sqrt(np.nanmean((ximp_soft.flatten() - xmiss.flatten()) ** 2)))\n","\n","    return cv_error, grid_lambda\n"]},{"cell_type":"markdown","id":"TMUwZwo5YTB1","metadata":{"id":"TMUwZwo5YTB1"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"KasS9Pc0q2LM","metadata":{"id":"KasS9Pc0q2LM"},"outputs":[],"source":["res_lambda, grid_lambda = choose_lambda(xmiss, grid_len = 15)\n","lambda_opt = grid_lambda[np.argmin(res_lambda)]\n","ximp_soft = softimpute(xmiss, lambda_opt)\n","mse_soft = mse(ximp_soft, xfull)\n","print(\"MSE softImpute:\", mse_soft)"]},{"cell_type":"markdown","id":"SeS9FNXxQ3zV","metadata":{"id":"SeS9FNXxQ3zV"},"source":["# Exercise 4: method using deep learning"]},{"cell_type":"markdown","id":"uxBbVz-ucWiG","metadata":{"id":"uxBbVz-ucWiG"},"source":["In this exercise, you will predict the missing values in the *Breast Cancer Wisconsin* dataset (from exercise 3) using the `MIWAE` method. The research paper proposing this method is available [here](https://arxiv.org/pdf/1812.02633). Notebooks are available in the Github repository [here](https://github.com/pamattei/miwae/tree/master). The code is taken from [this notebook](https://github.com/pamattei/miwae/blob/master/Tensorflow%202%20notebooks/MIWAE_UCI_single_multiple-imputation.ipynb).\n","\n","There are no implementation questions in this exercise; the goal is to better understand the method. The computations can be done on CPU or GPU but will be faster if a GPU is available.\n","\n"]},{"cell_type":"markdown","id":"cC6jWkiVD478","metadata":{"id":"cC6jWkiVD478"},"source":["`MIWAE` uses a deep latent variable model. Let $Z \\in \\mathbb{R}^{d_{\\textrm{lat}}}$ be the latent variables. The idea is to encode the information contained in the data into the latent space (of smaller dimension, $d_{\\textrm{lat}} < d$). The information is then decoded back to the data space with a distribution $p_\\theta(X|Z)$ parameterized by $f_\\theta(Z)$, which is trained by a neural network.\n","\n","`MIWAE` proposes to estimate the parameter $\\theta$ by computing a lower bound $L_K$ of the observed likelihood, such that $L_K \\leq L_{\\mathrm{obs}}$. This bound draws samples for the latent variables according to a distribution $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$, which encodes the data into the latent space, and which is parameterized by $g_\\gamma(X_{\\mathrm{obs}(M)})$, trained by another neural network.\n","$$L_K(\\theta,\\gamma)=\\sum_{i=1}^n \\mathbb{E}_{Z_{i1},\\dots,Z_{iK}\\sim p_\\gamma(Z|X_{\\mathrm{obs}(M)})}\\left[\\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_\\theta(X_{\\mathrm{obs}(M)}|Z_{ik})p(Z_{ik})}{{p_\\gamma(Z_{ik}|X_{\\mathrm{obs}(M)})}}\\right].$$\n","\n","Finally, the generative model is as follows:\n","$$\n","\\left\\{\n","    \\begin{array}{ll}\n","        Z\\sim p_\\gamma(Z|X_{\\mathrm{obs}(M)};{g_\\gamma(X_{\\mathrm{obs}(M)})}) &  \\textrm{(encoder)} \\\\\n","        X_{\\mathrm{obs}(M)} \\sim p_\\theta(X_{\\mathrm{obs}(M)}|Z;{f_\\theta(Z)}) & \\textrm{(decoder)}\n","    \\end{array}\n","\\right.\n","$$\n","\n","with $g_\\gamma(X_{\\mathrm{obs}(M)})$ and $f_\\theta(Z)$ trained by neural networks."]},{"cell_type":"markdown","id":"1QhlChoGEzJ1","metadata":{"id":"1QhlChoGEzJ1"},"source":["## Question 1: hyperparameters and model"]},{"cell_type":"markdown","id":"qcUA4oG7E1J7","metadata":{"id":"qcUA4oG7E1J7"},"source":["What are the hyperparameters? What choices need to be made for this imputation method?"]},{"cell_type":"markdown","id":"zW5tIUL9E3AV","metadata":{"id":"zW5tIUL9E3AV"},"source":["### Solution"]},{"cell_type":"markdown","id":"RqysqEvlE4qv","metadata":{"id":"RqysqEvlE4qv"},"source":["The hyperparameters are $d_{\\textrm{lat}}$ (dimension of the latent space) and $K$ (number of samples for the importance sampling).\n","\n","The distributions $p(Z)$, $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$ and $p_\\theta(X_{\\mathrm{obs}(M)}|Z)$ can be Gaussian, but a more relevant choice can be made depending on the situation and also on the type of variables (quantitative, categorical).\n","\n","Finally, the neural network architecture must be chosen, along with associated parameters, notably:\n","* the number of hidden layers,\n","* the size (dimension) of the hidden layers in the neural networks."]},{"cell_type":"markdown","id":"9yt8TUvdF95j","metadata":{"id":"9yt8TUvdF95j"},"source":["In the following, for the latent variables, a standard Gaussian distribution will be chosen (this prior distribution is suitable most of the time) : $p(Z)=N(0_d,I_{d\\times d})$. Pour $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$, A Gaussian distribution with a mean and a diagonal covariance matrix is also chosen. For $p_\\theta(X_{\\mathrm{obs}(M)}|Z)$, a Student’s t-distribution will be used.\n","\n","The neural networks for the encoder and the decoder share the same architecture, consisting of 3 layers. For example, for $p_\\gamma(Z|X_{\\mathrm{obs}(M)})$, we assume:\n","$f_\\theta(Z)=\\sigma(W_1\\sigma(W_0 Z + b_0)+ b_1)$, where $\\sigma$ is the ReLU activation function. The parameters of the Student’s t-distribution are then estimated with :\n","\\begin{align*}\n","\\mu_\\theta&=W_\\mu f_\\theta(Z) + b_\\mu \\quad \\textrm{(mean)} \\\\\n","\\Sigma_\\theta&=\\textrm{Softplus}(\\textrm{Diag}(W_\\sigma f_\\theta(Z) + b_\\sigma))+10^{-3} \\quad \\textrm{(covariance matrix)} \\\\\n","\\nu_\\theta &= \\textrm{Softplus}(W_\\nu f_\\theta(Z) + b_\\nu)+3 \\quad \\textrm{(number of degrees of freedom)},\n","\\end{align*}\n","We add $10^{-3}$ to effectively obtain a covariance matrix, and $3$ to have at least three degrees of freedom so that the tails are not too heavy. These choices are explained in more detail in the `MIWAE` method notebook mentioned above.\n"]},{"cell_type":"code","execution_count":null,"id":"0ab240f8","metadata":{"id":"0ab240f8"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # determine si CUDA est disponible (utilisation GPU)"]},{"cell_type":"code","execution_count":null,"id":"a5PrFGmwfDWI","metadata":{"id":"a5PrFGmwfDWI"},"outputs":[],"source":["h = 128 # number of hidden units in (same for all MLPs)\n","d_lat = 10 # dimension of the latent space\n","K = 20 # number of IS during training"]},{"cell_type":"code","execution_count":null,"id":"OP6bcHpLPzvA","metadata":{"id":"OP6bcHpLPzvA"},"outputs":[],"source":["p_z = td.Independent(td.Normal(loc=torch.zeros(d_lat).to(device), scale=torch.ones(d_lat).to(device)), 1)"]},{"cell_type":"code","execution_count":null,"id":"9LKFRenSQJg_","metadata":{"id":"9LKFRenSQJg_"},"outputs":[],"source":["decoder = nn.Sequential(\n","    torch.nn.Linear(d_lat, h),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(h, h),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(h, 3 * d),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",")"]},{"cell_type":"code","execution_count":null,"id":"lg7aPxSZQLYl","metadata":{"id":"lg7aPxSZQLYl"},"outputs":[],"source":["encoder = nn.Sequential(\n","    torch.nn.Linear(d, h),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(h, h),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(h, 2 * d_lat),  # the encoder will output both the mean and the diagonal covariance\n",")"]},{"cell_type":"code","execution_count":null,"id":"8kTRF_DBQqkM","metadata":{"id":"8kTRF_DBQqkM"},"outputs":[],"source":["def weights_init(layer): # this function will be used for initializing the neural networks\n","  if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)"]},{"cell_type":"markdown","id":"oQ_jqXSeFuu5","metadata":{"id":"oQ_jqXSeFuu5"},"source":["Below is the function that calculates the bound $L_K$ to be maximized."]},{"cell_type":"code","execution_count":null,"id":"ZZnZ-tecF22j","metadata":{"id":"ZZnZ-tecF22j"},"outputs":[],"source":["def miwae_loss(iota_x, mask):\n","  batch_size = iota_x.shape[0]\n","  out_encoder = encoder(iota_x)\n","  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d_lat], scale=torch.nn.Softplus()(out_encoder[..., d_lat:(2*d_lat)])), 1)\n","\n","  zgivenx = q_zgivenxobs.rsample([K])\n","  zgivenx_flat = zgivenx.reshape([K*batch_size, d_lat])\n","\n","  out_decoder = decoder(zgivenx_flat)\n","  all_means_obs_model = out_decoder[..., :d]\n","  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., d:(2*d)]) + 0.001\n","  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*d):(3*d)]) + 3\n","\n","  data_flat = torch.Tensor.repeat(iota_x,[K, 1]).reshape([-1, 1])\n","  tiledmask = torch.Tensor.repeat(mask,[K, 1])\n","\n","  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1, 1]), scale=all_scales_obs_model.reshape([-1, 1]), df=all_degfreedom_obs_model.reshape([-1, 1])).log_prob(data_flat)\n","  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K * batch_size, d])\n","\n","  logpxobsgivenz = torch.sum(all_log_pxgivenz * (1 - tiledmask), 1).reshape([K, batch_size])\n","  logpz = p_z.log_prob(zgivenx)\n","  logq = q_zgivenxobs.log_prob(zgivenx)\n","\n","  neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n","\n","  return neg_bound"]},{"cell_type":"markdown","id":"vjO-lPVaGw89","metadata":{"id":"vjO-lPVaGw89"},"source":["## Question 2: standardization"]},{"cell_type":"markdown","id":"gAkNXBRqGx8s","metadata":{"id":"gAkNXBRqGx8s"},"source":["Like many deep learning methods, data needs to be standardized.\n","\n","The dataset containing missing values is standardized with the following code. What could be the problem here?"]},{"cell_type":"code","execution_count":null,"id":"wAQnVGACHK_1","metadata":{"id":"wAQnVGACHK_1"},"outputs":[],"source":["mean_xmiss = np.nanmean(xmiss, 0)\n","std_xmiss = np.nanstd(xmiss, 0)\n","xmiss = (xmiss - mean_xmiss) / std_xmiss\n","\n","ximp_0 = init_zero_imputation(xmiss, mask)"]},{"cell_type":"markdown","id":"mOZV6TFHIwZO","metadata":{"id":"mOZV6TFHIwZO"},"source":["### Solution"]},{"cell_type":"markdown","id":"mtpJg96LIzuz","metadata":{"id":"mtpJg96LIzuz"},"source":["The data are standardized by computing the mean and standard deviation on the zero-imputed data. These estimates may therefore be biased, but it can be assumed that this will not have an impact on the final model training."]},{"cell_type":"markdown","id":"L8Qto5_vJD3P","metadata":{"id":"L8Qto5_vJD3P"},"source":["## Question 3: model training"]},{"cell_type":"markdown","id":"emwWKh2sJLLr","metadata":{"id":"emwWKh2sJLLr"},"source":["Which algorithm should be used to optimize the bound $L_K$ ?"]},{"cell_type":"markdown","id":"vY68tnuZPp-F","metadata":{"id":"vY68tnuZPp-F"},"source":["### Solution"]},{"cell_type":"markdown","id":"-d72Ed3RPrDQ","metadata":{"id":"-d72Ed3RPrDQ"},"source":["The mini-batch stochastic gradient descent (SGD) algorithm is used, recalled below in pseudo-code:\n","\n","`for epoch=0 in NB_epochs:` # loop over the epochs\n","*   `Draw a random permutation of the data and create mini-batches of size n_batch`\n","*   `For each batch`: # loop over the batches\n","  + `Update the parameters`\n","$$ (\\theta^{(t+1)},\\gamma^{(t+1)})=(\\theta^{(t)},\\gamma^{(t)})-\\lambda \\: \\partial_{(\\theta,\\gamma)} L_K(\\theta^{(t)},\\gamma^{(t)}), \\quad \\textrm{with $\\lambda$ is the learning rate.}$$\n","\n","\n","\n","\n","\n","The user selects the learning rate, the number of epochs, and the batch size.\n","\n","In practice, the `Adam` optimization algorithm is preferred. It is an extension of `SGD` that uses an adaptive learning rate and momentum.\n","\n","Below is the code for training the model."]},{"cell_type":"code","execution_count":null,"id":"G-AMdi54Qkni","metadata":{"id":"G-AMdi54Qkni"},"outputs":[],"source":["optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"id":"1AaoODSWQeor","metadata":{"id":"1AaoODSWQeor"},"outputs":[],"source":["encoder.to(device) # we'll use the GPU or CPU depending on the available device\n","decoder.to(device)"]},{"cell_type":"code","execution_count":null,"id":"zlfVKDSfQuQ-","metadata":{"id":"zlfVKDSfQuQ-"},"outputs":[],"source":["miwae_loss_train = np.array([])\n","mse_train = np.array([])\n","mse_train2 = np.array([])\n","bs = 64 # batch size\n","n_epochs = 2002 # 1 epoch = all the data are used once\n","ximp_MIWAE = np.copy(ximp_0) # This will be out imputed data matrix\n","\n","encoder.apply(weights_init)\n","decoder.apply(weights_init)\n","\n","for ep in range(1, n_epochs):\n","  perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n","  batches_data = np.array_split(ximp_0[perm,], n/bs)\n","  batches_mask = np.array_split(mask[perm,], n/bs)\n","  for it in range(len(batches_data)):\n","    optimizer.zero_grad()\n","    encoder.zero_grad()\n","    decoder.zero_grad()\n","    b_data = torch.from_numpy(batches_data[it]).float().to(device)\n","    b_mask = torch.from_numpy(batches_mask[it]).float().to(device)\n","    loss = miwae_loss(iota_x=b_data, mask=b_mask)\n","    loss.backward()\n","    optimizer.step()\n","  if ep % 100 == 1:\n","    print('Epoch %g' %ep)\n","    print('MIWAE likelihood bound  %g' %(-np.log(K) - miwae_loss(iota_x=torch.from_numpy(ximp_0).float().to(device), mask=torch.from_numpy(mask).float().to(device)).cpu().data.numpy())) # Gradient step"]},{"cell_type":"markdown","id":"YyK5Q9jANMfR","metadata":{"id":"YyK5Q9jANMfR"},"source":["## Question 4: imputation"]},{"cell_type":"markdown","id":"dWf1PKKEXrRW","metadata":{"id":"dWf1PKKEXrRW"},"source":["The final step consists in predicting the missing values. A simple imputation will be performed, but multiple imputation is also possible (see [specific notebook](https://github.com/pamattei/miwae/blob/master/Tensorflow%202%20notebooks/MIWAE_UCI_single_multiple-imputation.ipynb)).\n","\n","Which quantity should be computed to predict the missing values?"]},{"cell_type":"markdown","id":"SOAvJYhw3Qlu","metadata":{"id":"SOAvJYhw3Qlu"},"source":["### Solution"]},{"cell_type":"markdown","id":"BZx0tQ8LbdPI","metadata":{"id":"BZx0tQ8LbdPI"},"source":["The idea of `MIWAE` is to compute the conditional expectation of $X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)}$ as follows:\n","\n","   \n","   \\begin{align*}\n","        &\\mathbb{E}[X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)}] \\\\\n","        &=\\int X_{\\mathrm{mis}(M)} p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)})dX_{\\mathrm{mis}(M)} \\\\\n","        &= \\int \\int X_{\\mathrm{mis}(M)} p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)},Z)p_\\theta(Z|X_{\\mathrm{obs}(M)})dZdX_{\\mathrm{mis}(M)} \\\\\n","        &= \\int \\int X_{\\mathrm{mis}(M)} \\frac{p_\\theta(Z|X_{\\mathrm{obs}(M)})}{{p_\\gamma(Z|X_{\\mathrm{obs}(M)})}}{p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)},Z)p_\\gamma(Z|X_{\\mathrm{obs}(M)})}dZdX_{\\mathrm{mis}(M)}\n","    \\end{align*}\n","It is not explicit, so a self-normalized importance sampling method is used (more details on this sampling method are available [here, Section 9.2](https://artowen.su.domains/mc/)):\n","$$\\sum_{l=1}^L w_l X_{\\mathrm{mis}(M)}^{(l)}$$\n","with $w_l=\\frac{r_l}{\\sum_{l=1}^L r_l}$, $r_l=\\frac{p_\\theta(X_{\\mathrm{obs}(M)}|Z^{(l)})p(Z^{(l)})}{{p_\\gamma(Z^{(l)}|X_{\\mathrm{obs}(M)})}}$, et\n","$(X_{\\mathrm{mis}(M)}^{(l)},Z^{(l)}) \\overset{\\mathrm{iid}}{\\sim} {p_\\theta(X_{\\mathrm{mis}(M)}|X_{\\mathrm{obs}(M)},Z)p_\\gamma(Z|X_{\\mathrm{obs}(M)})}$"]},{"cell_type":"code","execution_count":null,"id":"vI30MswoQnuw","metadata":{"id":"vI30MswoQnuw"},"outputs":[],"source":["def miwae_impute(iota_x,mask, L):\n","  batch_size = iota_x.shape[0]\n","  out_encoder = encoder(iota_x)\n","  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d_lat], scale=torch.nn.Softplus()(out_encoder[..., d_lat:(2*d_lat)])), 1)\n","\n","  zgivenx = q_zgivenxobs.rsample([L])\n","  zgivenx_flat = zgivenx.reshape([L * batch_size, d_lat])\n","\n","  out_decoder = decoder(zgivenx_flat)\n","  all_means_obs_model = out_decoder[..., :d]\n","  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., d:(2*d)]) + 0.001\n","  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*d):(3*d)]) + 3\n","\n","  data_flat = torch.Tensor.repeat(iota_x,[L, 1]).reshape([-1, 1]).to(device)\n","  tiledmask = torch.Tensor.repeat(mask,[L, 1]).to(device)\n","\n","  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1, 1]), scale=all_scales_obs_model.reshape([-1, 1]), df=all_degfreedom_obs_model.reshape([-1, 1])).log_prob(data_flat)\n","  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L * batch_size, d])\n","\n","  logpxobsgivenz = torch.sum(all_log_pxgivenz * (1 - tiledmask), 1).reshape([L, batch_size])\n","  logpz = p_z.log_prob(zgivenx)\n","  logq = q_zgivenxobs.log_prob(zgivenx)\n","\n","  xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model), 1)\n","\n","  imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq, 0) # these are w_1,....,w_L for all observations in the batch\n","  xms = xgivenz.sample().reshape([L, batch_size, d])\n","  xm = torch.einsum('ki,kij->ij', imp_weights, xms)\n","\n","  return xm\n"]},{"cell_type":"code","execution_count":null,"id":"1qGbxEUSNFd8","metadata":{"id":"1qGbxEUSNFd8"},"outputs":[],"source":["ximp_MIWAE[mask] = miwae_impute(iota_x=torch.from_numpy(ximp_0).float().to(device), mask=torch.from_numpy(mask).float().to(device), L=10).cpu().data.numpy()[mask]\n","ximp_MIWAE_destandardized = ximp_MIWAE * std_xmiss + mean_xmiss\n","mse_MIWAE = mse(ximp_MIWAE_destandardized, xfull)\n","print(\"MSE MIWAE:\", mse_MIWAE)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["lMRqaWG2T_DZ"],"gpuType":"T4","provenance":[{"file_id":"1Xiy4IPKBNVZYarLmnt1o-bL2ntC-Aswh","timestamp":1756901070873}],"toc_visible":true},"kernelspec":{"display_name":"semipy-venv","language":"python","name":"semipy-venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}